# Outils de collecte de données {#sec-chap5}

La révolution numérique engendrée par l'émergence du Big Data représente un important défi pour la communauté académique en sciences sociales [@manovich11; @burrows_savage14]. Elle constitue également une opportunité de recherche enrichissante et innovante permettant une compréhension accrue des phénomènes sociaux étudiés par la communauté scientifique [@connelly_etal16]. Cette meilleure compréhension est permise, entre autres, par l'accès à une variété inégalée et une quantité massive de données permettant l'étude des différents objets de recherche [@kramer_etal14; @schroeder14]. Si l'accès à ces données numériques représente un défi éthique et théorique, tel qu'explicité lors des chapitres précédents, il représente également un défi technique pour les chercheurs.euses voulant exploiter le potentiel et les opportunités offertes par les données massives numériques [@burrows_savage14].

Le chapitre qui suit vise à offrir un portrait de différents outils de collectes de données pouvant être exploités par les scientifiques désirant entreprendre des recherches en sciences sociales, tout en mettant l'accent sur les opportunités offertes par le numérique. En effet, la portée de ce chapitre s'étend plus loin que les outils traditionnels de collecte de données en abordant le potentiel émanant de la programmation en matière de récolte de données brutes pouvant être analysées par les chercheurs.ses. Le chapitre débute par une description non-exhaustive d'outils de collecte de divers types de données sélectionnés en raison de la portée de leur utilisation ou encore en raison de leur capacités techniques. Cette section aborde, notamment, des outils de collecte de données de sondage ou encore des données textuelles de médias. Les sections suivantes mettront un accent particulier sur la méthode du web scraping par le biais de l'outil *rvest* et les opportunités de recherche qui en découlent. Ce chapitre offre donc un tour d'horizon des outils disponibles à la communauté scientifique tout en présentant les manières d'exploiter le web scraping et le package *rvest* et en offrant des exemples concrets de son utilisation.

## **Point d'observation: les outils traditionnels de collecte de données numériques en sciences sociales**

Toute recherche empirique repose sur l'analyse d'un objet de recherche, et doit donc comprendre une réflexion sur les manières d'observer l'objet d'étude. Avant l'utilisation de quelconque méthode, il faut un ensemble de données à analyser sur lesquelles les différentes méthodes seront appliquées. Le type de données à analyser dépend de l'objectif analytique de la recherche entreprise, mais surtout, de l'objet de recherche. Une fois que l'on sait *qu'est* ce qu'on veut étudier, il faut déterminer *comment* l'étudier, et ultimement, acquérir les données nécessaires à l'analyse. Les sciences sociales reposent sur l'analyse des sociétés et des pénomènes qui les caractérisent. Les objets d'études sont multiples, tout comme les manières de les observer. Les communautés scientifiques autant qualitatives que quantitatives ont donc besoin de données qui permettent de tels types d'analyses, et il existe un immense éventail d'outils permettant la collecte de données nécessaires à la tenue de recherche en sciences sociales.

L'exemple des entrevues représente bien l''étendue des manières de récolter des données en sciences sociales. Les entrevues (dirigée, semi-dirigée ou non-dirigée) sont des méthodes de recherche largement employées en sciences sociales, et peuvent constituer un outil de collecte de données qui permet d'aller récolter un savoir pouvant par la suite être analysé sous forme de texte retranscrit. Bien que l'objectif de ce chapitre ne soit pas de présenter les manières de conduire une entrevue en science sociale, il demeure pertinent de le mentionner afin d'exposer l'immensité du champ de la collecte de données en sciences sociales. Les prochaines lignes présenteront quelques outils de collecte de données particulièrement prisés en sciences sociales, soit les données de sondages et les données de médias, et comment l'émergence du numérique a radicalement transformé le processus d'acquisition de telles données. Il est important de noter que les données de sondages et les textes médiatiques se prêtent tout aussi bien aux analyses quantitatives que qualitatives. L'objectif des prochaines lignes vise donc à présenter des outils de qui se démarquent par la portée analytique des données qu'ils permettent de récolter.

L'opinion publique, une composante fondamentale des sciences sociales, est traditionnellement étudiée par le biais de données de sondages. L'émergence des technologies numériques a grandement transformé la collecte de données de sondages, qui sont désormais conceptualisés et administrés de manière beaucoup plus efficace. En effet, le numérique permet donc de créer un questionnaire, de cibler une population et de la contacter, d'entreposer les données des répondants pour ainsi les visualiser, le tout à un coût réduit et plus rapidement que s'il avait été conduit manuellement [@nayak_ka19]. Ainsi, les sondages en ligne ont une portée internationale, permettent le suivi de la ligne du temps, offrent des options qui contraignent le répondant à répondre à certaines questions et permettent d'utiliser des arbres de logique avancés que les sondages manuels ne permettent pas. Parmi les plateformes web les plus reconnues de construction et d'administration de sondage, Qualtrics figure en tête de liste. Cette plateforme est une des plus reconnues et utilisée à l'international, tant dans le milieu académique que dans le secteur privé. En plus d'offrir des outils de collecte de données et de sondages, Qualtrics est utilisé dans le marketing et dans la gestion de l'expérience client. Il est donc pertinent de se familiariser avec cet outil, car il offre des compétences pratiques pour la recherche, mais également pour obtenir des opportunités de carrière. Qualtrics offre plusieurs services pratiques pour la collecte de données, avec des options flexibles pour la programmation et l'administration des sondages. Par exemple, Qualtrics s'adapte à différents formats en fonction de l'appareil du répondant [@evans_mathur18]. Son principal désavantage provient de son coût d'acquisition, qui est extrêmement dispendieux. La présentation de cet outil dans le cadre de ce chapitre provient du fait qu'il s'agit de la principale plateforme de construction de sondage, autant dans le monde commercial qu'académique. Il devient d'autant plus important de présenter cet outil en raison du fait que, malgré l'obstacle monétaire, certaines universités possèdent des licenses permettant l'utilisation de l'outil par la communauté étudiante. De plus, de nombreus.e.s professeur.e.s possèdent également des comptes Qualtrics qui permettent la construction de sondage.

Il convient toutefois de présenter certaines alternatives à Qualtrics en raison des obstacles monétaires à son utilisation. Research Electronic Data Capture (REDCap) permet de construire et de gérer des sondages ainsi que des bases de données. Cette plateforme, particulièrement prisée dans le milieu de la santé, représente une alternative très efficace à Qualtrics. En plus d'être bien moins dispendieux que Qualtrics, l'immense majorité des institutions universitaires occidentales possèdent des licenses d'utilisation de RedCap, ce qui renforce son accessibilité. Pour accéder aux services de cette application, il est nécessaire d'être un partenaire du REDCap Consortium ou membre d'une organisation qui en fait partie. Seules les organisations à but non lucratif peuvent adhérer au Consortium. Les données et les sondages qui y sont produits peuvent être partagés et utilisés par différents chercheurs issus de diverses institutions. L'exportation vers différents types de fichiers (Excel, PDF, SPSS, SAS, Stata, R) est possible. Ce qui distingue REDCap des autres applications est sa compatibilité avec les dossiers médicaux, sa sécurité pour les données sensibles, ainsi que son approche académique à la collecte de données par sondage.

SurveyMonkey se distingue des autres plateformes en permettant de construire et gérer des sondages et formulaire formulaires à l'aide d'une interface conviviale sans toutefois perdre de ses fonctionnalités. En plus d'avoir recours aux nouvelles technologies de l'I.A. pour aider à construire des sondages adaptés aux besoins des scientifiques, cette application propose plusieurs centaines de modèles personnalisables élaborés par des experts dans le domaine. SurveyMonkey permet également l'analyse des données et la création de rapports directement sur l'application, en plus de permettre l'exportation vers d'autres types de programmes. Les forfaits varient en gamme de tarifs, allant de la gratuité et des fonctionnalités restreintes, jusqu'aux options payantes destinées aux particuliers et aux entreprises.

Enfin, Google Forms se distingue par sa simplicité et son accessibilité, en grande partie grâce à l'omniprésence de google tant dans le monde académique que dans la vie courante. Google Forms est inclus dans le forfait de base du Google Workspace, ce qui le rend largement compatible avec les autres applications de Google, en plus d'être disponible gratuitement. Bien que ses fonctionnalités soient moins avancées que celles de ses compétiteurs, Google Forms peut convenir pour des sondages plus simples et rapides grâce à son interface conviviale, à sa fonction d'analyse de données directement sur la plateforme, ainsi qu'à ses modèles préfabriqués.

La recherche en sciences sociales se penche également sur l'analyse de contenu médiatique, qui permet une mutltitude de devis de recherche dans un grand nombre de sous-discipline des sciences sociales. L'émergence du numérique facilite largement la récolte de données médiatiques tout en favorisant de nouvelles avenues de recherche pour les chercheurs.euses en sciences sociales en raison de l'importante quantité de données accessibles aux chercheurs.euses. À cet effet, l'outil Factiva offre un accès à l'ensemble des articles d'une panoplie de médias provenant d'une vaste sélection de pays. Le moteur de recherche est opéré par Dow Jones et offre également l'accès à des documents d'entreprises. En revanche, l'accès qu'il offre aux contenus médiatiques est particulièrement pertinent pour la communauté scientifique en communication et en sciences sociales. Il offre l'accès à plus de 15 000 sources médiatiques provenant de 120 pays. Il permet de télécharger une quantité illimitée de documents RTF, un format de fichier de texte, pouvant contenir jusqu'à 100 articles chacun. En outre, ils peuvent être sélectionnés automatiquement en cochant le bouton proposant de sélectionner les 100 articles de la page de résultat. Chaque page de résultat contient 100 articles à la fois. Enfin, Factiva permet également de filtrer les doublons. Ainsi, Factiva permet d'avoir accès facilement à des données utiles pour l'analyse textuelle d'articles médiatiques. Comme les textes deviennent accessibles rapidement et simplement aux chercheurs.euses, cet outil optimise considérablement l'analyse de contenu par thèmes ou par ton. Cependant, ce ne sont pas tous les médias qui sont accessibles sur Factiva. Dans l'optique ou un média recherché n'est pas trouvable sur Factiva, le logiciel Eureka représente une bonne alternative. Eureka se concentre principalement sur les médias francophones (autant au Québec qu'en Europe). La structure d'Eureka est similaire à celle de Factiva.

```{r, out.width="80%"}
#| label: interface-factiva
#| echo: false
#| fig-cap: "Interface de la plateforme Factiva"
knitr::include_graphics("images/chapitre3_factiva.png", dpi = 600)
```

```{r, out.width="80%"}
#| label: interface-eureka
#| echo: false
#| fig-cap: "Interface de la plateforme Eureka"
knitr::include_graphics("images/chapitre3_eureka.png", dpi = 600)
```

Ces outils sont très utiles et relativement faciles à utiliser. Il ne faut toutefois pas tomber dans le piège de se limiter aux outils traditionnels de recherche. En effet, les récentes transformations technologiques élargissent considérablement le champ de possibilités offertes à la communauté scientifique, notamment en raison de la nature massive des données qui lui est accessible. Non seulement ces données sont nombreuses, mais elles sont accessibles par le biais de connaissances de base en programmation. La section suivante aborde un outil fondamental de la collecte de données en sciences sociales numériques, les extracteurs webs.

## **Arpentage et choix éditorial: le web scraping grâce à l'outil *rvest*- collecter automatiquement des données provenant de sites web**

Les extracteurs de données sont des outils qui ont transformé la récolte de données en sciences sociales. Les extracteurs de données numériques sont des infrastructures de code permettant d'extraire des données brutes d'une source numérique définie. La section suivante explique comment ces extracteurs peuvent être utiles dans un contexte de recherche en sciences sociales numériques.

L'émergence du numérique représente une opportunité hors pair d'accès à un volume important de données, qui permettent ainsi une analyse approfondie des phénomènes politiques contemporains. Toutefois, l'accès à de telles données peut s'avérer complexe, non-fiable ou encore coûteux. Par exemple, des données parlementaires peuvent être accessibles sur les sites internet des parlements et institutions en questions. L'accès à ces données se voit toutefois complexifié par la nécessité d'avoir des identifiants ou encore de payer pour les dites données. De plus, la qualité de ces données n'est pas assurée, en plus du fait qu'elles peuvent être mal-structurées. Ainsi, l'accès à des données massives représente un défi considérable pour la communauté scientifique tentant d'entreprendre des recherches utilisant un volume important de données.

C'est dans cette optique que les extracteurs de données numériques peuvent être utiles. Il faut faire ici la distinction entre l'outil, soit l'extracteur de données, et la méthode, le web scraping. Plus précisément, le web scraping est la méthode qui permet l'extraction de données provenant de sites webs qui seront ensuite converties dans un format utile aux scientifiques de données. L'extracteur est l'outil qui permet cette méthode. Il sera question dans les lignes qui suivent de l'outil *rvest*, un package sur R, qui permet de récolter des données de site web gratuitement et efficacement.

Dans un contexte de recherche en sciences sociales, *rvest* permet de récolter automatiquement les données de sites internets pertinents qui pourront ensuite être utilisées afin de mener à terme un projet de recherche. Les sites desquels les données seront extraites dépendent du sujet de recherche d'intérêt de la personne entreprenant la recherche. Par exemple, un code peut extraire de manière automatisée les débats des parlements, les communiqués de presse des gouvernants, les plateformes électorales des partis politiques, ce qui offre un accès inégalé aux chercheurs.euses aux données de décideurs. De telles données pourraient mener à des analyses poussées sur le contenu et le ton des débats parlementaires. Dans une autre optique, des extracteurs peuvent également offrir l'accès aux données provenant de médias socionumériques comme Twitter (maintenant X) ou Facebook . Un extracteur peut, par exemple, être en mesure de répertorier l'ensemble des Tweets de journalistes, de politiciens ou encore de citoyens de manière automatisée, offrant un accès inégalé aux chercheurs.euses à des données massives exclusives. Ainsi, les extracteurs sont des outils qui changent drastiquement l'accès aux données massives en sciences sociales numériques.

Il faut toutefois être vigilant quant à la nature des données extraites. Avant d'extraire quelconque information, il est absolument primordial de s'assurer que les données soient publiques, faute de quoi l'extraction serait illégale. Il est donc recommandé de prendre connaissance des termes et conditions des sites webs étudiés afin de s'assurer de la légalité de l'extraction de données. Il est également important de respecter toute norme de droit d'auteurs et de propriété intellectuelles ou physique des données. De plus, ce n'est pas parce que des données sont publiques qu'il est nécessairement légal de les extraire et de les utiliser en recherche. En effet, il ne faut pas faire ressortir dans les données extraites quelconque information privée qui pourrait permettre d'identifier des individus (comme des numéros de téléphone, des adresses courriels, des codes postaux, etc.)

L'utilisation de cet outil est toutefois facilitée par l'existence d'API (*Application Programming Interface*) sur les plateformes exploitées. L'API d'un site ou d'une application, souvent fournie par le site, permet à un tierce partie d'avoir accès à du code expliquant le fonctionnement de la plateforme étudiée, ce qui en facilite l'extraction de données. Par exemple, Twitter possédait, avant les changements de directions récents, un API qui facilitait l'élaboration d'un extracteur. En contrepartie, Facebook ne possède pas d'API, ce qui rend l'accès à ses données beaucoup plus complexe. Un API fournit des données structurées dans un format lisible tel que JSON (JavaScript Object Notation). En raison de l'automatisation, les API réduisent les chances d'erreurs dans le processus de scraping, ils ont tendance à maintenir une interface plus stable et conviviale. C'est un grand changement comparé aux fichiers HTML, où ces derniers ont des mises en page qui changent fréquemment de structure.

Un extracteur peut également offrir l'accès à des données médiatiques, en codant un accès à des fils RSS ou encore aux HTML des médias extraits. Les fils RSS sont des formats de données qu'il est possible de recevoir automatiquement lors de mise à jour sur un site particulier. Par exemple, La Presse change ses Unes plusieurs fois dans une même journée. En extractant l'accès aux fils RSS, il est possible de recevoir lesdites mises à jour automatiquement. Ce processus accélère grandement la collecte de données.

Finalement, les API simplifient le travail de chacun et chacune par les mises à jour et maintenances. Pour être plus précis, les API, étant maintenus par les fournisseurs de services, sont modifiés au fur et à mesure que le site évolue. Par exemple, si la structure de X est modifiée, son API sera également modifié. Cela évite à ceux demandant l'accès d'ajuster leurs scripts de scraping pour prendre en compte les changements sur le site.

En somme, les extracteurs webs, et dans ce cas précis le package rvest, sont des outils qui facilitent grandement l'acquisition de données massives. Plutôt que d'avoir à payer pour des données dont la qualité n'est pas assurée, l'utilisation d'extracteurs permet un accès plus facile et précis à des données provenant de sites web. L'utilisation de ces outils peut toutefois s'avérer complexe et nécessitent un certain nombre de connaissances en lien avec les langages de programmation. Le chapitre 2 du présent ouvrage offre un survol du langage fonctionnel R, qui est utilisé par de nombreux développeurs lors de l'écriture d'extracteurs et qui permet d'utiliser le package *rves*t. R est également reconnu pour ces fonctionnalités statistiques qui sont, elles aussi, abordées ultérieurement dans ce livre. L'utilisation de cet outil permet aux chercheurs.euses de faire plusieurs coups d'une seule pierre. Non seulement *rvest* permet de se familiariser avec le langage R, ce qui est un atout essentiel dans la recherche en sciences sociales numériques, mais ce même développement permet un accès inégalé à des données massives qui pourront ensuite être analysées. Ainsi, des connaissances en R sont essentielles au développement d'extracteurs, et la complexité du code évoluera dépendamment du site qui sera extrait.

### Critères de sélection

L'extracteur *rvest* est un outil accessible, particulièrement comme il s'agit d'un outil libre de coûts monétaires. En effet, à moins de circonstances exceptionnelles, le web scraping avec rvest est gratuit. Le coût serait plutôt dans l'acquisition de connaissances préalables en code puis dans l'apprentissage de la méthode d'utilisation. Malgré cette nuance, l'absence de coûts monétaires liés à cet outil favorise son accessibilité à l'ensemble de la communauté scientifique. Il est par exemple possible d'étudier l'opinion publique grâce à l'extraction de publications sur des médias socionumériques, plutôt que de construire et administrer des sondages coûteux (voir par exemple l'article de Barbera et al., 2015).

Additionnellement, l'outil jouit d'une grande communauté d'utilisateurs.ices rendant accessible leurs connaissances, ce qui favorise la diffusion des savoirs liés à l'utilisation de cet outil. De nombreux forums d'aide peuvent alors être consultés gratuitement par les chercheurs.euses voulant se familiariser avec le package, et plus largement, la méthode du web scraping. De nombreux forums sont présents sur le site Stack Overflow, et certains chapitres d'ouvrages collectifs vulgarisant l'utilisation de l'outil sont également dispinible gratuitement en ligne (voir par exemple le chapitre *Web scraping* du livre *R for Data Science* d'Hadley Wickham, 2022, qui présente comment utiliser l'outil *rvest*). Bien que des outils comme Qualtrics et Factiva aient également un grand nombre d'utilsateurs.ices, ces outils ne se démarquent pas par la transparence et le partage de leur communauté utilisatrice.

Les extracteurs sont des outils de recherche très en vogue dans la communauté des sciences sociales. La possibilité d'obtenir une quantité importante de données de manière efficace et gratuite représente une opportunité depuis largement exploitée par les chercheurs.euses en sciences sociales, en témoigne l'émergence de cours dédiés à l'utilisation d'outils permettant le web scraping dans de nombreuses écoles méthodologiques de renom (telles qu'ICPSR ou l'école d'été méthodologique du European Consortium for Political Research).

Le web scraping avec le logiciel R via l'outil *rvest* permet de récolter des données qui pourront ensuite être traitées et exploiter avec R, ce qui favorise donc la compatibilité de l'outil. De plus, les données extraites peuvent être exportées dans de nombreux formats tels que des fichiers .csv, .RDS, .xlsx, et plus encore.

Tel q'explicité précédemment, la présence d'une grande communauté d'utilisateurs.ices de l'outil favorise la transparence et la réplicabilité des différents résultats produits lors de l'utilisation d'extracteurs.

## Manuel d'instruction: extraire des données avec R

La section suivante présente la méthode d'utilisation de l'outil *rvest*. Elle démontre comment l'outil permet d'accomplir du web scraping, une méthode très en vogue en science sociales.

### L'importance de comprendre la structure du code HTML

Afin d'être à l'aise avec les extracteurs web, et dans notre cas précis le package *rvest*, il peut être un atout de se familiariser avec la structure de base du langage HTML, dont l'acronyme signifie "Hypertext Markup Language". Il s'agit d'un langage de code qui permet la description du contenu de la page web. La structure du code HTML est hiérarchique, ce qui signifie que le code est divisé en différentes sections qui occupent différents rôles. Ces sections sont délimitées par des "tags", qui définissent le début ou la fin d'une section du site. Ce sont ces différentes sections qui seront accessibles pour l'extraction, et le code HTML permet de comprendre ce qui se situe dans ces sections. La Figure \[\] représente un exemple de base de code HTML.

```{r, out.width="80%"}
#| label: html-input
#| echo: false
#| fig-cap: "Exemple de code HTML"
knitr::include_graphics("images/chapitre5_html_input.png", dpi = 600)
```

Dans l'exemple ci-haut, le tag \<h1 représente le titre du HTML. Le signe \<p permet de débuter le paragraphe de texte suivant le titre, et cette section devra être terminée par le sigle p\>. Les sections \<p\> délimitent les paragraphes écrits dans chacunes des sections. Les sigles \<h2\> produisent une sous-section et un sous titre, qui pourra être complémenté d'un paragraphe écrit. La Figure 2 démontre le texte produit par le code HTML.

```{r, out.width="80%"}
#| label: html-output
#| echo: false
#| fig-cap: "Exemple du résultat de code HTML"
knitr::include_graphics("images/chapitre5_html_output.png", dpi = 600)
```

La structure de base du langage HTML est somme toute simple et intuitive et tous les sites web sur internet sont fondés sur du code HTML. N'importe qui étant intéressé à extraire des données de sites webs et tirer profit de la simplicité et l'accessibilité des données qui peuvent en émerger devraient donc se familiariser avec ce langage. De nombreuses sources en ligne sont disponibles afin d'apprendre sur le fonctionnement du code HTML. Nous vous encourageons donc fortement à explorer plus en profondeur les structures du code HTML afin d'obtenir une compréhension accrue du fonctionnement des sites webs que vous allez extraire. Comme le code HTML de chaque site est accessible grâce à l'URL, les données présentes sur des sites webs sont plus que jamais accessible à la communauté scientifique.

### **Le package rvest: son fonctionnement et ses possibilités**

Cet ouvrage recommande l'utilisation du paquetage *rvest* afin de récolter des données sur des pages web, qu'elles aient ou non un API. Rvest est construit autour des paquetages "xml2" et "httr" afin de faciliter la manipulation du HTML et XML. Rvest est principalement conçu pour scraper une seule page web alors que pour scraper de multiples pages, d'autres paquetages sont recommandés, notamment "polite". Ce chapitre ne rentre pas dans les détails et ne se concentrera que sur Rvest.

#### **Fonctions de base du paquetage rvest utilisant pour exemple le site de LEGISinfo**

La première étape est l'installation et le chargement des paquetages "tidyverse" et "rvest" sur sa console Rstudio. Il est important de les charger séparément car RVEST ne fait pas partie des paquetages de base du TIDYVERSE. Nous installerons ce dernier car il amène des fonctions pratiques au scraping

``` bash
install.packages("rvest")

install.packages("tidyverse")

library(rvest)

library(tidyverse)
```

Pour débuter l'extraction des données, il suffit de copier l'URL de la page web à scraper et la coller dans l'appel de la fonction read_html(). Il est important de stocker l'URL dans l'objet "html_LEGISinfo.

``` bash
html_LEGISinfo <- read_html("https://www.parl.ca/legisinfo/en/bills")

html_LEGISinfo
```

Lors de l'exécution des lignes de code ci-hautes, la console retournera les éléments suivants:

``` bash
{html_document}
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
[1] <head>\n<meta http-equiv="Content-Type" 
content="text/html; charset=UTF-8">\n<meta 
...
[2] <body class="body-wrapper ce-parl vh-100">\r\n    
<header class="d-print-none"><!-- ...
```

Une fois que les éléments que l'on souhaite extraire sont déterminés, il faut les trouver dans le document HTML. Pour ce faire, il faut se référer au style CSS (cascading style sheets), langage définissant la forme visuelle d'un document HTML. Les éléments HTML sont identifiés avec des "sélecteurs CSS", ayant pour but de les regrouper pour faciliter leur extraction. Pour les bases du scraping, il n'est pas primordial de comprendre les détails des sélecteurs CSS. Seule la compréhension de la structure d'un document HTML est nécessaire afin d'en faire l'extraction d'éléments. L'important est d'être en mesure d'identifier les sélecteurs CSS liés aux éléments souhaités, sans avoir à comprendre le sélecteur en question.

D'abord, html_elements doit être utilisé en premier pour trouver toutes les observations souhaitées, car cette fonction retourne une liste de tous les noeuds qui matchent avec l'appel de fonction. Le nombre d'observations est indiqué par xml_nodeset(). Comme html_element retourne seulement le premier élément qui match, il faut l'utiliser en deuxième, après html_elements. Cette seconde fonction à pour but de trouver les éléments qui deviendront les variables à extraire. Pour l'exemple de LEGISinfo, nous commencerons par extraire tous les éléments \<a\>. Comme html_elements retourne une liste, nous voulons commencer avec cette fonction.

``` bash
html_LEGISinfo |> html_elements("a")
```

Qui retourne les éléments suivants:

``` bash
{xml_nodeset (180)}
 [1] <a href="#StartOfContent" class="ce-parl-skipnav sr-only 
 sr-only-focusable">Skip to m ...
 [2] <a href="//www.parl.ca" class="ce-parl-btn float-left text-nowrap">
 Parliament of Cana ...
 [3] <a href="https://visit.parl.ca/index-e.html" rel="external">\n\t\t                    ...
```

``` bash
html_LEGISinfo |> html_elements("a")
```

Qui retourne les éléments suivants:

``` bash
{xml_nodeset (180)}
 [1] <a href="#StartOfContent" class="ce-parl-skipnav sr-only sr-only-
 focusable">Skip to m ...
 [2] <a href="//www.parl.ca" class="ce-parl-btn float-left text-nowrap">
 Parliament of Cana ...
 [3] <a href="https://visit.parl.ca/index-e.html" rel="external">\n\t\t                    ...
```

``` bash
html_LEGISinfo |> html_elements("a")
```

``` bash
{html_node}
<a href="#StartOfContent" class="ce-parl-skipnav sr-only sr-only-focusable">
```

Suite à l'inspection des éléments <a>, ceux qui nous intéressent sont ceux de classe "bill-tile-container". Il suffit d'ajouter un point "." avant la classe souhaitée lors de l'appel de la fonction afin de rechercher les éléments en fonction de leur classe. Les classes HTML servent à catégoriser les éléments HTML selon un style prédéterminé. Pour l'exemple de LEGISinfo, nous obtenons une liste d'éléments de classe bill-tile-container que nous allons stocker dans l'objet BillTile_LEGISinfo. Tous les éléments de cette classe auront donc tous une structure ou des comportements similaires entre eux.

``` bash
BillTile_LEGISinfo <- html_LEGISinfo |> html_elements(".bill-tile-container")
```

L'exécution du bloc de code ci-haut produit le résultat suivant dans la console

``` bash
{xml_nodeset (60)}
 [1] <a class="bill-tile-container senate" href="/legisinfo/en/bill/44-1/s-1">
 \r\n\r\n     ...
 [2] <a class="bill-tile-container senate" href="/legisinfo/en/bill/44-1/s-2">
 \r\n\r\n     ...
 [3] <a class="bill-tile-container senate" href="/legisinfo/en/bill/44-1/s-3">
 \r\n\r\n     ...
```

À partir de la liste d'éléments de classe bill-tile-container, nous appelons la fonction html_element, qui lorsque appliqué à une liste permet d'extraire la première correspondance de tous les éléments de cette liste au lieu de seulement retourner le premier nœud correspondant du document HTML. Nous cherchons à extraire ici les éléments de classe parliament-session par le biais de la ligne de code suivante:

``` bash
BillTile_LEGISinfo |> html_element(".parliament-session") 
```

Ce qui produit le résultat suivant dans la console:

``` bash
{xml_nodeset (60)}
 [1] <div class="parliament-session">\n<span class="parl-session-number">
 44th</span> Parlia ...
 [2] <div class="parliament-session">\n<span class="parl-session-number">
 44th</span> Parlia ...
 [3] <div class="parliament-session">\n<span class="parl-session-number">
 44th</span> Parlia ...
```

Bien que moins utile pour la mise en situation actuelle, il est également possible d'extraire les éléments en fonction de leur "id attribute". Pour ce faire, il faut mettre un hashtag (#) avant l'élément à extraire lors de l'appel de la fonction. Le id attribute retourne toujours un seul élément car ils sont uniques à chaque document HTML. Voici la ligne de code et le résultat produit par une telle opération

``` bash
html_LEGISinfo |> html_elements("#StartOfContent")
```

``` bash
{xml_nodeset (1)}
[1] <a id="StartOfContent" tabindex="-1"></a>
```

Nous avons créé précédemment l'objet BillTile_LEGISinfo pour ensuite y extraire les éléments de classe parliament-session. Nous appelons cette étape l'imbrication des sélections. Lorsque la fonction html_element est appliquée à un vecteur de liste html_elements, la console retourne le premier nœud correspondant de chaque élément de la liste. Il est important d'utiliser html_element à cette étape car il retourne un NA même lorsqu'il n'y a pas d'éléments correspondants, alors que html_elements ne retournera pas la valeur manquante. Dans l'exemple de LEGISinfo, c'est exactement ce que l'on a voulu faire pour obtenir les éléments de classe parliament-session. Nous sommes maintenant arrivés à l'étape d'extraire les données souhaitées. C'est assez simple, il ne suffit que d'appliquer la fonction html_text2 sur l'appel de html_element sur l'objet à moissonner, dans ce cas ci BillTile_LEGISinfo. Il est important de prendre en compte que nous connaissons ici les éléments à extraire, car le script du document a été scruté préalablement grâce à la fonction "inspect" de Google Chrome ainsi que les diverses fonctions du package rvest. Afin d'extraire les autres informations souhaitées, nous allons également créer deux autres objets qui seront à leur tour moissonnés. Voici les différentes opérations et leurs résultats dans la console:

Code

``` bash
BillTile_LEGISinfo |> html_element("h4") |> html_text2()
```

Console

``` bash
[1] "S-1"   "S-2"   "S-3"   "S-4"   "S-5"...
```

Code

``` bash
BillTile_LEGISinfo |> html_element(".parliament-session") |> html_text2().
```

Console

``` bash
[1] "44th Parliament, 1st session" "44th Parliament, 1st session"...
```

Code

``` bash
BillTile_LEGISinfo |> html_element("h5") |> html_text2()
```

Console

``` bash
[1] "An Act relating to railways"                                                                                                                                                                                                                                                                                  
 [2] "An Act to amend the Parliament of Canada Act and to make 
 consequential and related amendments to other Acts"                                                                                                                                                                                                  
 [3] "An Act to amend the Judges Act"   
...
```

Code

``` bash
BillBS_LEGISinfo <- html_LEGISinfo |> html_elements(".bottom-section")
BillBS_LEGISinfo |> html_element("dd") |> html_text2()
```

Console

``` bash
[1] "Introduced as pro forma bill"                                 
"Senate bill awaiting first reading in the House of Commons"               
 [3] "Bill not proceeded with"                                              
 "Royal assent received"                                                    
 [5] "Royal assent received"                                                 
 "At second reading in the House of Commons"     
...
```

Code

``` bash
Bill_stage <- html_LEGISinfo |> html_elements(".progress-bar-description")
Bill_stage |> html_element("dd") |> html_text2()
```

Console

``` bash
[1] "First reading in the Senate"            
"Third reading in the Senate"            
"First reading in the Senate"           
"Royal assent"                        
"Royal assent"                          
 [6] "First reading in the House of Commons" 
 "First reading in the House of 
 Commons"  "Royal assent"                          
 "First reading in the House of Commons"  "Royal assent"        
...
 
```

Maintenant que nous avons tous les éléments souhaités, il ne reste plus qu'à utiliser la fonction tibble du tidyverse. Ce paquetage permet de facilement créer des dataframes sur R. Voici le script à produire dans l'exemple de LEGISinfo, ainsi que son résultat, un dataframe contenant le numéro de projet de loi, sa session parlementaire, son nom, son statut et son dernier stage de réalisation :

``` bash
Table_LEGISinfo <- tibble(
  Bill = Bill_LEGISinfo |> html_element("h4") |> html_text2(),
  Session = Bill_LEGISinfo |> html_element(".parliament-session") |>
  html_text2(),
  Name = Bill_LEGISinfo |> html_element("h5") |> html_text2(),
  Status = BillBS_LEGISinfo |> html_element("dd") |> html_text2(),
  Stage = Bill_stage |> html_element("dd") |> html_text2()
)
```

Il est important de noter que ce chapitre ne permet que de scraper des documents html uniques. Afin de scraper plusieurs page web simultanément, il faudra utiliser d'autres paquetages ainsi que des boucles, ce qui est trop complexe pour cet ouvrage d'introduction. Maintenant que vous savez extraire des informations d'un document html pour le mettre dans une base de données, voici d'autres applications pratiques de rvest à cet effet.

Il est possible d'extraire les éléments en fonction de leur attribut grâce à html_attr(). Un attribut est une information supplémentaire associé à une balise html. Voici l'attribut href qui permet d'extraire l'URL du projet de loi en question. De cette façon, il est possible de boucler sur les href afin de moissonner divers niveaux d'une page HTML. Lorsque l'extraction se fait sur plusieurs niveaux, la pratique passe du moissonnage pour devenir de l'indexation. Cette pratique, bien que fondamentale, ne sera pas abordée en raison de sa complexité avancée. Tel que mentionné plus haut, cet ouvrage ne se concentre que sur le moissonnage.

``` bash
BillTile_LEGISinfo |> html_attr("href")
```

La ligne de code ci-haut produit le résultat suivant dans la console

``` bash
1] "/legisinfo/en/bill/44-1/s-1"  
"/legisinfo/en/bill/44-1/s-2" 
"/legisinfo/en/bill/44-1/s-3"  
"/legisinfo/en/bill/44-1/s-4"  
"/legisinfo/en/bill/44-1/s-5"  
"/legisinfo/en/bill/44-1/s-6"  
 [7] "/legisinfo/en/bill/44-1/s-7" 
 "/legisinfo/en/bill/44-1/s-8"  
 "/legisinfo/en/bill/44-1/s-9"  
 "/legisinfo/en/bill/44-1/s-10"  
 "/legisinfo/en/bill/44-1/s-11" 
 "/legisinfo/en/bill/44-1/s-12" 
[13] "/legisinfo/en/bill/44-1/s-13" 
"/legisinfo/en/bill/44-1/s-14" 
"/legisinfo/en/bill/44-1/s-15"  
"/legisinfo/en/bill/44-1/s-16" 
"/legisinfo/en/bill/44-1/s-17" 
"/legisinfo/en/bill/44-1/s-201"
```

Il est également possible d'extraire des tables. Pour cet exemple, le site de LEGISinfo ne comporte malheureusement pas de tables, le script de https://r4ds.hadley.nz/webscraping sera donc utilisé. Celui-ci utilise la fonction minimal_html pour créer un script html, qui n'est pas nécessaire au moissonnage, mais toutefois utilisé pour cet exemple.

``` bash
htmltest <- minimal_html("
  <table class='mytable'>
<tr><th>x</th>   <th>y</th></tr>
<tr><td>1.5</td> <td>2.7</td></tr>
<tr><td>4.9</td> <td>1.3</td></tr>
<tr><td>7.2</td> <td>8.1</td></tr>
</table>
  ")
  htmltest |>
  html_element(".mytable") |> html_table()
```

L'opération précédante produit le résultat suivant dans la console

``` bash
# A tibble: 3 × 2
      x     y
  <dbl> <dbl>
1   1.5   2.7
2   4.9   1.3
3   7.2   8.1
```

En conclusion de cette section, lorsque l'on moissonne un document HTML, il est important de ne pas se laisser intimider par la structure du document. Il ne faut pas perdre patience afin de trouver les bons sélecteurs. Ce sont des structures peu familières au début, mais on s'y habitue rapidement. Ensuite, nous recommandons d'utiliser l'outil de développeur de votre navigateur web afin de pouvoir trouver les sélecteurs souhaités. L'interface de Chrome est particulièrement conviviale, et est recommandée. Il suffit de cliquer sur "inspect" suite à un clic droit, et il est possible de chercher les éléments souhaités dans le script. Finalement, avant de scrapper le contenu d'un site web, il est important de vérifier s'il n'offre pas déjà une option pour télécharger les données ! C'est le cas de l'exemple utilisé ici (LEGISinfo), il est possible dans certains cas de télécharger les données directement sur le site web, ce qui rend parfois le besoin de moissonner désuet.

## Conclusion et discussion:

Ce chapitre a comme objectif de dresser un portrait des différents outils de collecte de données mis à la disposition des scientifiques s'intéressant aux sciences sociales tout en voulant exploiter le potentiel de la révolution numérique. Bien que non exhaustif, ce chapitre fait un survol d'outils traditionnels de récolte de données numériques en sciences sociales. Par exemple, les outils de sondages ou de récolte de données médiatiques sont présentés dans les paragraphes ci-haut. En revanche, ce chapitre s'ancre autour du postulat qu'il ne faut pas se limiter aux outils de récolte de données traditionnels, et que la révolution numérique engendre d'importantes opportunités d'acquisition de données massives et exclusives par le biais des extracteurs de données, plus précisemment l'outil d'extraction *rvest*. Ces extracteurs ont pour but de moissonner les données présentes sur un site web afin de les rendre disponibles pour l'analyse scientifique. Une section complète de ce chapitre vise à vulgariser le processus d'extraction de données provenant de site web en utilisant l'exemple du site LégisInfo, ce qui permet aux lecteurs.ices de se familiariser avec le processus de moissonnage de données.

Toutefois, un seul chapitre ne permet pas de relever l'ensemble des outils de collecte de données disponibles pour la communauté scientifique. Néanmoins, les outils présentés permettent un aperçu à la fois d'outils plus conventionnels et répandus de collecte de données numériques, mais également de dresser un portrait du potentiel d'extraction permis par la maîtrise de R.
