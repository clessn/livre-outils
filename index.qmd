# Avant-propos {.unnumbered}

Avant propos de Yannick Dufresne.

# Données massives, causalité et sciences sociales : Changements et réflexions sur l’avenir.

L'apparition des données massives (*big data*) dans le paysage technologique représente un cas de phénomène hautement technique dont les effets politiques et sociaux sont remarquables. Depuis quelques années, la discussion publique s'est en effet rapidement emparée du sujet, au point de transformer un développement technologique en phénomène social. Les données massives se trouvent ainsi régulièrement présentées dans l'espace public à la fois comme un moyen puissant de développement et d'innovation technoscientifique, de même que comme une menace à la stabilité de certaines normes sociales telles que la confidentialité des informations privées. Il n'est d'ailleurs pas rare que le discours public s'inquiète du danger que poseraient les données massives à la séparation des sphères publique et privée, pourtant centrale à la conception libérale du rôle de la politique qui structure la majorité des débats sociaux, en amalgamant parfois de manière trop rapide l'objet et l'utilisation qui en est faite. Toutefois, ce même discours public s'emporte aussi rapidement à propos des gains technologiques monumentaux réalisés par l'utilisation des données massives.

Dans le domaine des sciences sociales, les avancées dues à l'utilisation des données massives se font de plus en plus fréquentes et l'impact des données massives dans le domaine de la recherche sociale est en ce sens indéniable. Toutefois, d'un point de vue épistémologique, l'utilisation des données massives en recherche en sciences sociales dans les dernières années laisse plusieurs questions ouvertes dans son sillage.

Comment l'utilisation des données massives change-t-elle la pratique des sciences sociales? Les données massives causeront-elles un changement de paradigme scientifique?

Ce chapitre ne prétend pas offrir de réponses définitives à ces questions, mais plutôt des pistes de réflexion par le biais d'une introduction critique de certains points relatifs aux impacts des données massives sur la recherche en sciences sociales. Premièrement, nous présentons une conceptualisation des données massives. Deuxièmement, nous nous penchons sur les impacts des données massives en sciences sociales et soulignons tout particulièrement comment elles affectent les enjeux de la *validité* interne et externe dans le domaine des sciences sociales. Cela nous offre aussi l’opportunité d’aborder le sujet important de la différence entre les données expérimentales et observationnelles. Finalement, nous proposons quelques pistes de réflexion sur l'avenir des données massives en sciences sociales en identifiant certains changements *épistémologiques* que ces données pourraient potentiellement entraîner.

## Définition des données massives

Il existe au moins trois approches conceptuelles permettant de définir les « données massives » (voir Figure 1.1.).

![image1_1](images/chapitre1_definitions.png) 1. Premièrement, les données massives représentent une ***quantité importante de points d'information*** qui varient selon la nature, le type, la source, etc. Ici, la distinction entre données massives et données plus traditionnelles (ou « non-massives ») est simplement quantitative.

2.  Deuxièmement, les données massives constituent un ***ensemble de pratiques*** de collecte, de traitement et d'analyse de ces points d'information. Les données massives représentent une technique, c’est-à-dire une manière ou une méthode nouvelle de faire de la recherche.

3.  Finalement, d'une perspective sociologique, les données massives représentent les impacts sociaux de ces importants développements technologiques. Cette perspective souligne le caractère essentiellement social des données massives, en portant notamment attention aux risques liés à la confidentialité des données, aux enjeux relatifs au consentement et à l'autorisation de collecte des informations, aux innovations en intelligence artificielle, etc.

Dans les domaines scientifiques et technologiques, la définition courante attribuée aux données massives intègre des éléments de ces trois niveaux d'analyse en se référant à la composition et à la fonction des données. Premièrement, la *composition* des données massives est généralement conceptualisée comme comprenant « 4V » : le volume, la variété, la vélocité et la véracité. Cette conceptualisation jouit d'un large consensus scientifique (Chen, Mao et Liu, 2014; Gandomi et Haider, 2015; Kitchin et McArdle 2016). Par ailleurs, plusieurs chercheurs ont élargi cette définition de la composition des données massives en y incluant, par exemple, la variabilité et la valeur des points de données (Kitchen et McArdle 2016). Deuxièmement, la *fonction* des données massives comprend les innovations relatives à l'optimisation, à la prise de décision et à l'approfondissement des connaissances qui résultent de leur utilisation. Ces fonctions touchent des domaines sociaux disparates, incluant le souci d'efficacité et de rendement des secteurs privé et public ainsi que la recherche scientifique pure (Gartner 2012).

## Les données massives et les sciences sociales

Dans le domaine des sciences sociales, les changements causés par l'utilisation des données massives en recherche sont significatifs. Plusieurs n'hésitent d'ailleurs pas à les qualifier de changements de paradigme dans l'étude des phénomènes sociaux (Anderson 2008; Chandler 2015; Grimmer 2015; Kitchin 2014; Monroe et al. 2015). Dans le cas qui nous intéresse, deux dimensions majeures méritent d'être abordées : (1) une première relative à la validité (interne et externe) des données massives et (2) une seconde relative à la différence entre les données expérimentales et les données observationnelles. Ces deux dimensions sont présentées de manière simultanées dans les prochaines sections.

### La validité de la mesure en sciences sociales

La validité de la mesure constitue une exigence méthodologique centrale à la recherche en sciences sociales. Les scientifiques cherchent effectivement à s'assurer que ce qui est mesuré --- par un sondage, une entrevue, un thermostat ou tout autre outil de mesure --- constitue bel et bien ce qui est censé être mesuré. Adcock et Collier définissent plus spécifiquement l'application de la validité de la mesure en sciences sociales en affirmant que des scores (y compris les résultats de classification qualitative) doivent capturer de manière significative les idées contenues dans le concept correspondant (2001: 530).

Toutefois, les problèmes liés à la validité de la mesure sont nombreux et ont une importance considérable. Dans l'étude des phénomènes sociaux et humains, la validité de la mesure prend d'ailleurs une complexité supplémentaire du fait que les données collectées par le biais d'une mesure constituent le *produit de l'observation* d'un phénomène, mais non pas le phénomène en soi. Ainsi, lorsque, dans le contexte d'une recherche, on propose de mesurer l'humeur de l'opinion publique (le phénomène en soi) sur un enjeu politique, on utilise généralement un sondage qui a pour fonction de mesurer le pouls d'un échantillon de la population d'intérêt (ce qui est réellement observé). Cependant, ce que ce sondage mesure ne constitue pas tout à fait l'opinion publique elle-même, mais plutôt un segment populationnel qui se veut le plus souvent représentatif de l'humeur de l'opinion publique. Ceci est tout aussi vrai pour les sondages à petits échantillons que pour ceux utilisant des données massives. Autrement dit, la mesure et les données collectées ne représentent pas le phénomène --- l'opinion publique --- en soi.

On a déjà mentionné que la validité de la mesure a de l'importance puisqu'elle garantit que ce qui est mesuré représente réellement ce qu'on croit mesurer. Toutefois, pour être plus spécifique, dans une approche positiviste, la validité de la mesure se traduit généralement par une logique de classification des valeurs attribuées aux différentes manifestations distinctes d'un même phénomène. Par exemple, une mesure de la démocratie comme celle proposée par *Freedom House*, fréquemment utilisée en science politique, classifie les libertés civiles et les droits politiques des États du monde par degré afin de construire un index, ou une échelle, allant d'un autoritarisme complet à une démocratie parfaite. Les scores représentent, dans ce contexte, une mesure artificielle, mais ordonnée et logique, des idées contenues dans le concept de démocratie telles que libertés civiles et droits politiques. On peut ainsi dire que la question de la validité de la mesure est un élément central de ce qui unit (1) le phénomène social étudié (la démocratie), (2) son opérationnalisation (via les libertés civiles et droits politiques) et (3) la méthode de mesure utilisée pour observer et classifier d'une certaine façon le phénomène et les données qui en découlent (dans le cas de *Freedom House*, des codeurs travaillant de manière indépendante les uns des autres).

### La validité des données massives

En ce qui a trait aux données massives, la question de la validité de la mesure constitue un défi nouveau. Les données massives ont en effet comme avantage d'offrir aux chercheur.e.s soit de nouveaux phénomènes à étudier, soit de nouvelles manifestations et nouvelles formes à des phénomènes déjà étudiés. Les données massives permettent donc d'agrandir la connaissance scientifique.

L'étude de King et al. (2013) représente un cas éclairant de phénomène social que l'utilisation des données massives permet désormais d’étudier. En se basant sur la collecte de plus de 11 millions de publications en ligne, King et ses collègues ont pu mesurer la censure exercée par le gouvernement chinois sur ces réseaux sociaux. En utilisant des données massives nouvelles, les auteurs ont donc pu observer une manifestation inédite de censure massive qui, sans de telles données, serait probablement demeurée mal comprise d'une perspective scientifique. Le nombre de recherches basées sur l'utilisation des données massives similairement innovantes en sciences sociales est par ailleurs en croissance constante (Beauchamp 2017; Bond et al. 2012; Poirier et al. 2020; Bibeau et al. 2021).

Cependant, il faut aussi souligner que les données massives, en raison de leur complexité, peuvent avoir pour désavantage d'embrouiller l'étude des phénomènes sociaux. Les opportunités scientifiques liées aux données massives s'accompagnent en effet de certaines difficultés méthodologiques. Parmi ces difficultés, trois enjeux sont particulièrement cruciaux : (1) la validité interne, (2) la validité externe et (3) la question d'un changement de posture ou d'orientation épistémologique en sciences sociales causé par les données massives.

#### Validité interne des données massives

Premièrement, les données massives peuvent représenter un défi à la validité interne des études en sciences sociales en rendant pragmatiquement difficile l'établissement de ***mécanismes causaux clairs***. Ce défi est notamment une conséquence du fait que la plupart des données sont présentement issues d'un processus de génération (*data-generating process*) qui est hors du contrôle des chercheur.e.s. Les données massives proviennent en effet habituellement de sources diverses qui sont externes aux projets de recherche qui les utilisent. Elles ne sont pas donc générées de manière aléatoire sous le contrôle des chercheur.e.s.

Un des problèmes liés à cette situation est qu'il est difficile de garantir une source *exogène* de variation par laquelle les chercheur.e.s éliminent l'effet potentiel des facteurs confondants (*confounders*). Règle générale, la distribution aléatoire d'un traitement et d'un contrôle dans une expérience en laboratoire ou sur le terrain représente le standard le plus élevé permettant de fournir cette source exogène de variation, notamment parce qu’elle l’attribution aléatoire du traitement ou du contrôle est entièrement sous le contrôle du chercheur.e.s menant l’expérience. Cependant, en ce qui à trait à la plupart des données massives, elles sont générées de manière indépendante du contrôle du chercheur.e.s, et sont donc soumises aux mêmes enjeux et problèmes (biais) que les données observationnelles traditionnelles.

Pour le dire autrement, le défi de validité interne avec les données massives constitue un enjeu relatif à la qualité des données. Ce n'est évidemment pas un défi propre ou unique aux données massives. Ce défi s'applique également aux autres types de données. Cependant, dans l'état actuel des choses, le volume et la variété --- deux des 4V --- des données massives --- textuelles, numériques, vidéos, etc. --- peuvent miner la qualité de l'inférence causale entre une cause et une conséquence que permet habituellement un processus contrôlé de génération des données. En somme, la validité interne des données massives est une fonction de la qualité de ces mêmes données.

#### Validité externe des données massives

Deuxièmement, les données massives représentent aussi un défi important pour la validité externe des recherches en sciences sociales (Tufekci 2014; Lazer et Radford 2017; Nagler et Tucker 2015). Un des problèmes les plus évidents concerne la ***représentativité*** des données massives collectées.

Comme le soulignent Lazer et Radford (2017), la quantité de données, en soi, ne permet pas de corriger pour la non-représentativité des données. Les données massives sont ainsi soumises au même problème de biais de sélection que les autres types de données observationnelles, tels un sondage ou une série d'entrevues, traditionnellement utilisés en sciences sociales.

Le cas célèbre de l'erreur de prédiction du *Literary Digest* lors de la campagne présidentielle américaine de 1936 illustre bien ce problème. Lors de cette campagne, le *Literary Digest* a prédit à tort la victoire du candidat républicain Alf Landon sur le président démocrate sortant Franklin D. Roosevelt, puisque son échantillon de répondants surreprésentait les électeurs plus aisés, traditionnellement plus républicains, au détriment des électeurs moins aisés, plus généralement proches du Parti démocrate. Cette erreur de surreprésentation dans l'échantillon est due au fait que le *Literary Digest* a effectué un échantillonnage basé sur les listes téléphoniques et le registre des propriétaires de voitures, biaisant par le fait même l'échantillon au détriment des électeurs plus pauvres ne possédant pas de téléphone ou d'automobile, mais qui constituaient un électorat favorable à Roosevelt (Squire 1981). Le biais de sélection du sondage a ainsi sous-estimé le soutien populaire de Roosevelt de plus de 20 points de pourcentage.

Aujourd'hui, l'utilisation des données massives est soumise aux mêmes enjeux méthodologiques. L'accumulation massive de données ne permet pas de compenser pour la qualité des données. Les données massives, comme les données plus traditionnelles, sont soumises aux conséquences induites par le processus de génération des données (*data generating process*) comme un échantillonnage.

Toutefois, depuis quelques années, le développement de nouvelles méthodes de pondération des données offre des pistes de solutions. La grande quantité de données massives permet notamment d'appliquer des méthodes de pondération bien plus efficaces pour corriger les échantillons non-représentatifs (Wang et al. 2015).

### Données expérimentales

La question du processus de génération des données devient plus claire quand on considère comment les *données observationnelles* et les *données expérimentales* permettent d'effectuer des inférences de manière distincte (voir Figure 2). Toutefois, pour bien comprendre ce point, il faut comprendre les notions de données expérimentales et d’inférence causale, qui sont centrales au domaine de la causalité en recherche.

En quelques mots, l’essence de la démarche causale se résume comme suit : le processus de génération de données expérimentales a pour objectif d’assurer la validité d’une inférence causale estimée sur un échantillon sur l'ensemble de la population visée.

Plus spécifiquement, le processus de génération des données permet aux chercheur.e.s de s’assurer que la distribution du traitement entre les deux groupes, traitement et contrôle, est entièrement aléatoire. De manière technique, cette distribution aléatoire du traitement entre les deux groupes permet de garantir une source exogène (à l’opposé de endogène) de variation sur la variable indépendant (*x). Cette source exogène de variation permet, quant à elle, d'éliminer l'endogénéité entre la variable indépendante (*x*) et le résidu (*e\*).

Autrement dit, le fait de distribuer au hasard le traitement entre les membres du groupe traitement et ceux du groupe contrôle assure que la variation dans les résultats ne vient pas d'autres facteurs non-contrôlés (le résidu, *e*), mais plutôt du traitement lui-même (la variable indépendante, *x*). En distribuant le traitement de manière aléatoire, on s'assure que les différences dans les résultats sont vraiment dues au traitement et non à d'autres facteurs.

Il s’agit là d’assurer le respect de la condition d’indépendance, essentielle à la validité de l’identification de l’effet causal étudié. Autrement dit, en éliminant l’endogénéité entre la *x* et *e*, on s’assure que l'effet observé n'est pas dû à une variable confondante.

Pour revenir aux données massives, celles-ci ne peuvent pas résoudre les enjeux liés aux inférences causales ou explicatives (Grimmer, 2015). Elles sont en effet également soumises aux mêmes impératifs issus du processus de génération des données.

### Données observationnelles

En ce qui a trait aux données observationnelles, il y a deux points importants. Premièrement, des méthodes d'inférence basées sur des approches par design (*design-based methods*) comme une méthode de régression sur discontinuité ou de variable instrumentale peuvent également garantir des inférences explicatives et causales valides. Elles nécessitent toutefois plusieurs postulats plus restrictifs dont l'objectif est d'imiter ou de récréer, de la manière la plus fidèle possible, une distribution aléatoire du traitement -- ce que la littérature appelle un *as-if random assignment* (comme si l’attribution était aléatoire) (Dunning, 2008).

Dans un contexte observationnel, les données massives peuvent donc permettre d'augmenter la précision des estimations causales. Effectivement, comme dans un modèle de régression linéaire, plus l'échantillon est grand, plus l'estimation du coefficient causal ou probabiliste est précise. Par exemple, un échantillon large dans un modèle de régression sur discontinuité permet de restreindre la largeur de bande autour du seuil, garantissant ainsi une distribution presque parfaitement aléatoire des données et une validité plus élevée à l'estimation de l'effet causal.

Un autre exemple pourrait être l'utilisation du « matching », souvent utilisé dans les études économétriques. Supposons que vous souhaitez estimer l'effet d'un programme éducatif sur les résultats scolaires d’étudiant.e.s. Le devis de recherche idéal serait d’assigner aléatoirement les étudiant.e.s au programme (le groupe traitement) ou non (le groupe contrôle). Toutefois, puisque ce devis idéal peut être difficilement réalisable, un grand nombre de données pourrait permettre de trouver pour chaque étudiant.e dans le groupe traitement un étudiant.e « jumeau » dans le groupe contrôle. Ce « jumeau » serait similaire en âge, sexe, antécédents socio-économiques, etc. Il serait ensuite possible de comparer les résultats scolaires de ces jumeaux pour estimer l'effet du programme. Plus l’échantillon est grand, plus l’estimation sera précise et fiable, parce qu’il y aura plus de jumeaux possibles à apparier, réduisant ainsi le biais dû aux variables non observées.

Il s’agit d’un exemple où les données massives augmentent la validité interne de l'étude, même si les données sont de nature observationnelle et non expérimentale.

Deuxièmement, un échantillon de données massives observationnelles issues d'une plateforme comme X --- anciennement Twitter --- ou Facebook peut fournir une *description* plus fine de certaines dynamiques sociales observées sur les réseaux sociaux. Cependant, c'est la manière dont sont collectées les données de cet échantillon de données massives qui garantit la représentativité de l'échantillon --- avec pour objectif l’absence d’un biais de sélection --- et non pas la quantité de données. Généralement, le biais d'un échantillon est une conséquence de la non-représentativité des répondants; dans notre exemple, les utilisateurs des médias sociaux ne sont généralement pas représentatifs de la population entière.

Dans un tel cas, des méthodes de pondération sur des données observationnelles peuvent compenser pour la sur- ou la sous-représentativité de sous-groupes dans un échantillon afin d'assurer la validité de l'inférence entre échantillon et population. Les données massives ont ici une importance puisqu'une pondération fiable nécessite une quantité substantielle d'observations. Une pondération *a posteriori* sera donc plus fiable plus l'échantillon est grand. Les données massives ont ainsi une valeur ajoutée afin d'établir des inférences descriptives plus précises et sophistiquées.

### Validité écologique et observation par sous-groupes

Les données massives peuvent aussi jouer d'autres rôles importants relatifs à la validité externe. Premièrement, les données massives facilitent effectivement la validité externe de certaines études en accroissant la validité écologique (*ecological validity*) des tests expérimentaux, c'est-à-dire le réalisme de la situation expérimentale (Grimmer, 2015: 81). En effet, la variété des sources et des formats de données permet aux chercheurs d'imiter plus fidèlement la réalité sur le terrain vécue par les participants aux études.

Deuxièmement, la quantité importante de données rend possible l'observation d'effets précis, spécifiques et inédits par sous-groupes (Grimmer 2015: 81). Alors qu'auparavant, la taille réduite des échantillons ne permettait pas d'effectuer des inférences valides pour des sous-groupes de la population --- les écarts-types par sous-groupes étaient trop grands, rendant difficile l'estimation précise d'un paramètre comme la moyenne et impossible celle d'un coefficient ---, la taille énorme des échantillons de données massives permet aux chercheurs d'estimer des paramètres qui étaient demeurés extrêmement imprécis jusqu'à aujourd'hui. Notre compréhension des phénomènes sociaux s'en trouve par le fait même approfondie de façon considérable.

![image2_2](images/chapitre1_tableau.png)

## Conclusion : trois questions ouvertes pour le futur

Comme nous venons de le voir, la quantité et la variété nouvelle des données massives permettent à la fois un approfondissement de l'analyse de certains phénomènes et l'ouverture de nouvelles avenues de recherche. L'analyse des données massives peut permettre de mettre en lumière des tendances subtiles échappant aux ensembles d'informations plus restreints.

Il faut toutefois souligner que les données massives représentent une complexification de l'analyse des phénomènes en sciences sociales d'une perspective non pas seulement méthodologique/technique mais également épistémologique.

Cela soulève au moins trois questions d'importance, dont les réponses ne nous sont pas encore accessibles, pour l'avenir de la recherche en sciences sociales : (1) les données massives entrent-elles (partiellement du moins) en conflit avec l'impératif de parcimonie qui caractérise la science moderne?; (2) ces données sont-elles dans la continuité ou représentent-elles une coupure dans la tradition béhavioraliste en sciences sociales (et en science politique tout particulièrement)?; (3) et finalement, de manière reliée, les données massives proposent-elles ou non une manière de dépasser l'individualisme méthodologique qui caractérise les sciences sociales contemporaines?
