# Outils de gestion de flux de travaux: la méthode Agile pour le monde académique {#sec-chap3}

\begin{center}
\textit{Adrien Cloutier\footnotemark[1]}
\end{center}

\footnotetext[1]{Université Laval}

À l'ère du numérique, le monde de la recherche est en fulgurante transformation. Les outils de recherche sont plus nombreux que jamais et l'écart dans la littératie numérique des chercheurs, comme dans la population générale, se creuse chaque année. Avant même de se lancer dans la recherche scientifique -- mais ceci est vrai pour tous les domaines d'emplois -- les travailleurs qui prennent le temps de s'intéresser et de réfléchir à la gestion de leur « workflow » (l'OQLF recommande la traduction « flux de travaux », qui sera utilisée dans ce chapitre pour représenter la large catégorie des outils numériques permettant d'augmenter l'efficacité et la productivité), en visant l'apprentissage et l'utilisation des meilleurs outils numériques sur le marché, sortiront avec un avantage compétitif énorme par rapport à leurs collègues.

Dans cette quête incessante de l'optimisation du travail, la réflexion sur vos méthodes de travail et l'utilisation des bons outils devient une clé de la réussite. Que vous soyez un chercheur en herbe ou un professionnel chevronné, la manière dont vous organisez votre flux de travaux et gérez vos ressources peut déterminer la qualité, la quantité et l'impact de vos résultats professionnels.

<!-- Attention cependant: à notre époque, se perdre dans un monde infini d'outils, dont la constante évolution est bien souvent menée par des entreprises désireuses de fidéliser une clientèle en lui offrant toujours plus de possibilités peut, ironiquement, entrainer une contre-productivité. Il est, dès lors, essentiel de réfléchir à ses besoins, et de cibler les outils pertinents à son travail, sans exagération. Viser à tout savoir, tout connaitre des dernières tendances et innovations sur le marché de l'outil numérique peut être aussi fastidieux et prenant que de ne pas en utiliser. Nombreux sont les chercheur.e.s qui perdent une quantité impressionnante de leur temps à passer d'un outil à l'autre, coincé dans la peur de ne pas être à la fine pointe des dernières tendances. Dans une recherche incessante de la dernière mode. -->

<!-- Ce chapitre vise à offrir une voie de passage structurée; une classification logique des différents « types » d'outils de gestion de flux de travaux, dans l'objectif de calmer la *FOMO* (*Fear of missing out*: expression anglophone représentant la peur de ne pas tout savoir, d'être en retard sur les tendances, sur les connaissances et les nouvelles opportunités). Si les outils numériques changent et évoluent, les méthodologies derrière demeurent. -->

Loin de souhaiter proposer *LA* bonne manière de travailler et de structurer son flux de travaux, ce chapitre (et ce livre dans sa globalité) fait le choix de présenter de manière plus générale les différentes méthodes, l'historique des outils, puis d'en présenter les leaders, avant d'entrer dans une présentation détaillée des « choix éditoriaux » de notre équipe. Ces choix sont le fruit d'une utilisation sérieuse et réfléchie de ces outils sur plusieurs années, avec la constante compréhension qu'il existe autant d'outils et d'utilisation pertinents qu'il existe d'individus et de besoins.

Autrement dit, la gestion optimale d'un flux de travaux est propre à vous! Votre personnalité, vos qualités, vos défis, vos connaissances, votre emploi du temps et vos compétences sont autant de facteurs qui peuvent vous mener à préférer un outil plutôt qu'un autre, à choisir une méthode de travail particulière plutôt qu'une autre. Si vous préférez travailler seul, certains outils vous seront bien davantage utiles. Si vous connaissez la suite Microsoft par coeur, y demeurer pour la gestion de vos tâches peut être efficace. Par contre, si vous êtes du genre à collaborer, d'autres outils offriront à vous et vos collègues une optimisation de votre communication et de votre gestion de projet. Votre flux de travaux est unique à vous, à vos besoins, mais ce chapitre vous permettra d'y réfléchir et de le structurer.

Il est utile de noter qu'à l'ère du numérique et de la multiplication des technologies de l'information et de la communication, le travail collaboratif est plus simple et nécessaire que jamais. L'époque où il était plus naturel pour les chercheurs de travailler seul, isolés avec leurs livres, leurs données et leurs idées, est loin derrière. Les outils de gestion de projet comme *Notion*, *Trello* ou *Asana* permettent de diviser les activités et d'en suivre les progrès, et les outils de gestion de données comme *git*, *GitHub*, *Google Drive* ou *Dropbox* d'en partager les codes, les fichiers textes, les données et les résultats avec le monde entier. La philosophie et les valeurs du logiciel libre et du code source ouvert s'inscrivent désormais facilement à toutes les étapes du processus scientifique.

<!-- Si vous travaillez actuellement seul, ce chapitre vous recommandera d'emblée de vous intéresser aux groupes, centres ou chaires de recherche dans votre champ. Vous constaterez que de nombreux universitaires partagent vos champs d'intérêt, et recherchent la collaboration. Des réseaux existent autour de vous, à l'intérieur de votre université, mais également à l'international. La coopération scientifique n'a plus de frontière. Les outils numériques permettent à notre époque de créer des synergies internationales, et d'augmenter rapidement votre production en échangeant des visions et en partageant des responsabilités. Si ce n'est pas déjà le cas, plus tard dans votre carrière, vous serez mené à travailler en équipe. Il vaut mieux, dès maintenant, développer votre réseau et en maitriser les codes et les outils. -->

Un flux de travaux médité, personnalisé et collaboratif construit en début de carrière tracera la voie d'une efficacité optimisée des années durant, créant et augmentant sur le court, moyen et long terme votre « valeur » sur le marché professionnel.

Voici un secret: dans le milieu universitaire, des capacités en gestion de flux de travaux vous permettront de vous distinguer, d'augmenter rapidement votre valeur en présentant des compétences rares et recherchées. Ceci est vrai dans tous les domaines, mais comme étudiant et professionnel aux études supérieures, à la recherche de moyens de sortir du lot, entouré d'une multitude de collègues brillants qui s'illustrent académiquement, qui s'imposent par leurs expertises techniques et intellectuelles, leur travail acharné et leur volonté d'être remarqué, votre savoir-faire en gestion de projet pourrait bien faire la différence.

Il existe particulièrement six grandes « fonctions » à maitriser pour opérer dans le milieu académique: la communication, la gestion de partenaires, le développement, l'enseignement, le financement, et la publication. Tout professeur doit jongler presque quotidiennement avec ces fonctions. Dans ses tâches, il est attendu d'enseigner, il en va de soi, mais également de publier des études scientifiques, de travailler avec sa communauté, de développer des projets novateurs et porteurs, d'encadrer des étudiant.e.s à la maitrise et au doctorat, de trouver le financement nécessaire pour mener ses recherches et d'en communiquer les résultats. Un professeur doit faire tout cela, minimalement, en plus de participer à la vie administrative de son département, de diriger des groupes, des centres ou des chaires de recherches, et plus encore.

Rares sont les professeurs, les étudiants ou les professionnels à parfaitement maitriser chacune de ces six fonctions. Certains vont s'illustrer en enseignement, d'autres seront particulièrement doués dans la recherche et la publication scientifique. Il y a celles et ceux que l'ont lit dans les journaux, écoute à la radio ou regarde à la télé, vulgarisant leur expertise au grand public. Quelques-uns.s sont habiles pour remplir des demandes de financements pour leurs travaux de recherches, alors que des scientifiques préfèrent créer des partenariats avec des entreprises de la société civile et développer des projets pour leur collectivité, enregistrant des brevets et quelques fois même, transformant des idées en *start-up*.

Malgré les talents particuliers et les préférences, dès l'entrée aux cycles supérieurs, le travail sur ces six fonctions mené en parallèle n'est pas optionnel. Il existe cependant une septième fonction, transversale aux six autres, plus rarement perfectionnée et mise de l'avant: la capacité de gestion de cet immense flux de travaux. Cette septième fonction permet de diriger simultanément les six autres fonctions pour créer un tout cohérent, efficace et productif. De multiples projets, publications, contributions peuvent alors être menées parallèlement ou conjointement, en accomplissant de multiples pierres d'un coup. Bien gérées, des équipes comptant des dizaines d'étudiants, professeurs et professionnels peuvent collaborer, se partager des tâches et miser sur les talents particuliers de leurs membres pour offrir des résultats supérieurs en qualité et en quantité.

L'exercice de cette septième fonction est le secret bien gardé de grands talents de notre société. Elle vous permettra d'émerger et de vous faire remarquer à l'intérieur d'un immense bassin de personnes qualifiées, voire surdouées, car peu y accorde le temps et l'énergie nécessaire. Pour comprendre comment développer une expertise en gestion de flux de travaux, ce chapitre présente trois sous-catégories d'outils:

1.  Les outils de gestion de projet;
2.  Les outils de gestion de données;
3.  Les outils de gestion de la communication.

Chacune de ces « sous-catégories » d'outils de gestion de flux de travaux vous sera d'abord présentée, puis des exemples d'utilisation seront offerts. Rappelez-vous cependant: bien que nous vous présentions des formes possibles d'utilisation, il existera toujours autant de bonnes façons de travailler qu'il existe d'individus uniques, de contextes ou d'équipes. Ces exemples sont donc offerts uniquement à des fins d'inspiration. Au bout du compte, la réflexion sur votre flux de travaux vous appartient. L'important est que vous preniez le temps nécessaire pour mener cette réflexion.

## Point d'observation : retour sur l'origine de la recherche de productivité

Avant le début du XXe siècle, « la seule façon d’améliorer la productivité était d’exiger du travail plus dur et des heures plus longues de la part des travailleurs », écrit la compagnie *Microsoft* dans un « [Bref historique de la gestion de projet](https://support.microsoft.com/fr-fr/topic/bref-historique-de-la-gestion-de-projet-a2e0b717-094b-4d1e-878a-fcd0978891cd) », publié sur son site Web. Tout change avec les travaux scientifiques de l'Américain Frederick Taylor (1856-1915), considéré comme *le père de la gestion scientifique*, au début du XXe siècle. Taylor a démontré que la gestion de projet était en soi une science que l'on peut étudier, théoriser, comprendre, pour permettre l'innovation. Son associé, Henry Gantt (1861-1919), a légué son nom à l'une des visualisations de suivi des étapes de production les plus populaires au monde, encore à ce jour. Le *diagramme de Gantt* permet depuis plus d'un siècle de projeter les différentes phases de production d'un projet sur une ligne du temps, en représentant les dépendances entre elles, de manière à prévoir leurs séquences de réalisation. *Microsoft Project* intègrera dans les années 1990 le diagramme de Gantt, pour la première fois dans un outil numérique, popularisant l'outil auprès du grand public et ouvrant une nouvelle ère de réflexion sur la productivité.

Des pyramides aux chemins de fer, de multiples solutions de gestion ont été développées au cours des siècles, mais l'absence des technologies de l'information et de la communication n'a pas permis avant les années 1950 de structurer et de populariser les idées. En 1958, à la demande de la marine américaine, le *Program Evaluation and Review Technology* (PERT) voit le jour avec comme objectif de structurer le développement du programme de missiles balistiques nucléaires des États-Unis, alors en retard sur celui de l'Union soviétique. La méthode PERT est toujours utilisée aujourd'hui pour visualiser les relations entre tâches, coûts et délais.

À partir de ce succès, les entreprises modernes à la recherche de profits ont vite compris les avantages liés à l'optimisation du travail. Le « Project Management Institut » (PMI) est lancé en 1969 aux États-Unis, structurant et distribuant un langage désormais universel pour la gestion de projet, basé sur l'idée d'une « méthodologie » bien définie. Le PMI décrit la méthodologie comme « un système de pratiques, de techniques, de procédures et de règles utilisé par ceux qui travaillent dans une discipline ». De nombreuses méthodologies de gestion de flux de travaux se sont développées au cours des dernières décennies, dans une course vers la *recette secrète* de la productivité d'entreprise. Tous les outils numériques de gestion de flux de travaux contemporain mettent de l'avant une ou plusieurs de ces différentes méthodologies.

Aujourd'hui, ces méthodes sont généralement présentées en deux grandes catégories: les méthodes dites « traditionnelles », comme celles présentées ci-dessus (en plus des méthodes Waterfall, PRINCE2, etc.), et les méthodes « Agiles ». L'Agilité voit le jour en 2001, avec la publication du [*Manifeste Agile*](https://agilemanifesto.org/iso/fr/manifesto.html), conceptualisé et destiné au nouveau domaine en pleine expansion du développement logiciel. 

Les auteurs se sont toutefois bien retenus d'en inscrire une définition précise, misant plutôt sur une séries de valeurs et de principes qui se doivent d'être partagés par tous les membres d'une équipe dite « agile ». C'est peut-être cette décision qui permit à l'Agilité de traverser le temps et de s'inscrire davantage comme une grande philosophie du travail, pouvant être facilement adaptée à divers contexte parfois bien éloigné du développement logiciel, plutôt que comme une méthodologie aux règles strictes.

Ainsi, pour être considéré Agile, un respect des valeurs et des principes demeure essentiel. Contrairement aux méthodes traditionnelles privilégiant la planification du « haut vers le bas » (des décisions prises par les dirigeant.e.s, qui envoient les ordres à accomplir à l'intérieur d'un budget et d'un calendrier), la gestion de projet Agile mise sur des cycles itératifs courts, des équipes autonomes et disciplinées, dans une logique du bas vers le haut, basée sur les besoins réels de la clientèle.

Pour illustrer la différence entre les deux approches, un exemple fréquemment donné est celui du logiciel *Excel* de *Microsoft*, dont le développement était, à l'origine, issu d'une méthode traditionnelle de gestion de projet. La compagnie rendait disponible à l'utilisateur des versions créées à l'interne, par des développeurs respectant des budgets, des calendriers et des directives provenant de leur gestionnaire. Dans les années 1990 et 2000, le public a ainsi connu Excel 97, puis Excel 2000, et Excel 2007. Il est régulièrement cité qu'environ 70% à 80% des fonctionnalités d'Excel ne sont que rarement utilisées par le consommateur, alors qu'elles ont pris des années à être développées.

La logique de la gestion de projet Agile est inversée: un logiciel se construit sur de courtes périodes *itératives*, en offrant à chaque fin d'itération un nouvel *incrément* du produit en fonction du besoin communiqué par l'utilisateur. Ainsi, *Apple* propose des mises à jour régulières de ses logiciels, réglant des *bugs* ou offrant un nouveau service attendu de sa clientèle. La très forte majorité des entreprises informatiques ont aujourd'hui délaissé les méthodes traditionnelles pour se retourner vers le travail Agile, *itératif et incrémental*. 

Si ces philosophies de gestion de projet peuvent sembler à première vue en complète opposition, elles offrent chacunes des avantages et des inconvénients qui peuvent encourager le développement d'une utilisation mixte, précisément adaptée aux besoins de son utilisateur. Et si ces méthodes ont été développées pour optimiser la production en entreprise ou le développement logiciel, elles sont aujourd'hui répandues dans toutes les sphères de travail de la société. Au Québec, par exemple, plusieurs municipalités, caisses, milieux d'enseignement et entreprises de tous les secteurs opèrent actuellement un virage *Agile*. Chaque année en octobre, depuis déjà plus de 15 ans, la ville de Québec accueille [l'*Agile Tour*](https://www.agilequebec.ca/fr/), grand évènement organisé par l'organisme *Agile Québec* lors duquel se rencontrent plus de 700 adeptes de l'Agilité pour « partager leurs savoirs, leurs expériences et leurs dernières découvertes ».

Les « méthodes Agiles », grande famille toujours en expansion dont tous le membres (méthodes Scrum, Kanban, XP, etc.) ont en commun la « philosophie » Agile, ont le vent dans les voiles. Leur popularité prouve que la gestion de projet ne se limite pas au déploiement de nouvelles technologies. Si les entreprises et les administrations ont toutes le réflexe de mettre en place une méthode de gestion de projet, peu d'universitaires prennent le temps de réfléchir à leur processus afin d'optimiser leur production (c'est-à-dire le rapport entre le temps investi, la qualité, la quantité et la pertinence de leurs travaux scientifiques). 

Dans ce contexte, certains qualifient l'Agilité de « mode », davantage utile pour des exercices de communication publique et d'image d'entreprise, plutôt que pour une réelle recherche d'efficacité et d'optimisation. L'Agilité a été (et est toujours) galvaudée et utilisée à toutes les sauces. Mais les valeurs et les principes du manifeste de 2001 demeurent. Sans un respect et une bonne compréhension de ceux-ci, une équipe échouera dans sa mise en place d'une stratégie agile. Lorsque bien réfléchie et adaptée à son contexte, la philosophie Agile a maintes fois fait ses preuves.

La suite de ce chapitre propose une marche à suivre Agile, adaptée au milieu académique, ainsi que des outils pour développer et appliquer une gestion du flux de travaux efficace, utile à la fois pour les individus travaillant seuls, mais particulièrement adaptée pour les équipes de recherche.

## Les outils de gestion de projet

Comme le logiciel libre, l'Agilité est avant tout une philosophie. Une façon de percevoir son travail, son temps, la division de son travail, les relations et l'implication de son équipe, afin de produire des livrables structurés qui répondent à des besoins concrets. Cette philosophie repose sur quatre grandes valeurs fondamentales visant à valoriser:

1.  **Les individus et leurs interactions**, de préférence aux processus et aux outils;
2.  **Des solutions opérationnelles**, de préférence à une documentation exhaustive.
2.  **La collaboration avec les clients**, de préférence aux négociations contractuelles;
3.  **L'adaptation au changement**, de préférence au respect d'un plan;

Dans le contexte du milieu académique, cela pourrait se traduire par:

1.  **Des échanges constructifs avec la direction et les pairs**, de préférence aux apprentissages isolés des méthodologies et des outils de recherche;
2.  **Des résultats tangibles et réguliers**, de préférence à une idéation interminable;
3.  **Une contribution scientifique utile à la société et à l'avancement des connaissances**, de préférence à un acharnement intellectuel aussitôt tabletté;
4.  **L'adaptation aux nouvelles découvertes et aux nouvelles idées**, de préférence à l'application rigide du devis de recherche initial.

Loin de dire que l'apprentissage d'outils n'est pas utile en Agilité, la première valeur rappelle que la recherche académique est avant tout une aventure collaborative lors de laquelle les discussions avec les collègues permettent de résoudre de nombreux défis, de comprendre les contributions recherchées et de créer des publications ou projets pertinents et de qualité. L'utilité des méthodes de travail et des outils est optimisée à l'intérieur d'une équipe engagée.

La deuxième valeur présente le coeur de l'application concrète de l'Agilité: un projet Agile doit être divisés, dès le départ, en de nombreux livrables, chacun avec une date d'échéance déterminée. Plutôt que de prendre des mois, voire des années à rédiger un devis --- qui sera largement modifié dans tous les cas, la version finale d'une thèse n'a bien souvent rien à voir avec son idéation initiale --- un projet Agile produit une grande quantité de petits « incréments », qui permettent de le faire avancer concrètement. Chaque livrable (une petite partie du projet, par exemple « 2 pages de la revue de littérature ») peuvent ensuite être évaluée, adaptée et potentiellement permettre une réévaluation du devis initial. 

La troisième valeur répond à un impératif de la science: offrir des contributions originales répondant à des lacunes de la littérature. Le chapitre 4 traitera de cet aspect et présentera des outils pour réaliser des revues systématiques de la littérature, de sorte à circonscrire un champ d'études et à repérer les failles et apports potentiels. La quatrième valeur présente le  En recherche, il est essentiel d'accepter que les résultats puissent dévier des hypothèses initiales, et être capable de réajuster son plan en fonction de l'évolution des données et de nouvelles orientations méthodologiques ou théoriques.

Le respects de ces trois valeurs permet ainsi l'application de la dernière: un projet Agile divisé en de multiples petits livrables peut et doit être révisé régulièrement. Des résultats concrets permettent des rétroactions fréquentes, qui viendront alimenter la réflexion sur le devis initial au fur et à mesure de l'évolution parallèle de la science, de la technologie, des méthodologies, de l'arrivée de nouvelles questions de recherche et de nouveaux besoins de la société.

Ces valeurs ouvrent la porte à la méthode de travail proposée dans ce chapitre pour appliquer concrètement l'Agilité au contexte académique. Il s'agit d'une proposition unique, testée et adaptée à l'intérieur de plusieurs équipes de recherche. Il est à noter ici que l'Agilité se divise en de multiples méthodologies, telles que Scrum, Kanban, XP, Crystal, Lean, etc. Ce chapitre proposera l'application conjointe des approches Scrum et Kanban, particulièrement adaptée et utile au milieu académique, et qui reposent sur les caractéristiques suivantes:

1.  Une cadence de travail soutenue grâce à un cycle de **Sprints**;
2.  Un suivi régulier des objectifs grâce à de courts **Scrums**;
3.  Un avancement **itératif** et **incrémental** pour des **livraisons** régulières de petites parties fonctionnelles;
4.  Une division du travail en **Stories** à l'intérieur d'un tableau **Kanban**.

```{r, out.width="80%"}
#| label: fig-scrum
#| echo: false
#| fig-cap: "Cycle d'application du processus Scrum, une approche Agile."
knitr::include_graphics("images/chapitre8_agilite.png", dpi = 600)
```

La @fig-scrum présente le cycle traditionnel d'application de l'approche Scrum dans un contexte de travail collaboratif. Dans le cadre académique, toutes ces composantes peuvent facilement être adaptées.

Le processus commence par l'organisation d'une « planification de sprint ». Un sprint dure généralement entre une et quatre semaines (dans le contexte du développement logiciel, une semaine peut être parfois très utile lorsque les délais sont serrés). Dans le contexte académique, un sprint de trois ou quatre semaines s'avèrent plus adaptés, voire de 2 ou 3 mois dans le cas d'une rédaction de thèse. Lors de la planification, toute l'équipe se rencontre quelques heures (généralement 2h ou 3h, en fonction du nombre de projets en cours et/ou des publications en rédaction) pour déterminer les grands objectifs qui devront être réalisés, projet par projet, publication par publication, d'ici la fin du sprint. Un sprint se conclut toujours par le retour en plénière pour la planification du sprint suivant. Cette nouvelle planification de sprint doit désormais, et dorénavant, débuter par une *revue de sprint*, lors de laquelle les objectifs du sprint terminé sont évalués, projet par projet, publication par publication, puis par une *rétrospective*, où les processus cette fois sont évalués. L'équipe est alors invitée à partager son avis sur la méthode de gestion de projet, afin d'adapter le processus au contexte particulier et à renforcer son utilité.

La planification de sprint est menée par le *Scrum Master*, un membre désigné par l'équipe pour faire le lien entre le *Product Owner* (dans notre contexte, la direction de l'équipe de recherche) et tous les membres de l'équipe qui travaillent concrètement sur les projets et publications (les étudiants, par exemple). Dans le langage universitaire, le Scrum Master pourrait représenter le rôle de *coordonnateur*. Il est toutefois recommandé que le Scrum Master détienne sa certification de Scrum Master, ou ait du moins suivi sa formation. De nombreuses entreprises offrent des formations au Québec pour des prix variant entre 1000\$ et 3000\$, pour 2 à 3 jours de formations. [AFI U.](https://www.afiexpertise.com/fr/formation/preparation-a-la-certification-scrum-master-1785), par exemple, offre une formation de préparation à la certification Scrum Master deux fois par année. La certification, quant à elle, coûte 100\$, et peut être obtenue après un examen via le site Web officiel [scrum.org](https://www.scrum.org/professional-scrum-certifications/professional-scrum-master-assessments). Une formation Agile comme Scrum Master a une énorme valeur sur le marché de l'emploi. De plus en plus d'entreprises au Québec sont à la recherche de Scrum Master certifiés, offrant des salaires dépassant les 100 000\$ par année. 

Chaque projet est dirigée par un.e *chargé.e de projet*, dont le rôle est de veiller à la coordination de son équipe. Le chargé de projet organise les rencontres d'équipe au besoin, divise les tâches entre les membres, prend la parole lors de la planification de sprint pour présenter la revue du sprint, et annoncer les prochains objectifs prévus, qui seront ensuite discutés en groupe. Quand tous les objectifs pour tous les projets et pour toutes les publications ont été clairement établis, approuvés par l'ensemble les membres, en considération du temps que tous sont réalistement en mesure d'accorder, puis après avoir déterminé la date de la prochaine planification de sprint, la séance est levée. Une activité sociale d'équipe est à ce moment fortement encouragée!

Pour assurer un suivi régulier de l'avancement des objectifs, le Scrum Master met à l'agenda un ou deux *scrums* par semaine. Il s'agit de très courtes rencontres où l'ensemble de l'équipe se retrouve, idéalement entre 15 et 30 minutes maximum, selon le nombre de projets, et lors desquelles chaque chargé de projet doit répondre à trois questions:

1.  Qu'est-ce qui a été accompli par mon équipe depuis le dernier scrum?
2.  Qu'est-ce qui sera fait d'ici le prochain scrum?
3.  Y a-t-il des blocages?

Tour à tour, le Scrum Master nomme les projets et les publications, dans leur ordre de priorité, et offre la parole aux chargés de projet pour résumer les avancées de leur équipe. Contrairement à la planification de sprint, la direction n'a pas besoin d'assister à ces scrums. Le *Scrum Master* s'assure, si nécessaire, de la tenir informé des bloquants ou des ajustements à mener au calendrier ou aux prochains livrables. Les scrums sont l'occasion pour l'équipe d'assurer un suivi régulier des *livraisons* attendues d'ici la fin du sprint.

L'une des caractéristiques fondamentales des méthodes Agiles est le développement des projets de manière *itérative* et *incrémentale*. L'itération est le processus répété et cyclique mené grâce aux sprints. Une équipe Agile est en tout temps en sprint, jusqu'à la livraison finale du projet ou de la publication (si fin prévue il y a). Un incrément est la réalisation d'une petite partie dite « fonctionnelle » du projet, qui peut être soumise à évaluation. Dans le cadre académique, un incrément pourrait être la remise d'une première version d'une revue de littérature, la réalisation d'une première étape d'un code R pour l'analyse de données de thèse, le premier jet d'un devis, etc. Plutôt que d'attendre à la toute fin du processus pour le dépôt complet d'un projet, une équipe Agile divise son projet en de multiples itérations, qu'elle soumet pour évaluation à chaque planification de sprint lors de la phase de la *revue de sprint*. Par une démonstration, toute l'équipe peut alors constater et discuter du nouvel incrément proposé, par exemple la présentation de la section méthodologie d'un article scientifique, et déterminer ensuite les objectifs du prochain sprint pour la livraison de l'incrément suivant, disons la collecte de données. Tout projet est ainsi divisé en phase, en « séquence de développement », d'un sprint à l'autre, de manière itérative, de sorte à livrer des incréments fonctionnels du projet qui peuvent être discutés et révisés.

Contrairement aux méthodes traditionnelles, il est essentiel de comprendre que les méthodes Agiles se nomment « Agile » pour une raison: elles encouragent la flexibilité. Plutôt que de passer d'un sprint à l'autre en suivant un plan rigide déterminé des mois, voire des années auparavant en fonction d'un budget et d'un calendrier coulés dans le béton, les méthodes Agiles mettent de l'avant le « cycle » de travail, les *itérations*, qui permettent de revoir le plan, de l'adapter en cours de route, particulièrement lors de la planification de sprint. Normalement, lorsque les objectifs de sprint ont été validés par l'équipe, il n'est pas recommandé de les changer en cours de sprint. Par contre, de retour en *planif*, chaque membre de l'équipe a son mot à dire sur la suite; sur ce qui fonctionne bien, sur ce qui devrait être adapté, toujours avec le projet final en tête.

Pour tout projet ou publication menés seuls, comme la rédaction d'un mémoire, d'une thèse, ou la création d'un grand album de photos familiales, ou pour tout besoin de gestion de vie personnelle et d'objectifs d'avenir, ce processus n'est pas bien différent. Une personne Agile pourrait, le soir du jour de l'an, mettre sur papier un certain nombre de résolutions. Elle peut ensuite découper son année en 12 sprints de 4 semaines, et répartir l'accomplissement de ses résolutions en 12 incréments. Chaque incrément planifié doit être réaliste et faire avancer, petit à petit, les objectifs vers la livraison finale, à la fin de l'année. Une fois aux quatre semaines, cette personne peut prendre un moment seul de *planification de sprint* pour faire la *revue* des quatre dernières semaines et réviser les objectifs à atteindre pour les quatre semaines suivantes.

L'Agilité ne s'applique pas uniquement au monde du développement logiciel, voire à une grande équipe académique. Elle est d'abord et avant tout une philosophie, possible d'être menée, réfléchie et appliquée à son quotidien.

L'efficacité et les nombreuses possibilités qu'offre cette méthode ont généré, bien entendu, un marché très lucratif d'outils numériques. La section suivante présentera les leaders du marché, et offrira des instructions sous forme de conseils pour l'utilisation adaptée de ces outils, notamment par la création de *stories* à l'intérieur d'un tableau *Kanban*.

### Arpentage et choix éditorial: pourquoi utiliser un outil de gestion de projet

Tous les outils numériques de gestion de projets actuels (ou presque) sont conçus sur les enseignements des méthodes Agiles. Pour le contexte académique, ce chapitre propose en premier lieu l'application de l'approche Scrum, détaillé précédemment. Une seconde approche Agile, la méthode (et l'outil) Kanban, se combine aisément et efficacement à l'approche Scrum et au travail scientifique. Le Kanban est l'outil par excellence mis de l'avant dans les très populaires logiciels de gestion de projet *Trello*, *Jira*, *Monday*, *Asana* et *Notion* (cette liste n'est pas limitative. À ce jour, il existe des dizaines de logiciels de gestion de projets tout aussi populaires les uns que les autres comme *ClickUp*, *Wrike*, *Basecamp*, et même la plateforme de communication *Slack*, qui propose depuis 2024 sa solution de gestion de projet pour répondre aux attentes en gestion de projet de ses clients).

Offert de base dans tous les outils numériques nommés ci-dessus, le Kanban permet d'intégrer et de visualiser, dans un tableau comptant au minimum 3 colonnes (« À faire », « En cours », « Terminé »), les « tâches » à réaliser dans une échéance pour avancer la réalisation des objectifs de sprint. En Agilité, une tâche est nommée *Story*. Une story est un livrable, un morceau du projet (le « quoi »), pris en charge volontairement ou délégué à un membre de l'équipe par le chargé de projet. On utilise le terme story pour distinguer de la rapide tâche; une story peut quelques fois comporter plusieurs tâches (le « comment »).

Voici un exemple d'application de tous les concepts Agile vu jusqu'à présent. Dans le milieu académique, un *projet* pourrait être la création d'une étude scientifique. La création de cette étude pourrait être divisée en dix *sprints* de quatre semaines, présentant ainsi dix *livrables* clairs qui permettent un suivi en planification de sprint (bien sûr, certaines études peuvent prendre des années à aboutir. Certaines collectes de données sont plus longues, mais là n'est pas le point: toute étude menée en gestion de projet Agile peut être réfléchie d'emblée en fonction de son contexte et de ses objectifs, et être divisée en un nombre logique et réaliste de sprints, que ce soit 5 ou 15):

-   1er sprint: Idéation et structuration d'une question de recherche;
-   2e sprint: Première version d'une revue de littérature;
-   3e sprint: Rédaction du devis, choix théoriques, enregistrement des hypothèses, demande éthique;
-   4e sprint: Préparation méthodologique, raffinement de la revue de littérature;
-   5e sprint: Collecte des données;
-   6e sprint: Analyse des données;
-   7e sprint: Rédaction des résultats;
-   8e sprint: Rédaction de l'étude complète (introduction, discussion);
-   9e sprint: Révision du texte;
-   10e sprint: Soumission.

En divisant un projet de cette manière, les ressources nécessaires peuvent être réfléchies et réparties à l'avance (par exemple: il faudra engager deux auxiliaires de recherche pour la collecte de données du 5e sprint). Tous les objectifs de sprint deviennent évidents et prévisibles, et chaque story peut être rédigée pour accomplir ces objectifs.

Un outil numérique comme *Notion* peut ensuite être utilisé pour créer et remplir un Kanban, qui se compose ici de quatre colonnes:

1.  **À venir**: c'est le *backlog* (le « registre des tâches ») du projet. Toutes les stories qui seront à réaliser pour chaque objectif lors de sprints futurs s'y trouvent déjà, en ordre, de sorte à visualiser le travail précis « à venir »;
2.  **À faire**: c'est le *backlog* du sprint. Toutes les stories qui sont à réaliser à l'intérieur du sprint en cours s'y trouvent, avec une date d'échéance et une assignation;
3.  **En cours**: Quand une story du sprint est engagée, le membre de l'équipe à qui elle est assignée doit la déplacer dans cette colonne, pour signaler son avancement;
4.  **Terminé**: Toutes les stories du sprint en cours qui sont accomplies y apparaissent.

```{r}
#| label: fig-kanban
#| echo: false
#| fig-cap: "L'outil de gestion de projet Notion propose d'emblée des modèles de Kanban pour la création et le suivi des stories."
knitr::include_graphics("images/chapitre8_kanban_notion.png", dpi = 600)
```

En sortant de la première planification de sprint, l'équipe de recherche se retrouve face à des objectifs clairs, réalistes, divisés en stories rédigées dans un Kanban bien rempli. Ne reste plus qu'à se mettre à la tâche, et à se retrouver lors des scrums hebdomadaires pour assurer le suivi.

À la fin du sprint, lors de la rencontre de planification du sprint suivant, une équipe Agile débute toujours par la *revue de sprint*, qui permet de s'assurer que toutes les stories qui avaient été déplacées, il y a quatre semaines, de la colonne « À venir » à « À faire », se trouvent désormais dans « Terminées ». Grâce à la réalisation de toutes les stories, les objectifs de sprint devraient être considérés comme accomplis. Il est maintenant temps de se préparer à l'atteinte des objectifs du sprint suivant, en vidant la colonne « Terminé », puis en déplaçant les stories du sprint suivant, qui étaient dans « À venir », dans « À faire » en leur ajoutant une échéance et une assignation. Le cycle peut alors recommencer. Une itération est terminée, un incrément du projet a été livré, et une nouvelle phase de développement est prête à être entamée.

En mélangeant de cette manière les approches Agile *Scrum* et *Kanban*, on obtient tout simplement l'approche *Scrumban*!

Comme mentionné, tous les outils numériques sur le marché (ou presque) proposent actuellement les éléments nécessaires à la mise en place d'une gestion de projet *Scrumban*. En 1984, la sortie de *Microsoft Project* est une révolution en gestion de projet. La première version disponible sur Windows en 1990 permet enfin aux chefs de projet d'utiliser un outil numérique de planification pour gérer des tâches, visualiser les étapes de développement dans des diagrammes de Gantt, suivre la gestion des coûts et des ressources, et extraire l'ensemble des données pour produire des analyses et des rapports d'avancement de projet. À l'ère des méthodes traditionnelles, *Microsoft Project* domine le marché, et poursuit sa domination bien au-delà, étant toujours le #1 mondial en 2011, utilisé par plus de 20 millions d'utilisateurs. À l'ère de l'Agilité, les solutions de *Microsoft* pour la gestion de projet existent toujours, mais ne s'adressent plus uniquement aux chefs d'équipe. [*Microsoft Planner*](https://tasks.office.com/), sorti en 2015, est la proposition plus simple de *Microsoft* pour le travail d'équipe qui se compare davantage aux applications numériques visuelles, flexibles et extrêmement populaires ayant fait irruption sur le marché depuis 2010. En croisant [ses outils de gestion de projet et l'outil de communication *Teams*](https://www.microsoft.com/fr-ca/microsoft-365/project/agile-methodology), *Microsoft* permet à tous les membres d'une équipe de gérer et suivre l'évolution de leur travail de manière Agile. Pour les utilisateurs des services *Microsoft*, cette solution intégrée peut être très avantageuse, avec un faible coût d'apprentissage.

Longtemps seule et dominante dans le marché, *Microsoft* compétitionne aujourd'hui avec une multitude d'acteurs. Les premières plateformes collaboratives comme *Basecamp* (1999) et *Jira* (2002) marquent le début du suivi en ligne, permettant aux équipes de travailler à distance, en temps réel, et de suivre leur progression de manière Agile. Développé par la compagnie *Atlassian*, *Jira* se concentre particulièrement sur la gestion des suivis de *bugs*, d'incidents et de projets au niveau « macro », par le suivi des *sprints*, des *objectifs de sprint* et des *backlog*. L'application est intégrée à *Google Workspace* en 2021, et évolue en parallèle à *Trello*, un autre outil de gestion de projet en ligne lancé par *Atlassian* en 2011 et concentré sur le Kanban utilisateur, pour le suivi « micro » des tâches. Les différences sont légères avec les applications [*Asana*](https://asana.com/) (2008) et [*monday.com*](monday.com) (2012), toutes basées principalement sur le développement et le suivi de projets Agiles via des vues de tâches en tableau Kanban, en calendrier, voire en Gantt (le diagramme de Gantt, même à l'ère de l'Agilité, demeure toujours très populaire).

Des propositions de logiciels libres et à code source ouvert sont bien sûr présentes en ligne. [*OpenProject*](https://www.openproject.org/) (2012) et [*Taiga*](https://taiga.io/) (2014) sont des outils Agiles présentant tous les avantages liés aux logiciels libres tels que la gratuité du service et la possibilité de modification et de redistribution.

<!-- Structurer ses tâches est un processus fondamental pour mener un projet à terme. Particulièrement dans le monde académique, où les travaux s'échelonnent souvent sur plusieurs années, il est facile de perdre de vue ses objectifs ou de prendre des détours coûteux en temps si le chemin vers le produit final est mal défini. Gérer et structurer ses tâches de manière efficace facilite la mesure des progrès et permet de constamment vérifier si ceux-ci sont encore alignés avec les objectifs finaux. -->

<!-- Gérer ses tâches de façon efficace passe par une structuration claire des objectifs du projet. Il est important de connaître la destination finale afin de choisir la meilleure direction pour y parvenir. Pour ce faire, il est utile de schématiser ou de lister la conception de la version finale du projet. Dans l'idéal, à quoi ressemble-t-il dans sa forme aboutie? Une fois cette vision clairement définie, il est possible de désagréger le projet en grandes étapes. Que faut-il accomplir, à l'échelle macro, pour atteindre les objectifs fixés? -->

<!-- À cette étape, il est crucial de prendre en compte les ressources financières, temporelles et humaines disponibles. Cela permet de déterminer de manière réaliste ce qui est possible. Identifier ces grandes étapes contribue à la création d'un plan de projet structuré où chaque phase est clairement définie. Cela aide à anticiper les besoins en ressources et à ajuster les échéances en conséquence. -->

<!-- La révision continue est également un élément clé du processus de gestion des tâches. En réévaluant régulièrement l'état d'avancement du projet par rapport au plan initial, il est possible d'apporter des ajustements nécessaires pour rester sur la bonne voie. Cet astuce permet de répondre aux changements inévitables qui surviennent au cours de la recherche, qu'ils soient dus à des découvertes inattendues, des changements dans les directives institutionnelles ou des feedbacks des pairs. -->

<!-- Avec des objectifs bien définis et des étapes claires pour y parvenir, la structure du projet est complète. Il est donc temps de se lancer dans la gestion des tâches. En fonction des objectifs établis, certaines tâches sont plus importantes que d'autres. Un projet est généralement composé de tâches qui doivent être réalisées dans un ordre spécifique, où certaines doivent impérativement précéder d'autres. Le défi est de déterminer efficacement ce qui doit être priorisé pour maintenir une progression fluide et efficace. -->

<!-- L'agilité est un processus de travail qui facilite cette priorisation. En adoptant une approche agile, les objectifs sont fixés dans le temps et sont constamment évalués. Cela permet une adaptation rapide et une réponse aux changements sans compromettre les résultats finaux. De cette façon, les tâches sont déterminées et ajustées en fonction de l'avancement du projet et des éventuels obstacles rencontrés. Le projet avance de façon incrémentale. -->

<!-- Pour une mise en œuvre efficace de l'agilité, il est utile de planifier ses objectifs sur une période de quelques semaines, connues sous le nom de sprints en méthode Scrum, où on évalue le travail accompli et on redéfinit les priorités pour la prochaine période. Ces sprints permettent de s'assurer de rester concentré sur les tâches qui apportent le plus de valeur au projet et d'ajuster les plans en temps réel en fonction des résultats obtenus. -->

<!-- Toutes ces pratiques deviennent rapidement complexes si elles ne sont pas encadrées dans un environnement qui permet d'en faire le suivi. Il peut être judicieux de faire appel à des outils de gestion de projet qui supportent l'agilité, tels que Notion ou Mondays. Ces outils permettent de visualiser les tâches à faire sous forment de tableaux de bords interactifs, dans lesquels il est possible de les déplacer en fonction de leur statut d'avancement. Ces outils permettent de structurer les tâches d'un projet et d'en faire le suivi facilement du début à la fin. -->

<!-- Il est également judicieux de faire appel à des outils de gestion de projet qui supportent l'agilité, tels q. Ces outils permettent de visualiser les tâches sous forme de tableaux de bord interactifs où les tâches peuvent être déplacées, modifiées ou mises à jour en temps réel. Ils favorisent la transparence et la communication entre les membres de l'équipe, essentielles pour une gestion agile des tâches. -->

<!-- Enfin, il est crucial d'intégrer des pratiques de réflexion et d'amélioration continue. Après chaque sprint, l'équipe devrait se réunir pour une rétrospective afin de discuter de ce qui a bien fonctionné et de ce qui pourrait être amélioré. Cette culture de l'amélioration continue est au cœur de l'agilité et contribue à l'efficacité et à la réussite du projet à long terme. -->

<!-- Pour déterminer quelles tâches accomplir et dans quel ordre, voici une court processus par étapes : -->

<!-- 1.  Élaborer les tâches en fonction des objectifs de sprint. -->

<!-- 2.  Déterminer la linéarité des tâches, c'est-à-dire, quelle tâche doit être accomplie afin d'en début une autre. -->

<!-- 3.  Quantifier le poids de chaque tâche. Certaines tâches sont plus longues que d'autres. Adopter un système qui vous permet d'identifier quelles tâches prendront quelques minutes seulement (comme l'envoi du courriel), et quelles tâches prennent plusieurs jours. Si une tâche est trop longue, c'est un signe qu'elle pourrait être désagrégée en plusieurs tâches plus petites. Cela facilite également le suivi. -->

<!-- 4.  Donner une échéance réaliste à chaque tâche, en fonction des étapes précédentes. Idéalement, toutes les tâches ne sont pas dues pour la même date, pour éviter un goulot d'étranglement. Les échéances aident à prioriser les tâches. -->

<!-- 5.  Prioriser les tâches qui ont l'échéance le plus sérré. Si certaines tâches accumulent un retard, c'est peut-être parce que vous devez réévaluer les échéances, les objectifs, ou encore parce qu'il y a des blocants dans vos méthodes de travail. Faire un tel suivi permet d'évaluer sa propre efficacité dans ses méthodes de travail. -->

<!-- L'utilisation d'outils numériques pour la gestion des tâches ne signifie pas qu'il faut abandonner l'agenda papier ou le cahier de notes. Plusieurs trouvent essentiels de prendre des notes et de se faire des listes de tâches à la main. Il est tout à fait possible de combiner les méthodes. À chaque début de semaine, mettez à jour votre gestionnaire de tâches, puis faites votre liste de tâches à la main en conséquence, et planifiez votre semaine. De cette façon, vous savez chaque jour le travail à prioriser. -->

```{=html}
<!-- Gilbert : Je crois que cette section n'est plus pertinente ### Enregistrement de protocole

Après avoir établi l'importance de la gestion des tâches et comment une approche agile peut optimiser ce processus, il est complémentaire de d'aborder l'enregistrement méthodique de ces tâches et des étapes du projet. Cette documentation assure la transparence, la réplicabilité et la rigueur scientifique de la recherche. L'enregistrement du protocole de recherche sert plusieurs objectifs clés qui se connectent directement à la gestion agile des tâches. Il agit comme une archive vivante des décisions prises, des méthodes utilisées et des modifications apportées tout au long du projet. Cela permet la vérification et la validation des résultats, et de maintenir une vision claire de l'évolution du projet.

L'enregistrement de protocole en science est une pratique facultative, mais de plus en plus populaire, qui consiste à documenter et à déposer de manière détaillée le plan de recherche d'une étude avant que celle-ci ne soit menée. Cette démarche s'inscrit dans le cadre des pratiques de recherche ouverte et transparente. Elle a plusieurs avantages : D'abord, rendre les protocoles de recherches publics démontre un engagement envers des méthodes rigoureuses. Cela augmente la confiance envers les résultats obtenus et les méthodes employées. Une démarche détaillée permet aussi la réplicabilité du projet, en offrant aux autres chercheurs du domaine les étapes détaillées employées pour se rendre aux résultats.

Un autre avantage est d'éviter qu'une étude soit réalisée par deux chercheurs au même moment. En enregistrant sa recherche, tous peuvent consulter les recherches en cours, et ainsi s'assurer que leurs projets sont uniques. De cette manière, les ressources académiques sont maximisées. L'enregistrement du protocole permet aussi d'évaluer la recherche par les pairs avant d'amorcer sa réalisation, ce qui peut augmenter la crédibilité de l'étude, faciliter la publication dans une revue scientifique, et gagner du temps dans la réalisation du projet.

Enfin, au coeur du concept de l'enregistrement de protocole se trouve l'idée de l'intégrité de la recherche. La science, par définition, est transparente dans ses démarches. Rendre public ses intentions et devoir justifier chaque modification s'inscrit dans cette optique d'intégrité. Un tel processus rend difficile la chasse au résultats, un fléau en science où les chercheurs privilégient l'atteinte de résultats avant la démarche. Le système très compétitif et chronophage de la publication scientifique encourage ce genre de pratique. L'enregistrement de protocole tente d'encourager des pratiques transparentes, qui sont bien acceuillies par les revues scientifiques.

Pour enregistrer votre protocole de recherche, il faut suivre ces quatre étapes :

1.  Préparer le protocole. Un document détaille les hypothèses, les méthodes et les analyses prévues. l'objectif est de rédiger un document suffisamment détaillé pour permettre à d'autres chercheurs de reproduire l'étude. La rédaction de ce document n'est pas une perte de temps, car une majorité devrait pouvoir être réutilisée dans l'étude finale.
2.  Enregistrer le protocole. L'enregistrement se fait dans un registre public. Il existe différents registres, ouverts à tous les domaines (Open Science Framework, Research Registry) ou plus spécifiques aux sciences sociales (EGAP, AEA RCT).
3.  Valider le protocole. Le protocole est évalué par les pairs, pour assurer sa complétude, puis il devient public. Ainsi, vous obtenez des commentaires avant même de soumettre à une revue, ce qui peut vous faire sauver du temps en apportant des modifications avant la réalisation, plutôt qu'après.
4.  Suivi du protocole. À chaque étape de la recherche, les chercheurs confirment qu'ils suivent le processus annoncé, ou justifient les changements apportés, ce qui assure la transparence dans leurs démarchent scientifiques.
-->
```

```{r}
#| tbl-cap: Résumé des principaux outils de gestion de projet
#| echo: false
#| message: false
#| warning: false

library(tidyverse)
library(gt)

tbl_resume_gestion_projet <- tibble(
  Critères = c(
    "Accessibilité (Gratuit ou peu dispendieux)",
    "Existence d'une communauté d'utilisateurs",
    "Popularité dans le champ",
    "Compatibilité avec d'autres outils",
    "Transparence et réplicabilité",
    "Adaptabilité et flexibilité"
  ),
  Microsoft = c(
    "Dispendieux",
    "Très grande",
    "Populaire",
    "Avec les outils Microsoft",
    "Faible",
    "Peu flexible"
  ),
  `monday.com` = c(
    "Dispendieux",
    "Grande",
    "Peu populaire",
    "Facilement compatible",
    "Faible",
    "Peu flexible"
  ),
  OpenProject = c(
    "Gratuit (logiciel libre)",
    "Limitée",
    "Peu populaire",
    "Demande un peu d'ajustement",
    "Excellent",
    "Très flexible"
  ),
  Notion = c(
    "Gratuit jusqu'à un seuil",
    "Très grande",
    "Populaire",
    "Facilement compatible",
    "Excellent",
    "Très flexible"
  )
)

tbl_resume_gestion_projet |>
  gt() |>
  # Légende du tableau
  tab_caption("Résumé des principaux outils de gestion de projet") |>
  # Libellés de colonnes (avec possibilité de sauts de ligne si nécessaire)
  cols_label(
    Critères = "Critères",
    Microsoft = "Microsoft",
    `monday.com` = md("monday.com"),
    OpenProject = "OpenProject",
    Notion = "Notion"
  ) |>
  # Alignements + largeurs
  cols_align("left", columns = everything()) |>
  cols_width(
    Critères ~ pct(32),
    Microsoft ~ pct(17),
    `monday.com` ~ pct(17),
    OpenProject ~ pct(17),
    Notion ~ pct(17)
  ) |>
  # Style “booktabs” + entêtes en gras
  tab_style(
    style = list(cell_text(weight = "bold")),
    locations = cells_column_labels(everything())
  ) |>
  opt_table_lines() |>
  # Typo / espacement (mêmes réglages que votre modèle)
  tab_options(
    table.font.size = px(12),
    data_row.padding = px(11),
    column_labels.background.color = "white",
    table.width = pct(95)
  )
```

### Manuel d'instruction: Notion pour la vie professionnelle et personnelle

En août 2024, [*Notion*](https://www.notion.so/fr) dépasse la barre du 100 millions d'utilisateurs. Juste avant la pandémie de COVID-19, *Notion* atteignait son premier million. En l'espace de quelques années, l'outil a rejoint, voire dépassé ses concurrents. Une popularité incontestable, notamment due à sa grande flexibilité.

Créé en 2013 par Ivan Zhao et Simon Last, *Notion* se présente comme un outil de gestion de projet « tout-en-un », offrant à l'utilisateur des pages blanches qu'il doit lui-même remplir en fonction de ses besoins. Les pages peuvent intégrer du texte, des bases de données, des images, des vidéos, des PDF, des liens Web, et bien plus. Chaque page peut ainsi devenir un outil de documentation, ou un tableau Kanban rempli de stories, ou des notes de rencontres, ou un calendrier des livrables de sprint. Il importe à l'utilisateur de connaitre ses besoins, et d'être créatif.

Cette liberté peut représenter un défi. Il est nécessaire d'avoir l'esprit « architecte » pour apprécier *Notion.* En comparaison, *monday.com* ou *Trello* offrent un cadre de travail déjà défini et balisé à l'utilisateur. Les instructions à l'entrée sont claires: l'utilisateur a un Kanban à remplir, et il peut facilement faire le suivi de ses tâches.

Sur *Notion*, tout (ou presque) est possible. Notion possède d'ailleurs [son propre langage de programmation](https://www.notion.so/fr/help/formulas) pour coder des fonctions originales dans des bases de données.

Il n'est toutefois pas nécessaire d'aller dans la complexité: sans être un logiciel libre, *Notion* offre la possibilité à ses utilisateurs de créer leur propre « modèle », et de les offrir (voire de les vendre, si désiré!) à la communauté Web. Ainsi, [plus de 20 000 modèles *Notion* sont disponibles en ligne](https://www.notion.so/fr/templates). Vous trouverez de multiples modèles pour répondre à tous vos besoins: que ce soit pour la gestion de vos finances ou pour gérer votre vie étudiante, la communauté *Notion* est large et structurée. Tous les modèles disponibles s'importent en deux clics, et vous permettent de les modifier complètement au besoin. Avec le temps et l'expérience, vous pourrez vous-même créer vos propres modèles et les offrir gratuitement (ou à votre prix!) à la communauté.

Notez également que l'utilisation individuelle de *Notion* est tout à fait gratuite. En payant, vous vous offrez bien sûr davantage de possibilités (comme l'utilisation de l'intelligence artificielle intégrée de *Notion*), mais l'utilisation gratuite de base peut être bien suffisante pour vos besoins.

Avec un seul outil, *Notion* peut vous permettre de gérer à la fois votre vie personnelle et votre vie professionnelle. Pour se lancer, [des guides simples sont disponibles sur le site Web](https://www.notion.so/fr/help/category/new-to-notion).

Notez également que *Notion* possède sa propre application de calendrier. *Notion Calendar* permet d'importer vos calendriers *Google*, en plus d'afficher tous les jours vos échéances de stories.

Vous souhaitez créer une page *Notion* pour votre prochain voyage entre amis? Rien de plus simple! Vous pourrez y intégrer une liste d'idées d'activités, un calendrier de vos destinations, un Kanban rassemblant toutes les tâches à accomplir d'ici au départ, et partager le tout à vos amis via le Web. En quelques clics, toutes les pages *Notion* peuvent être transformées en site Web consultable et modifiable (si on le souhaite et l'autorise).

Depuis son lancement, *Notion* est en constante évolution. Impossible de savoir à quoi ressemblera l'application dans 10 ans. Victime de son succès, l'entreprise s'est plusieurs fois réinventée, misant sur une gestion de projet Agile, en communication avec sa clientèle, pour développer un outil à l'image des besoins et des intérêts de ses utilisateurs.

## Les outils de gestion de données <!-- Nouveau Chapitre? -->

En débutant en recherche, vous êtes probablement dans une position où vous devez débuter prochainement la rédaction d'un mémoire ou d'une thèse. Vous avez peut-être commencer à prendre des notes sur la littérature, vous créez des fichiers, vous commencez à collecter des données. Rapidement, une question simple mais cruciale se pose : **où devriez-vous mettre tout ce contenu?**

### Brève histoire de la gestion des données : du entreposage personnel à la science ouverte

Il y a vingt ans, la gestion des données de recherche était généralement une affaire personnelle et locale. Chaque chercheur développait ses propres méthodes d'organisation, souvent artisanales : dossiers sur le bureau, copies de sauvegarde sur disque dur externe, fichiers envoyés par courriel aux collaborateurs. Les apports, les résultats et les processus scientifiques étaient soit fermés, soit accessibles moyennant des frais, soit uniquement accessibles aux chercheurs et à leurs collaborateurs.

Ce paradigme a progressivement changé avec l'émergence de la **science ouverte**, un mouvement visant à rendre les processus et pratiques scientifiques, y compris la méthodologie et les résultats de la recherche, plus ouverts et transparents. Au Canada, ce mouvement s'est concrétisé par plusieurs initiatives majeures. En février 2020, la conseillère scientifique en chef du Canada a publié la *Feuille de route pour la science ouverte*, définissant la science ouverte comme "la pratique qui consiste à rendre les intrants, les résultats et les processus scientifiques librement accessibles à tous avec un minimum de restrictions."

En 2021, le Conseil de recherches en sciences naturelles et en génie (CRSNG), les Instituts de recherches en santé du Canada (IRSC) et le Conseil de recherches en sciences humaines (CRSH) ont lancé la **Politique des trois organismes sur la gestion des données de recherche**. Cette politique repose sur trois piliers qui vous concernent. Vous êtes encouragés à :

1. Rédiger un **plan de gestion des données** (PGD) pour vos demandes de subvention
2. Déposer vos données dans un **dépôt sécurisé** à la fin de votre projet
3. Obtenir le **soutien** de votre institution pour y arriver

Cette évolution reflète une transformation profonde : la gestion des données n'est plus une pratique marginale ou optionnelle, mais un élément central de la conduite *responsable* de la recherche.

### Pourquoi la question "où entreposer?" est devenue essentielle

Aujourd'hui, lorsque l'on aborde le domaine de la recherche scientifique en sciences sociales numériques, la collaboration et la gestion efficace des données et du code sont devenues des éléments cruciaux pour progresser dans ses projets. La question "où je mets mes fichiers?" n'est plus seulement une question pratique de sauvegarde personnelle. Elle touche désormais à quatre dimensions fondamentales :

**Sécurité** : Les chercheurs manipulent une quantité croissante de données, dont certaines sont sensibles ou confidentielles. Un ordinateur qui plante, un disque dur qui lâche, un fichier perdu peuvent compromettre des mois de travail. Les solutions d'entreposage modernes offrent des sauvegardes automatiques, du chiffrement, et des contrôles d'accès sophistiqués.

**Transparence** : La reproductibilité des résultats de recherche exige de pouvoir retracer l'évolution d'un projet : quelles données ont été utilisées, quelles versions du code, quelles analyses ont été effectuées. Un bon système d'entreposage permet de documenter ces étapes et de les rendre accessibles pour validation.

**Traçabilité** : Qui a modifié quoi, quand et pourquoi? Dans un contexte de collaboration, mais aussi pour votre propre mémoire à long terme, la capacité de suivre l'historique des modifications est essentielle. Cela permet également de revenir en arrière en cas d'erreur.

**Collaboration** : Les projets de recherche impliquent rarement une seule personne. Directeurs, codirecteurs, collègues, assistants de recherche doivent pouvoir travailler ensemble efficacement. Envoyer des fichiers par courriel avec des noms comme Analyse_v2_final.R n'est plus viable. Les outils modernes permettent le travail simultané, les commentaires intégrés, et la fusion intelligente des contributions.

Votre façon d'entreposer, de gérer et de partager vos données a donc un impact direct sur la qualité de vos recherches, sur votre capacité à collaborer efficacement, et sur votre conformité aux exigences de votre institution et des organismes subventionnaires.

### Trois grandes catégories de besoins : rédiger, collaborer, archiver

Face à cette complexité, il est utile de distinguer trois grandes catégories de besoins qui correspondent à différentes phases et différents types d'activités dans votre projet de recherche :

**Rédiger** : Vous écrivez votre document principal (mémoire, thèse, articles). Ce document évolue quotidiennement. Vous ajoutez des paragraphes, révisez des chapitres, votre directeur commente vos brouillons. Vous avez besoin d'un outil qui permet la **rédaction active**, avec synchronisation, versionnement automatique, et idéalement la collaboration en temps réel ou quasi-réel.

**Collaborer** : Vous collectez des données, vous rédigez du code, vous générez des analyses. Ces fichiers sont en constante évolution et sont souvent partagés avec d'autres chercheurs. Certaines données sont sensibles (entrevues, informations personnelles) et requièrent des mesures de sécurité strictes. D'autres sont publiques ou destinées à l'être. Vous avez besoin d'un **espace de travail partagé** qui respecte les contraintes éthiques et permet la collaboration efficace.

**Archiver** : Votre projet se termine, votre mémoire est déposé, vous vous apprêtez à publier. Vous devez maintenant **préserver et partager** vos données selon les principes de la science ouverte. Ces données deviennent "finales" : elles ne changeront plus (ou très peu), elles doivent être documentées, elles doivent être trouvables et citables. Vous avez besoin d'un **dépôt de données** qui garantit la pérennité et l'accessibilité à long terme.

Ces trois catégories ne sont pas étanches. Un même projet mobilise les trois, souvent simultanément. Un bon système de gestion des données de recherche doit donc intégrer ces différentes dimensions de manière cohérente.

Dans les sections qui suivent, nous explorerons d'abord ces trois catégories à travers quatre questions pratiques : où entreposer votre rédaction, où entreposer vos données pendant la recherche active, où déposer vos données finales, et comment gérer les données massives qui débordent des solutions standards. Nous consacrerons ensuite les deux dernières sections à un outil transversal qui traverse ces trois catégories et qui mérite une attention particulière : **Git et GitHub**. Bien que ces outils puissent sembler intimidants au premier abord, ils offrent une solution puissante pour gérer simultanément votre rédaction, votre code, votre documentation et la collaboration avec vos collègues. Nous verrons pourquoi de plus en plus de chercheurs en sciences sociales les adoptent, puis comment les mettre en place concrètement pour votre projet de recherche.

## Où entreposer votre rédaction

La première question que vous vous poserez est probablement la plus immédiate : où allez-vous écrire votre mémoire ou votre thèse? Ce document central va évoluer pendant des mois, voire des années. Vous y travaillerez quotidiennement, votre directeur y ajoutera des commentaires, vous réviserez des chapitres entiers, vous fusionnerez peut-être des contributions de collaborateurs. Le choix de votre outil de rédaction et de la plateforme d'entreposage qui l'accompagne n'est donc pas anodin.

### Les solutions courantes : un aperçu

**Microsoft Word + OneDrive institutionnel** : C'est probablement la solution la plus répandue dans les universités canadiennes. Si vous avez accès à la suite Microsoft 365 via votre institution (ce qui est généralement le cas), vous disposez de Word pour la rédaction et de OneDrive pour la synchronisation automatique. OneDrive offre une capacité d'entreposage intéressante, se synchronise avec votre ordinateur local, et permet de travailler hors ligne. La fonction de suivi des modifications de Word facilite la collaboration avec votre directeur, et OneDrive conserve un historique des versions pendant 30 jours, ce qui vous permet de récupérer une version antérieure en cas d'erreur. Toutefois, la collaboration en temps réel est moins fluide qu'avec d'autres options présentées ci-dessous. Les conflits de versions peuvent survenir si plusieurs personnes modifient simultanément le fichier sans coordination.

**Google Docs** : Similaire à Microsoft Word dans sa présentation, Google Docs offre une alternative infonuagique. C'est un outil assez simple et permet de collaborer en temps réel. Plusieurs personnes peuvent éditer simultanément le même document, les commentaires s'ajoutent facilement, et l'historique des versions est illimité. Son utilisation nécessite cependant une connexion Internet pour un usage optimal. Également, la mise en page complexe est moins sophistiquée.

**Overleaf** : Si vous rédigez en LaTeX, Overleaf est la plateforme collaborative de référence. Elle combine un éditeur LaTeX en ligne, la compilation automatique de votre document, et des fonctionnalités de collaboration similaires à Google Docs. C'est une bonne option pour les documents techniques et pour collaboration en temps réel, et cela permet l'intégration avec des modèles de revues académiques. Si vous ne connaissez la LaTeX, il y a une certaine courbe d'apprentissage à surmonter, mais qui peut en valoir la peine.

**Markdown** : Cela représente une approche différente de la rédaction, particulièrement adaptée aux chercheurs qui intègrent beaucoup d'analyses quantitatives dans leurs travaux. Markdown est un langage de balisage léger qui permet d'écrire en texte brut tout en spécifiant la structure du document (titres, listes, emphases). R Markdown et son successeur Quarto permettent de combiner du texte rédigé en Markdown avec du code et de générer automatiquement des documents finaux en PDF, Word ou HTML incluant vos analyses, graphiques et tableaux. L'avantage majeur est la reproductibilité complète : votre document contient à la fois votre texte et le code qui a produit vos résultats, ce qui facilite grandement les révisions et la validation. Pour la collaboration, les fichiers Markdown fonctionnent bien avec Git (comme nous le verrons plus loin), car ce sont des fichiers texte que Git peut suivre ligne par ligne. Ce livre, rédigé en Quarto, contient un chapitre enier sur les langages de balisages comme Markdown et LaTeX.

### Le problème persistant du versionnement

Quel que soit l'outil choisi, vous rencontrerez un problème classique : **comment gérer les versions de votre document?** Vous voudrez éviter de vous retrouver avec plusieurs versions du même fichier, nommé différent en fonction de sa version au fil des révisions et des relectures. Ce genre de nomenclature est source de confusion, d'erreurs, et de perte de temps. Vous ne savez plus quelle version est la plus récente, vous risquez d'écraser accidentellement du travail récent, et la collaboration devient un casse-tête.

Les outils mentionnés ci-dessus offrent tous un certain niveau de versionnement automatique, mais avec des limites. OneDrive conserve 30 jours d'historique, Google Docs et Overleaf (version payante) offrent un historique complet, mais la navigation dans cet historique peut être fastidieuse si vous cherchez une version spécifique datant de plusieurs semaines.

### Une solution plus rigoureuse : Git pour la rédaction

Il existe une approche plus rigoureuse pour gérer le versionnement, particulièrement si vous rédigez en formats textuels (Markdown, LaTeX, ou même Word si vous acceptez certaines contraintes) : **Git**. Nous y reviendrons en détail un peu plus tard, mais sachez que Git permet de :

- Enregistrer explicitement chaque version de votre document avec un message descriptif
- Créer des "branches" pour tester différentes structures ou versions sans affecter votre version principale
- Collaborer de manière asynchrone avec un contrôle précis sur qui change quoi
- Conserver un historique complet et illimité de toutes vos modifications

Si vous aimez avoir un contrôle précis sur vos fichiers et/ou prévoyez travailler avec des collègues sur des projets complexes, Git mérite votre attention. Nous y reviendrons.

### Bonnes pratiques pour l'entreposage de votre rédaction

Peu importe l'outil que vous choisissez, voici quelques principes à respecter :

1. **Ne jamais garder un seul exemplaire local** : Votre ordinateur peut planter, être volé, ou subir une défaillance de disque dur. Utilisez systématiquement une solution infonuagique avec synchronisation automatique.

2. **Sauvegardes croisées pour les documents critiques** : Si votre thèse représente des années de travail, envisagez une stratégie de sauvegarde redondante. Par exemple : travailler dans OneDrive, mais faire une copie hebdomadaire sur Google Drive ou sur un disque dur externe.

3. **Attention aux données sensibles dans votre document** : Si votre mémoire contient des extraits d'entrevues ou des données sensibles, assurez-vous que la plateforme d'entreposage respecte les exigences éthiques de votre institution (nous y reviendrons dans la section suivante).

En résumé, pour la rédaction de votre mémoire ou thèse, privilégiez une solution infonuagique institutionnelle avec synchronisation automatique. Si vous êtes à l'aise techniquement et que vous rédigez en formats textuels, Git peut offrir un contrôle encore plus fin. Mais l'essentiel est d'avoir **au moins une sauvegarde automatique et hors de votre ordinateur personnel**.

## Où entreposer vos données pendant la recherche

Au-delà de la rédaction de votre mémoire, vous allez collecter, manipuler et analyser des données. Ces données peuvent prendre plusieurs formes : transcriptions d'entrevues, enregistrements audio, données de sondages, bases de données statistiques, scripts de code, images, tableaux, etc. Vos données de recherche peuvent poser des défis spécifiques en matière de sécurité, d'éthique et de collaboration.

La question centrale ici n'est pas seulement "où entreposer?", mais surtout **"où puis-je entreposer selon la nature de mes données?"** Car toutes les solutions ne se valent pas, et certaines sont inappropriées selon le type de données que vous manipulez.

### La distinction entre données sensibles et non-sensibles

Avant de choisir une solution d'entreposage, vous devez d'abord déterminer si vos données sont **sensibles** ou non. Cette distinction n'est pas qu'une question théorique : elle a des implications éthiques et pratiques importantes.

Une donnée est considérée comme sensible si elle permet d'identifier directement ou indirectement des individus, ou si elle contient des informations confidentielles ou privées. Voici des exemples courants en sciences sociales :

- **Entrevues ou groupes de discussion** contenant des informations personnelles (noms, lieux, événements personnels)
- **Sondages ou questionnaires** incluant des données démographiques détaillées qui pourraient identifier des participants
- **Données administratives** obtenues auprès d'organisations (dossiers médicaux, dossiers scolaires, données d'entreprise)
- **Enregistrements audio ou vidéo** de participants identifiables
- **Données géolocalisées** précises (traces GPS, adresses)

Dans la plupart des institutions canadiennes, la gestion de données sensibles est encadrée par un comité d'éthique de la recherche. Lorsque vous soumettez votre protocole de recherche pour approbation éthique, vous devez spécifier comment vous allez collecter, entreposer, protéger et éventuellement détruire vos données sensibles. Le non-respect de ces engagements peut avoir des conséquences sérieuses, allant du retrait de votre approbation éthique à des sanctions institutionnelles.

Les principes à respecter pour les données sensibles incluent la minimisation (ne collectez que les données nécessaires), l'anonymisation  (retirez ou masquez les identifiants dès que possible), le chiffrement (protégez les fichiers par mot de passe ou chiffrement), l'accès limité (seules les personnes autorisées dans votre certificat éthique peuvent accéder aux données), et l'entreposage sécurisé (utilisez des serveurs approuvés, idéalement situés au Canada).

À l'inverse, certaines données ne posent pas de problème de confidentialité. C'est le cas des statistiques publiques, des corpus textuels ouverts comme les articles de presse ou les transcriptions de débats parlementaires, des données administratives agrégées sans possibilité d'identification, ou encore de votre propre code d'analyse (scripts R par exemple). Pour ces données, les contraintes sont beaucoup plus souples. Vous pouvez les entreposer sur des plateformes grand public, les partager plus librement, et éventuellement les rendre publiques sans approbation éthique spécifique.

### Solutions d'entreposage pour données sensibles

Si vos données ne sont pas sensibles, vous avez beaucoup plus de flexibilité dans le choix de vos outils. Dropbox est très utilisé dans plusieurs milieux, même à titre d'utilisation personnelle. Dans le milieu académique, le partage de fichiers peu volumineux par Dropbox est très fréquent. Son utilisation est relativement accessible : 2 Go sont offerts gratuitement, puis un abonnement est nécessaire pour entreposer davantage. Dropbox se démarque par sa popularité et par son interface intuitive, s'intégrant facilement dans le gestionnaire de fichiers. Sa synchronisation est rapide et familière pour la plupart des utilisateurs, bien que sa capacité gratuite soit limitée et que ses serveurs américains le rendent problématique pour des données sensibles. Dropbox est ainsi très utile dans le cadre de gestion de données collaborative relativement simple.

Google Drive est facile d'utilisation par son intégration au sein des outils Google. La plateforme, en général, est fréquemment utilisée dans le cadre de rédaction, pour son suivi des modifications et la possibilité de travailler à plusieurs sur un document en même temps. Google Drive offre 15 Go gratuits, et son utilisation est simple et intuitive. Pour des projets impliquant du code et des données non-sensibles de taille modérée, GitHub et GitLab sont d'excellentes options. Nous y reviendrons en détail plus loin, mais sachez qu'ils permettent non seulement d'entreposer vos fichiers, mais aussi de suivre rigoureusement leur évolution et de collaborer efficacement grâce au versionnement et aux *pull requests*. Ils sont gratuits pour les projets publics et très utilisés en recherche, bien qu'ils nécessitent une courbe d'apprentissage et ne soient pas adaptés pour des fichiers très volumineux.

Considérant ce qui a été mentionné précédemment, voici un résumé de bonnes pratiques pour l'entreposage de données de recherche :

1. **Identifiez la sensibilité de vos données dès le début** : consultez votre comité d'éthique si vous avez un doute.

2. **Ne mélangez pas sensible et non-sensible sur la même plateforme** : séparez clairement vos données sensibles de vos données publiques.

3. **Documentez vos données** : créez un fichier README expliquant ce que contient chaque dossier, comment les données ont été collectées, et quelles sont les conventions de nomenclature.

4. **Sauvegardez régulièrement** : même sur des plateformes infonuagiques, faites des copies périodiques.

5. **Contrôlez les accès** : ne partagez vos dossiers qu'avec les personnes autorisées dans votre certificat éthique.

6. **Préparez l'archivage dès maintenant** : réfléchissez à ce que vous devrez déposer à la fin de votre projet et organisez vos fichiers en conséquence.

En résumé, le choix de votre solution d'entreposage dépend avant tout de la **sensibilité de vos données**. Pour des données sensibles, privilégiez les solutions institutionnelles. Pour des données non-sensibles, vous avez plus de flexibilité. Mais encore une fois, **ne gardez jamais vos données uniquement sur votre ordinateur personnel**.

## Où déposer vos données finales (archivage et science ouverte)

Les deux sections précédentes concernaient vos données **actives** : celles que vous utilisez quotidiennement pendant votre recherche. Mais arrive un moment où votre projet se termine, votre mémoire est déposé, et vous vous apprêtez peut-être à publier un article. C'est à ce stade que vos données deviennent **finales** et que vous devez penser à leur archivage et à leur partage potentiel.

Le dépôt sécurisé suit une logique scientifique : Cela rend vos données trouvables, citables, et potentiellement réutilisables par d'autres chercheurs, ce qui contribue à l'avancement de la science de façon *transparente*.

### Données actives vs. données finales : quelle différence?

Les **données actives** sont celles que vous manipulez pendant votre recherche. Elles évoluent constamment : vous ajoutez de nouvelles observations, vous corrigez des erreurs, vous créez de nouvelles variables. Ces données vivent dans votre espace de travail.

Les **données finales** sont celles qui accompagnent une publication ou un mémoire déposé. Elles ne changeront plus, ou très peu. Au moment où elles sont hébergées dans un dépôt et qu'un identifiant unique leur est attribué (un DOI, par exemple), elles sont figées. Toute modification ultérieure entraîne la création d'une nouvelle version des données.

Cette distinction est importante parce qu'elle détermine où et comment vous entreposez vos données. Les outils que vous utilisez pendant votre recherche (Nextcloud, GitHub) ne sont pas conçus pour l'archivage. À l'inverse, les dépôts de données (Borealis, Zenodo, etc.) ne sont pas pratiques pour un travail quotidien, mais excellent pour la préservation et le partage.

### Pourquoi placer vos données dans un dépôt?

Plusieurs raisons justifient le dépôt de données finales :

**Conformité aux exigences** : Certains organismes subventionnaires et certaines revues académiques exigent que vous déposiez vos données dans un dépôt reconnu. Ne pas le faire peut compromettre l'acceptation de votre article ou votre admissibilité à des subventions futures.

**Conservation sûre** : Les dépôts de données offrent une infrastructure professionnelle de sauvegarde à long terme. Contrairement à votre compte OneDrive qui pourrait disparaître après votre départ de l'université, un dépôt institutionnel garantit que vos données seront accessibles pendant des décennies.

**Visibilité et impact** : Déposer vos données augmente la visibilité de votre recherche. D'autres chercheurs peuvent découvrir vos données, les citer, et potentiellement les réutiliser pour de nouvelles analyses. Cela augmente l'impact de votre travail au-delà de votre publication initiale.

**Reproductibilité** : La science ouverte repose sur la capacité de reproduire et de vérifier les résultats. En rendant vos données accessibles (avec la documentation appropriée), vous permettez à d'autres chercheurs de reproduire vos analyses et de valider vos conclusions.

**Respect du principe FAIR** : Les données déposées dans un dépôt structuré deviennent facilement **trouvables** (avec des métadonnées), **accessibles** (via un identifiant permanent), **interopérables** (avec des formats standards), et **réutilisables** (avec une licence claire). Ce sont les quatre piliers du principe FAIR qui guide la gestion moderne des données de recherche.

### Borealis : le dépôt Dataverse canadien

Borealis est le dépôt Dataverse canadien, une infrastructure nationale accessible à tous les chercheurs canadiens. Plusieurs institutions canadiennes mettent à la disposition de leur communauté de recherche une collection institutionnelle dans Borealis, avec un accompagnement complet pour les chercheurs qui souhaitent l'utiliser. Borealis accepte tous les types de fichiers, héberge les données sur des serveurs canadiens sécurisés, émet des DOI (identifiants permanents) pour chaque jeu de données, et permet un contrôle précis de l'accès. Vous pouvez choisir de rendre vos données entièrement publiques, de les mettre en embargo pour une période déterminée, ou de les garder en accès restreint si vos contraintes éthiques l'exigent. 

Les données déposées dans Borealis sont repérables via des moteurs de recherche grâce aux métadonnées structurées. Pour utiliser Borealis,  vous créez un compte, vous créez un nouveau jeu de données, vous remplissez les champs de métadonnées, vous téléversez vos fichiers, et vous publiez. À ce moment, un DOI est attribué et vos données deviennent officiellement archivées. Il y a de fortes chances que votre bibliothèque institutionnel offre un support pour l'utilisation de Borealis.

### Autres dépôts multidisciplinaires

Plusieurs autres dépôts multidisciplinaires sont disponibles :

**Dépôt fédéré de données de recherche (DFDR)** : Comme Borealis, le DFDR est une infrastructure canadienne hébergée sur des serveurs canadiens. Il est gratuit pour les projets académiques, accepte tous les types de fichiers, et offre des fonctionnalités similaires à Borealis. Le choix entre Borealis et le DFDR dépend souvent de préférences institutionnelles ou de contraintes techniques spécifiques.

**Zenodo** : Hébergé par le CERN en Suisse, Zenodo est un dépôt multidisciplinaire très populaire en sciences. Il est gratuit jusqu'à 50 Go par jeu de données, offre une intégration étroite avec GitHub (idéal si vous déposez du code), et permet de déposer non seulement des données mais aussi des présentations, des rapports, etc. Zenodo est particulièrement apprécié pour sa simplicité et sa rapidité de dépôt.

**Open Science Framework (OSF)** : OSF est une plateforme complète pour la recherche ouverte. Au-delà du simple dépôt de données, OSF permet de gérer l'ensemble d'un projet de recherche : pré-enregistrement, collaboration, versionnement, liens avec d'autres dépôts. C'est une solution intéressante si vous adoptez une approche de science ouverte dès le début de votre projet.

**Figshare** : Similaire à Zenodo, Figshare offre un dépôt gratuit pour des fichiers de taille modérée, avec attribution de DOI et intégration avec des outils de recherche. Il est particulièrement utilisé pour déposer des figures, des tableaux supplémentaires, ou des jeux de données complémentaires à une publication.

Lorsque vous déposez vos données, vous devez également spécifier sous quelle **licence** vous les partagez. La licence définit ce que les autres chercheurs peuvent faire avec vos données : les télécharger, les réutiliser, les modifier, les republier, etc. Les licences Creative Commons (CC) sont les plus courantes pour les données de recherche.

### Quelles données devriez-vous préserver?

Il n'est pas nécessaire de préserver toutes les données collectées pendant votre projet. Les données à préserver devraient être **réutilisables** (par vous ou d'autres), **compréhensibles** (avec une documentation adéquate), et avoir une **certaine valeur** (complexes, coûteuses à obtenir, ou impossibles à récolter à nouveau). Certaines données peuvent être détruites à la fin du projet : les notes préliminaires, les premières versions de documents, ou du matériel facile à recueillir à nouveau. Les données de recherche qui ne mèneront pas à une publication et qui ont servi pour l'enseignement ou un travail académique n'ont généralement pas besoin d'être conservées après l'obtention du diplôme, sauf si elles ont une valeur pour des projets futurs.

En résumé, le dépôt de données finales est une étape importante qui garantit la pérennité de votre travail, contribue à la science ouverte, et peut augmenter l'impact de votre recherche. 

## Où entreposer des données massives

Les sections précédentes ont couvert les besoins typiques de la plupart des projets de recherche aux cycles supérieurs : quelques gigaoctets de données, des fichiers de taille raisonnable, des outils standards. Mais certains projets génèrent des volumes de données qui dépassent largement ces limites. Si vous travaillez avec des images satellite, du *scraping web* à grande échelle, des enregistrements vidéo, ou des simulations computationnelles, les chances sont élevées que vous utilisiez des **données massives**.

Qu'est-ce qu'une donnée massive en recherche? Il n'y a pas de définition universelle, mais voici quelques termes pratiques qui indiquent que vous entrez dans le domaine des données massives. Un **volume** de important de données; une **vélocité**, c'est-à-dire une collecte continue ou très fréquente de données; de la **variété**, présente dams des données hétérogènes nécessitant un traitement comme des images; ou une  **complexité computationnelle** lorsqu'il y a un besoin de traitements avec plusieurs jours de calcul. Si votre projet correspond à une ou plusieurs de ces caractéristiques, les solutions présentées plus tôt ne conviendront pas. Vous aurez besoin d'infrastructures spécialisées.

### Le cycle de vie des données massives

Les données massives nécessitent une approche structurée qui couvre quatre phases distinctes :

**Collecte** : Le cycle de vie d'une donnée commence au moment où elle est créée ou collectée. Pour des données massives, la collecte est souvent automatisée (scripts de scraping, API). Avant de commencer, déterminez clairement la valeur et la pertinence de vos données. Établissez des règles pour collecter les données d'une manière qui préserve leur utilité : documentez quand, où, comment et pourquoi elles ont été générées. Planifiez l'infrastructure dès le début : avez-vous besoin d'un serveur qui tourne en continu? D'un espace de stockage temporaire pour les données brutes?

**Entreposage** : Les données doivent être entreposées dans un environnement stable et adapté à leurs origines et à leurs applications potentielles. Pour des volumes importants, vous aurez besoin d'espaces de grande capacité, souvent avec des systèmes de fichiers distribués. Toute donnée digne d'être collectée mérite d'être protégée, ce qui suppose des sauvegardes régulières et une infrastructure fiable. Les données sensibles doivent être chiffrées pour respecter les exigences éthiques et réglementaires. C'est à cette étape que les infrastructures de calcul haute performance (comme Calcul Québec) deviennent essentielles, car elles offrent non seulement de l'entreposage massif, mais aussi les ressources computationnelles pour traiter et transformer vos données brutes.

**Partage** : Les données n'ont de valeur que si elles peuvent être mises à la disposition des utilisateurs autorisés. Pour des projets collaboratifs, établissez clairement qui peut accéder aux données, quand et comment. Les utilisateurs doivent pouvoir localiser, accéder, modifier et analyser les données selon les besoins. Documentez vos conventions de nomenclature, vos structures de fichiers, et les transformations appliquées. Cette documentation est cruciale pour que vos collaborateurs puissent comprendre et utiliser les données efficacement.

**Archivage** : À la fin du projet, vos données cessent d'être utilisées quotidiennement, mais elles conservent une valeur à long terme. Pour des données massives, l'archivage complet n'est pas toujours faisable ou nécessaire dans un dépôt comme Borealis. Vous devez décider quelles données conserver : les données brutes irremplaçables, un échantillon représentatif, les données agrégées, ou simplement les scripts et métadonnées permettant de régénérer les résultats. Les données archivées doivent rester organisées et protégées, même si leur accessibilité immédiate perd de son importance.

### Solutions pour données massives

Comme mentionné dans la section précédente, l'Alliance de recherche numérique du Canada opère des centres de calcul haute performance à travers le pays. Pour des données massives, ces infrastructures deviennent non seulement recommandées, mais essentielles. Au-delà du simple stockage sécurisé, elles offrent l'accès à des grappes de serveurs pour traiter vos données, ce qui est crucial lorsque vos analyses nécessitent plusieurs heures ou jours de calcul. Pour des projets nécessitant des ressources substantielles (plusieurs téraoctets de stockage ou des milliers d'heures de calcul), vous devrez soumettre une demande d'allocation annuelle avec justification scientifique lors des concours d'allocation.

Certaines institutions offrent des crédits pour des services infonuagiques académiques (AWS Educate, Google Cloud for Education, Microsoft Azure for Research). Ces services peuvent être utiles pour des projets nécessitant une grande flexibilité ou des outils spécifiques non disponibles sur Calcul Québec. Toutefois, les crédits gratuits sont généralement limités dans le temps et en montant. Comme il a été mentionné plus tôt, certaines facultés ou groupes de recherche disposent de leurs propres serveurs. Si vous faites partie d'un laboratoire avec de telles ressources, c'est souvent la solution la plus simple pour démarrer.

En résumé, les données massives nécessitent une planification et des infrastructures spécialisées. **Calcul Québec et l'Alliance** sont vos meilleurs alliés pour des projets académiques d'envergure. Ne sous-estimez pas la courbe d'apprentissage, mais ne laissez pas cela vous décourager : les ressources et le support sont disponibles. Commencez tôt, documentez rigoureusement, et automatisez autant que possible.

## Arpentage et choix éditorial : Pourquoi Git et GitHub pour votre projet de recherche?

Dans les sections précédentes, nous avons exploré différentes solutions pour entreposer votre rédaction, vos données pendant la recherche, vos données finales, et vos données massives. Chaque solution a ses forces, mais il existe un outil transversal qui mérite une attention particulière, car il peut traverser plusieurs de ces catégories : **Git et GitHub**. Git offre un niveau de contrôle, de traçabilité et de collaboration qui dépasse largement les outils conventionnels. Cette section explique ce qu'est Git, pourquoi il est devenu si populaire en recherche, et comment il pourrait s'intégrer dans votre flux de travail.

### L'histoire de Git : d'un outil de programmeurs à un outil de recherche

Git, développé par Linus Torvalds en 2005, s'est imposé comme le système de gestion de versions décentralisé de référence. Avec l'essor des projets logiciels dans les décennies précédentes, un besoin s'est créé pour suivre l'évolution des fichiers de code au fil du temps. Quand plusieurs centaines de développeurs travaillent sur un même projet, ce suivi est essentiel pour éviter les conflits entre les versions. Bien que des systèmes centralisés existaient pour ce type de gestion, Git se distingue par sa décentralisation : chaque développeur détient une copie complète du projet, incluant toutes les modifications passées. Cela permet aux équipes de travailler de manière indépendante, sans dépendre d'un serveur central, ce qui réduit les risques de conflits. Sa principale force réside dans sa capacité à suivre l'évolution d'un projet en enregistrant les modifications du code source. Chaque modification, ou *commit*, est enregistrée avec un message explicatif et un identifiant unique, permettant aux collaborateurs de comprendre facilement les évolutions du projet et d'assurer l'intégrité des données.

GitHub, lancé en 2008, est né pour répondre aux besoins de collaboration croissants dans le monde du développement du logiciel libre. Avant GitHub, les développeurs pouvaient utiliser Git en local pour gérer les versions, mais il manquait un espace centralisé pour partager le code et coordonner les efforts. GitHub a donc apporté cette solution en intégrant Git à une plateforme web, où les développeurs pouvaient héberger leurs dépôts, collaborer, et suivre le développement en ligne. Ce qui distingue GitHub, c'est son aspect social : les fonctionnalités comme les *pull requests*, les gestionnaires de problèmes, et le suivi des modifications permettent aux équipes de travailler efficacement et d'interagir facilement autour du code. Cette combinaison d’outils a transformé GitHub en un espace incontournable pour les projets en libre accès, facilitant le partage des connaissances et la collaboration.

### De la programmation à la recherche : pourquoi Git pour un mémoire?

Si Git a été créé pour gérer du code informatique, pourquoi un étudiant en sciences sociales devrait-il s'y intéresser? Parce que les problèmes que Git résout ne sont pas spécifiques à la programmation. Ce sont des problèmes universels de **gestion de versions**, de **collaboration**, et de **traçabilité**. Repensez au problème évoqué précédemment : le fichier Analyse_v2_final.R. Ce problème n'existe pas avec Git. À chaque étape significative de votre travail, vous créez un *commit* (une "version enregistrée") avec un message descriptif. Vous n'avez jamais besoin de créer des copies multiples de vos fichiers. Vous travaillez toujours sur la version actuelle, et Git conserve automatiquement tout l'historique. Git et GitHub incarnent efficacement l'évolution vers des méthodes de travail plus flexibles, collaboratives et transparentes. Grâce à leur capacité de gestion de versions et d'historique des modifications, ils permettent aux chercheurs d'embrasser une philosophie Agile en assurant un suivi minutieux des changements, facilitant ainsi l'itération rapide et la reproductibilité des projets.

### Comment fonctionne Git?

Lorsqu'un projet est initialisé avec Git, un dossier caché appelé ".git" est créé dans le répertoire. Ce dossier contient tout l'historique des modifications apportées, y compris les informations sur chaque *commit*, sur les différentes "branches" créées (versions parallèles du projet) et sur les métadonnées associées. Ainsi, même en travaillant avec plusieurs collaborateurs, chacun peut voir quels changements ont été effectués, par qui et pourquoi, tout en s'assurant qu'aucune version de leur travail ne soit perdue.

Contrairement aux outils comme OneDrive ou Google Docs qui font du versionnement automatique en arrière-plan, Git exige que vous enregistriez explicitement vos changements. Cela peut sembler contraignant au début, mais c'est en réalité une force : vous décidez consciemment quand créer une "version" et vous documentez pourquoi. Ce processus explicite crée une trace historique beaucoup plus riche et utile.

Git et GitHub favorisent le travail collaboratif de manière plus structurée que les outils conventionnels. Plusieurs chercheurs peuvent travailler sur le même projet simultanément, chacun dans sa branche de développement. Une fois les modifications effectuées, il est possible de fusionner les branches pour intégrer les changements. Cette approche évite les conflits majeurs. Un conflit survient lorsque deux collaborateurs modifient et enregistrent le même fichier ou la même partie de texte de manière indépendante, comme cela peut arriver dans un dossier Dropbox partagé. Ainsi, contrairement aux outils classiques comme Microsoft Word, où deux personnes peuvent se retrouver avec des versions distinctes d'un même document et perdre beaucoup de temps à fusionner manuellement les modifications, Git identifie automatiquement les zones conflictuelles et aide à les résoudre de façon plus méthodique.

### Pour quels types de fichiers Git est-il adapté?

Git fonctionne mieux avec des fichiers **textuels** : code, documents en format texte, fichiers de configuration, scripts, etc. Pour ces types de fichiers, Git peut montrer exactement quelles lignes ont changé entre deux versions, ce qui est extrêmement utile. Git fonctionne moins bien avec des fichiers **binaires** comme Word (.docx), Excel, PDF, des images ou des bases de données. Ces fichiers peuvent être stockés dans Git, mais vous perdez la capacité de voir les changements ligne par ligne. Pour un mémoire rédigé en Word, Git peut quand même être utile (il garde l'historique des versions du fichier), mais vous n'exploitez pas pleinement ses capacités.

C'est pourquoi Git est particulièrement populaire auprès des chercheurs qui :
- Rédigent en LaTeX ou Markdown
- Écrivent du code pour leurs analyses
- Collaborent sur des projets techniques avec plusieurs contributeurs
- Valorisent la traçabilité complète de leur travail

### Les avantages de Git et GitHub pour la recherche

En sciences sociales numériques, où le partage et la collaboration sont essentiels, Git et GitHub offrent plusieurs avantages majeurs. <!-- Tout d'abord, ils permettent de suivre les modifications apportées au texte ou au code, ce qui facilite la reproductibilité des résultats. Les chercheurs peuvent revenir à n'importe quelle version précédente de leur projet, une fonctionnalité particulièrement utile pour corriger des erreurs ou analyser l'impact de différentes approches. Lorsqu'un projet est initialisé avec Git, un dossier caché appelé .git est créé dans le répertoire. Ce dossier contient tout l'historique des modifications apportées, y compris les informations sur chaque *commit*, sur les différentes « branches » créées (versions parallèles du projet) et sur les métadonnées associées. Ainsi, même en travaillant avec plusieurs collaborateurs, chacun peut voir quels changements ont été effectués, par qui et pourquoi, tout en s'assurant qu'aucune version de leur travail ne soit perdue. -->

<!-- Git et GitHub favorisent le travail collaboratif. Plusieurs chercheurs peuvent travailler sur le même projet simultanément, chacun dans sa branche de développement. Une fois les modifications effectuées, il est possible de fusionner les branches (encore une fois, comprendre ici les différentes « versions ») pour intégrer les changements. Cette approche évite les conflits majeurs. Un conflit survient lorsque deux collaborateurs modifient et enregistrent le même fichier ou la même partie de texte de manière indépendante, comme cela peut arriver dans un dossier Dropbox partagé. Ainsi, contrairement aux outils classiques comme Microsoft Word, où deux personnes peuvent se retrouver avec des versions distinctes d'un même document et perdre beaucoup de temps à fusionner manuellement les modifications, Git identifie automatiquement les zones conflictuelles et aide à les résoudre de façon plus méthodique. -->

1.  *Intégration et adoption répandue* : Git est devenu un standard de facto dans l'industrie du développement logiciel. Sa popularité et son adoption répandue signifient que de nombreuses ressources d'apprentissage, des tutoriels et des forums de support sont disponibles en ligne, ce qui facilite l'utilisation de cet outil pour les chercheurs en sciences sociales débutants. GitHub, en tant que plateforme principale de gestion des versions, bénéficie également d'une grande base d'utilisateurs et d'une communauté active, ce qui encourage la collaboration et le partage des connaissances.

2.  *Facilité de collaboration* : Git et GitHub sont conçus pour faciliter la collaboration entre les individus et les équipes. Les chercheurs en sciences sociales travaillent souvent ensemble sur des projets de recherche, et la capacité de suivre les modifications, de gérer les conflits et de fusionner les contributions devient essentielle. L'interface conviviale de GitHub, avec des fonctionnalités telles que les demandes de fusion et les commentaires en ligne, simplifie grandement la collaboration.

3.  *Suivi des versions et recherche reproductible* : Les chercheurs en sciences sociales doivent s'assurer que leurs travaux sont reproductibles et vérifiables. Git permet de suivre les versions du code, ce qui signifie que les chercheurs peuvent retrouver facilement des versions antérieures pour reproduire des analyses spécifiques ou corriger des erreurs. Cette fonctionnalité est cruciale pour maintenir l'intégrité des résultats de recherche.

4.  *Infrastructure et sécurité* : GitHub offre une infrastructure robuste pour l'entreposage sécurisé des dépôts Git. Les chercheurs peuvent être assurés que leurs travaux sont sauvegardés et protégés contre les pertes de données accidentelles. De plus, les contrôles d'accès et les autorisations granulaires de GitHub permettent aux chercheurs de contrôler qui peut accéder et contribuer à leurs projets.

Bien qu'il existe plusieurs alternatives à l'utilisation combinée de Git et de GitHub sur le marché, ces deux plateformes liées continuent de dominer le domaine de la gestion de versions décentralisée. Parmi les alternatives notables, on peut citer Mercurial, Bitbucket, GitLab et SourceForge. Chacun de ces outils offre des fonctionnalités similaires à celles de Git et GitHub. GitHub n'est pas un logiciel libre, mais il est gratuit pour ses fonctionnalités essentielles, et est abondamment utilisé pour y déposer des codes sources ouverts. Cela permet aux chercheurs en sciences sociales numériques de partager leurs textes et leurs codes avec la communauté académique et de bénéficier des contributions d'autres chercheurs. Cela favorise un environnement de partage des connaissances et de collaboration fructueuse.

En somme, Git et GitHub offrent aux chercheurs en sciences sociales numériques un moyen puissant de gérer leur code, de collaborer efficacement et de contribuer à la communauté académique grâce au logiciel libre. Bien que leur apprentissage puisse représenter un défi initial, les avantages qu'ils apportent en termes de suivi des versions, de collaboration et de partage des connaissances en font des outils essentiels dans l'arsenal de tout chercheur moderne.

<!-- Lorsqu'il s'agit d'entreposer vos données de recherche, la règle d'or est de ne jamais perdre d'informations précieuses. Cette préoccupation prend toute son importance lorsqu'un chercheur en sciences sociales, seul ou en équipe restreinte, se lance dans un projet. Pour répondre à ce besoin, les services d'entreposage infonuagiques se révèlent indispensables. Voici quelques avantages d'un entreposage sur le nuage pour la recherche :

1.  *Sécurité des données* : Que vous travailliez avec des données sensibles ou non, la sécurité est essentielle pour protéger l’intégrité de vos fichiers. De bons outils garantissent que vos données ne sont pas exposées à des risques tels que des piratages ou des pertes accidentelles.

2.  *Collaboration et partage* : En particulier dans le cadre de collaborations interinstitutionnelles, il est indispensable d’avoir des outils qui permettent de partager facilement des fichiers avec vos collègues, où qu’ils se trouvent.

3.  *Sauvegarde et versionnage* : Une bonne gestion des données permet de garder des copies de sauvegarde de vos fichiers et de suivre l’historique des versions, vous offrant la possibilité de revenir à une version antérieure si nécessaire.

Pour répondre aux besoins variés des chercheurs, des dizaines d'outils de gestion de données ont émergé. Nous allons ici comparer quatre d'entre eux : Dropbox, Google Drive, Nextcloud, et Microsoft OneDrive.

Dropbox est très utilisé dans plusieurs milieux, même à titre d'utilisation personnelle. Dans le milieu académique, le partage de fichiers peu volumineux par Dropbox est très fréquent. Son utilisation est relativement accessible : 2 Go sont offerts gratuitement, puis un abonnement est nécessaire pour entreposer d'avantage. Dropbox se démarque par sa popularité et par son interface intuitive, s'intégrant facilement dans le gestionnaire de fichiers. Bien que ce soit simple d'utilisation, Dropbox ne permet pas une grande flexibilité et réplicabilité dès qu'il y a un besoin plus important pour des données à gros volume ou plus sensible. Dropbox est ainsi très utile dans le cadre de gestion de données collaborative relativement simple.

Google Drive est très facile d'utilisation par son intégration au sein des outils Google. La plateforme, en générale, est fréquemment utilisée dans le cadre de rédaction, pour son suivi des modifications et la possibilité de travailler à plusieurs sur un document en même temps. Google Drive offre 15 Go gratuits, et son utilisation est simple et intuitive. Cependant, son utilisation en local n'est pas très adaptée. Il nécessite souvent une interaction via son interface web pour le téléchargement ou le partage de fichiers. Si vous codez des analyses statistiques et manipulez des jeux de données en R, par exemple, Google Drive est peu adapté pour la sauvegarde de jeux de données directement dans votre arborescence.

Microsoft OneDrive Fait partie de la suite Microsoft 365. C'est donc un choix judicieux pour les utilisateurs des outils Microsoft, pour leur compatibilité. Comme Dropbox, OneDrive permet la sauvegarde de fichiers en local, ce qui facilité son utilisation pour l'entreposage de fichiers manipulés avec un language de programmation. OneDrive est généralement perçu comme étant moins simple et rapide pour sa synchronisation que Dropbox, mais sa valeur de centralisation est indégnable si la suite Microsoft est employée.

Nextcloud est une plateforme de stockage libre d'accès qui se distingue par sa flexibilité et son contrôle personnalisé. Contrairement à des services comme Dropbox, Nextcloud permet aux utilisateurs d’héberger leurs fichiers eux-mêmes, ce qui garantit une meilleure sécurité et confidentialité des données. C’est idéal pour les chercheurs qui veulent un accès libre et plus de contrôle sur leurs données. Cependant, Nextcloud nécessite plus de compétences techniques pour l'installation et la gestion, ce qui peut être un obstacle pour ceux qui recherchent une solution "clé en main". -->


<!-- ### Entreposage de données sensibles

Lorsqu'il s'agit d'entreposer des données sensibles, tels que des données de sondage comportant des informations personnelles identifiables, la sécurité et la confidentialité sont essentielles. Comme abordé précédemment, GitHub n'est pas adapté à l'entreposage de telles données en raison de ses caractéristiques publiques et de son orientation vers le code source ouvert. Une solution courante est d'utiliser des services de cloud sécurisés, tels qu'AWS, qui offrent des mesures de sécurité robustes pour protéger vos données sensibles.

AWS regroupe un ensemble de services *cloud*  proposés par Amazon. Il offre une vaste gamme de services, allant de l'entreposage et de la gestion des données à la computation et à l'analyse avancée. AWS est conçu pour offrir une infrastructure hautement évolutive et sécurisée, ce qui en fait un choix attrayant pour les chercheurs qui gèrent des données sensibles. L'outil présente de multiples avantages:

1.  *Sécurité robuste* : AWS met l'accent sur la sécurité, avec des fonctionnalités telles que le chiffrement des données en transit et au repos, la gestion des accès basée sur les rôles et la conformité à des normes de sécurité strictes.

2.  *Scalabilité* : AWS permet de faire évoluer vos ressources en fonction des besoins, garantissant des performances optimales même lorsque vos projets de recherche croissent en taille et en complexité.

3.  *Flexibilité* : AWS propose une variété de services adaptés à différentes utilisations, allant de l'entreposage de données au calcul intensif pour l'analyse avancée.

4.  *Collaboration simplifiée* : Bien que le coût d'entrée soit généralement bas, la possibilité de partager des ressources avec des collègues et de travailler en équipe rend AWS adapté à la collaboration.

AWS n'est pas le seul service cloud disponible. Microsoft Azure et Google Cloud Platform (GCP) sont des concurrents majeurs offrant des fonctionnalités similaires. Lorsque vous choisissez un fournisseur, prenez en compte les coûts, la convivialité et les fonctionnalités offertes. Le coût d'utilisation d'AWS peut varier en fonction des services utilisés, de la quantité de données entreposées et de la capacité de calcul requise. Lorsque vous travaillez seul, le coût peut sembler élevé par rapport à l'utilisation de solutions gratuites telles que Dropbox. Cependant, en équipe, la répartition des coûts peut rendre AWS plus abordable. -->


<!-- L'entreposage des données est une étape cruciale dans la recherche en sciences sociales numériques. Choisissez des outils adaptés à la sensibilité des données, privilégiez des services sécurisés comme AWS pour les données sensibles, et utilisez Dropbox pour la collaboration et l'entreposage de fichiers non sensibles. Une gestion efficace des versions, de la structure des dossiers et de la sécurité garantira l'intégrité de vos données et facilitera la collaboration tout au long de vos projets de recherche. -->
<!--
```{r}
#| echo: false
#| warning: false
library(tidyverse)
library(tinytable)

# Second table
tbl_resume_chap_Y <- tibble(
    "Critères" = c('Accessibilité (Gratuit ou peu dispendieux)', 
                 'Existence d\'une communauté d\'utilisateurs', 
                 'Popularité dans le champ', 
                 'Compatibilité avec d\'autres outils', 
                 'Transparence et réplicabilité',
                 'Adaptabilité et flexibilité'),
    "Dropbox" = c('Limitée - Abonnement mensuel', 
                'Oui', 
                'Oui', 
                'Oui', 
                'Limitée', 
                'Limitée'),
    "Google Drive" = c('Oui', 
                     'Oui', 
                     'Oui', 
                     'Avec les outils Google', 
                     'Moyenne',
                     'Oui'),
    "OneDrive" = c('Limitée', 
                 'Oui', 
                 'Limitée', 
                 'Avec les outils Microsoft', 
                 'Moyenne',
                 'Moyenne'),
    "Nextcloud" = c('Oui - Si auto-hébergé', 
                    'Oui', 
                    'Limitée', 
                    'Oui, avec configuration', 
                    'Oui', 
                    'Très flexible')
)

tt(tbl_resume_chap_Y, width = 0.75, caption = 'Résumé des principaux outils de gestion de données')
```
-->
### Manuel d'instructions : Git et GitHub pour votre projet de recherche

Maintenant que vous comprenez pourquoi Git et GitHub peuvent être utiles pour votre recherche, passons à la pratique. Cette section vous guidera dans les étapes essentielles pour mettre en place Git pour un projet typique de recherche en sciences sociales.

**Prérequis** : Vous devez d'abord installer Git sur votre ordinateur (téléchargeable depuis [git-scm.com](https://git-scm.com)) et créer un compte GitHub gratuit sur [github.com](https://github.com).

**Étape 1** : Création d'un répertoire local et initialisation de Git

Ouvrez votre terminal (sur macOS et Linux) ou l'application Git Bash (sur Windows) et naviguez vers le dossier où vous souhaitez enregistrer votre projet.

```bash
cd chemin/vers/votre/dossier
```

Créez un nouveau répertoire pour votre projet et accédez-y.

```bash
mkdir mon_projet
cd mon_projet
```

Initialisez Git dans ce répertoire.

```bash
git init
```

**Étape 2** : Structurer votre projet

Une bonne organisation facilite la gestion à long terme. Créez une structure logique :

```bash
mkdir redaction
mkdir code
mkdir data
mkdir resultats
mkdir documentation
```

Votre projet ressemblera maintenant à ceci :

```
mon_projet/
├── redaction/          # Chapitres de votre mémoire
├── code/               # Scripts d'analyse (R, Python)
├── data/               # Données NON SENSIBLES uniquement
├── resultats/          # Graphiques et tableaux générés
└── documentation/      # README, notes méthodologiques
```

**Important** : Le dossier `data/` ne devrait contenir que des données **non sensibles**. Les données sensibles restent dans Nextcloud ou OneDrive institutionnel.

**Étape 3** : Le fichier .gitignore

Créez un fichier `.gitignore` à la racine de votre projet pour indiquer à Git quels fichiers ne doivent **jamais** être ajoutés au dépôt :

```
# Ne JAMAIS pousser sur GitHub:
data/entrevues/              # Données sensibles
data/sondages_bruts/         # Données avec infos personnelles

# Fichiers système et temporaires
.DS_Store
.Rhistory
*.tmp
~$*.docx
```

**Étape 4** : Ajout de votre code et de vos fichiers

Ajoutez vos fichiers R contenant le code pour vos analyses dans le répertoire. Par exemple, vous pouvez avoir des fichiers `analyse_medias.R` et `analyse_sondages.R`.

Utilisez la commande `git status` pour vérifier l'état de vos fichiers.

```bash
git status
```

**Étape 5** : Ajout, validation et dépôt de vos modifications

Ajoutez vos fichiers pour qu'ils soient prêts à être validés.

```bash
git add -A
```

Validez vos modifications avec un message descriptif.

```bash
git commit -m "Structure initiale du projet avec code d'analyse"
```

**Étape 6** : Création du répertoire sur GitHub et du lien avec votre répertoire local

Allez sur GitHub et connectez-vous à votre compte. Créez un nouveau répertoire (privé ou public selon vos besoins) avec le nom de votre projet.

De retour dans votre terminal, ajoutez le lien GitHub à votre répertoire local.

```bash
git remote add origin https://github.com/votre-utilisateur/mon_projet.git
```

**Étape 7** : Pousser votre travail sur GitHub

Envoyez vos dépôts locaux vers GitHub.

```bash
git push -u origin main
```

**Étape 8** : Collaboration avec vos collègues

Si vos collègues souhaitent contribuer à votre projet, ils peuvent dupliquer (en langage Git, on utilise le terme *Fork*) votre répertoire sur GitHub, ce qui créera une copie dans leur propre compte.

Lorsqu'ils ont fait des modifications dans leur copie, ils peuvent soumettre une *demande de pull request* pour vous demander la permission de fusionner leurs modifications dans votre répertoire principal.

**Étape 9** : Acceptation des modifications de vos collègues

Lorsque vos collègues ont soumis des modifications et vous ont demandé de les fusionner, vous pouvez mettre à jour votre répertoire local avec leurs changements.

```bash
git pull origin main
```

**Étape 10** : Répéter le processus

Répétez les étapes au fur et à mesure que vous développez votre projet, ajoutez du code, effectuez des analyses et collaborez avec vos collègues. Assurez-vous de valider et de pousser régulièrement vos modifications pour maintenir le répertoire à jour. Faites des commits fréquents avec des messages clairs plutôt qu'un gros commit en fin de semaine.

### Alternative : GitHub Desktop et intégrations

Alors que le terminal reste une approche fondamentale pour maîtriser Git et GitHub, il existe des outils conviviaux tels que **GitHub Desktop** qui offrent une alternative intuitive. Cet outil simplifie le processus de gestion de versions décentralisée, en particulier pour ceux qui souhaitent commencer par une approche visuelle. La plupart des environnements de code comme RStudio et VSCode ont également des interfaces pour faciliter ces opérations.

```{r, out.width="80%"}
#| label: fig-github
#| echo: false
#| fig-cap: "GitHub Desktop facilite l'utilisation conjointe de Git et de GitHub."
knitr::include_graphics("images/chapitre8_gitdesktop.png", dpi = 600)
```

GitHub Desktop fournit une vue claire de vos répertoires, de vos modifications, de vos branches et de vos demandes de fusion (*pull requests*). Il élimine la nécessité de mémoriser les commandes en ligne de terminal, ce qui peut être un défi pour certains chercheurs. L'application simplifie également la résolution des conflits lors de la fusion des branches.

Toutefois, en utilisant GitHub Desktop, il est possible de perdre la compréhension des commandes Git en ligne de commande, ce qui pourrait devenir un inconvénient si vous devez travailler dans un environnement sans interface visuelle. De plus, GitHub Desktop est spécifiquement conçu pour interagir avec GitHub. Si vous devez travailler avec d'autres plateformes de gestion de versions, cela pourrait poser des problèmes.

La décision entre l'utilisation du terminal et de GitHub Desktop dépend de vos préférences et de vos besoins. Pour les chercheurs qui débutent, GitHub Desktop offre une transition en douceur vers les concepts de gestion de versions. Cependant, il est important de ne pas se limiter à une interface visuelle. Comprendre les commandes Git en ligne de commande reste essentiel pour résoudre des problèmes complexes, gérer des projets avancés et collaborer avec d'autres chercheurs qui utilisent des approches basées sur le terminal.



\<!-- \`\`\`
