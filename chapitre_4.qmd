# Les outils de gestion de données : de l'entreposage personnel à la science ouverte {#sec-chap4}

\begin{center}
\textit{Adrien Cloutier (Université Laval), \\
Jérémy Gilbert (Western University)}
\end{center}

En début de parcours aux cycles supérieurs, le chercheur se trouve généralement dans une position où il doit amorcer la rédaction d'un mémoire ou d'une thèse. Il a peut-être commencé à prendre des notes sur la littérature, à créer des fichiers de codes, à collecter des données, à collaborer avec des coauteurs. Rapidement, une question simple, mais cruciale se pose : dans une optique de respect des valeurs scientifiques, **où entreposer tout ce contenu ?**

## Point d'observation : brève histoire de la gestion des données

La transparence et la réplicabilité sont des valeurs fondamentales de la recherche scientifique. Comme le soulignent @king_etal94 dans leur ouvrage de référence *Designing Social Inquiry*, les « procédures doivent être publiques » pour permettre la validation et la critique des résultats. Sans accès aux données, aux méthodes et aux processus ayant mené à une conclusion, la communauté scientifique ne peut ni reproduire ni vérifier les travaux de ses pairs.

Pourtant, ces idéaux ont longtemps été difficiles, voire impossibles à atteindre. C'est ce constat qui a donné naissance au mouvement de la **science ouverte**. Comme l'explique le gouvernement du Canada :

> « La science ouverte n'est pas la façon dont la science a été historiquement conduite. La tradition, la culture et les mesures incitatives ont créé un environnement dans lequel les apports, les résultats et les processus scientifiques sont soit fermés, soit accessibles moyennant des frais, soit uniquement accessibles aux chercheurs et à leurs collaborateurs eux-mêmes. La science ouverte est un mouvement visant à rendre les processus et pratiques scientifiques, y compris la méthodologie et les résultats de la recherche, plus ouverts et transparents. »^[https://science.gc.ca/site/science/fr/science-ouverte]

Jusque dans les années 2000, chaque chercheur développait ses propres méthodes d'organisation, souvent artisanales : dossiers sur le bureau, copies de sauvegarde sur disque dur externe, fichiers envoyés par courriel aux collaborateurs. La démocratisation d'Internet dans les années 1990, puis l'arrivée de l'infonuagique et des outils collaboratifs dans les années 2010, ont progressivement rendu possible ce que les contraintes technologiques empêchaient auparavant : une gestion des données transparente, accessible et sécuritaire.

Ce mouvement s'est concrétisé par des politiques à tous les niveaux. À l'échelle internationale, 194 pays ont adopté à l'UNESCO en 2021 la *Recommandation sur une science ouverte*^[https://unesdoc.unesco.org/ark:/48223/pf0000379949_fre], un cadre mondial soulignant l'importance de rendre la science plus accessible et transparente pour tous. Au Canada, la conseillère scientifique en chef, Mona Nemer, a publié la *Feuille de route pour la science ouverte*^[https://www.canada.ca/fr/innovation-sciences-developpement-economique/nouvelles/2020/02/feuille-de-route-pour-la-science-ouverte-visant-a-reduire-les-obstacles-et-a-accelerer-les-decouvertes.html] en 2020, suivie en 2021 par la **Politique des trois organismes sur la gestion des données de recherche**^[https://science.gc.ca/site/science/fr/financement-interorganismes-recherche/politiques-lignes-directrices/gestion-donnees-recherche/politique-trois-organismes-gestion-donnees-recherche] (CRSNG, IRSC et CRSH). Au Québec, les Fonds de recherche du Québec (FRQ) se sont également dotés d'une ambitieuse **Politique de diffusion en libre accès**^[https://frq.gouv.qc.ca/science-ouverte/politique-diffusion-2022/] en 2019, mise à jour en 2022. 

Toutes ces politiques lient de plus en plus directement le financement de la recherche à des responsabilités claires en matière de gestion des données. Règle générale, tout chercheur financé par ces organismes subventionnaires est maintenant tenu de rédiger un **plan de gestion des données**, de déposer ses données finales dans un **dépôt sécurisé** à la fin du projet pour en assurer la pérennité et l'accessibilité, et de rendre disponible en libre accès ses publications, codes et résultats.

Même en l'absence de financement ou d'une obligation formelle, ces principes constituent des bonnes pratiques pour tout projet de recherche. Une saine gestion des données, planifiée avant même le début du projet et maintenue tout au long du cycle de la recherche jusqu'à l'archivage final, est ce qui garantit la transparence, la reproductibilité et l'intégrité de la démarche scientifique. La gestion des données n'est plus une pratique marginale, optionnelle, complexe ou impossible, mais un élément central de la conduite *responsable* de la recherche. La question « où entreposer ses données ? » n'est plus seulement une question pratique de sauvegarde personnelle. Elle touche désormais à trois dimensions fondamentales :

**Transparence** : La reproductibilité des résultats de recherche exige de pouvoir accéder à chaque étape détaillée de l'évolution d'un projet.

**Accessibilité** : Les projets de recherche impliquent régulièrement un partage de dossier entre de multiples chercheurs, puis avec la communauté scientifique et avec le public.

**Sécurité** : Les chercheurs manipulent une quantité croissante de données, dont certaines sont sensibles ou confidentielles.

La façon d'entreposer, de gérer et de partager ses données a donc un impact direct sur la qualité de la recherche, sur la capacité à collaborer efficacement, et sur la conformité aux exigences de l'institution et des organismes subventionnaires. 

Pour prendre de bonnes décisions en matière de gestion des données, il est utile de situer ses besoins dans le cycle de vie de la recherche. Plutôt que de multiplier les outils au fil du projet, le chercheur gagne à réfléchir dès le départ à une stratégie cohérente qui répond aux exigences de la science ouverte. Pour chaque phase du cycle, la question de l'entreposage se pose différemment. Dans les sections qui suivent, nous explorons ces phases à travers des questions pratiques d'entreposage. Nous consacrons ensuite une attention particulière à **`Git` et *GitHub***, des outils transversaux qui permettent de gérer l'ensemble du cycle avec une seule solution : code, données, documentation et collaboration.

## Arpentage et choix éditorial : où entreposer ses données à chaque phase du cycle

### Planification

Dès l'étape de **planification**, le chercheur est parfois contraint à rédiger un plan de gestion des données, voire à enregistrer ses hypothèses. *Comment organiser ses fichiers dès le départ pour conserver des traces de chaque décision et de chaque étape du processus de recherche ?*

Cette réflexion en amont évite de se retrouver avec des données éparpillées sur plusieurs plateformes, des versions contradictoires de fichiers, ou une documentation insuffisante pour permettre la réplication.

Les outils de rédaction (où écrire son mémoire, quelle plateforme utiliser pour collaborer avec son directeur) sont détaillés au @sec-chap8. Ici, l'enjeu est différent : il s'agit de planifier simultanément la gestion du texte, des données, des analyses et de leurs multiples versions dès le départ.

Des outils comme *Open Science Framework* (*OSF*)^[https://help.osf.io/article/330-welcome-to-registrations] offrent une plateforme complète pour la recherche ouverte, permettant de gérer l'ensemble d'un projet de recherche, incluant la préinscription.

La **préinscription** consiste à planifier sa recherche avant de commencer et à soumettre ce plan à un registre public. Les hypothèses de départ, dérivées de la littérature, sont enregistrées sur la plateforme, et toute modification au devis initial peut être retracée. Cette pratique améliore la qualité et la transparence des recherches, renforce la crédibilité des résultats, et facilite la reproductibilité. Elle permet également de revendiquer ses idées plus tôt et de distinguer clairement la recherche exploratoire (générer des hypothèses) de la recherche confirmatoire (tester des hypothèses). Cette pratique est de plus en plus répandue en recherche quantitative et expérimentale, où les hypothèses et les analyses statistiques peuvent être spécifiées à l'avance. En recherche qualitative ou exploratoire, la préinscription est moins courante, car la démarche repose souvent sur une approche itérative où les questions de recherche évoluent au contact des données.

Outre *OSF*, de multiples plateformes offrent l'enregistrement des hypothèses, comme *AsPredicted*^[https://aspredicted.org/] ou le *Center for Open Science*^[https://www.cos.io/initiatives/prereg].

Au-delà de la préinscription des hypothèses, plusieurs organismes subventionnaires exigent maintenant la rédaction d'un **plan de gestion des données** (PGD) dès la demande de financement. Ce document décrit comment les données seront collectées, organisées, documentées, entreposées et éventuellement partagées tout au long du projet. Au Canada, l'**Assistant PGD**^[https://dmp-pgd.ca/] est l'outil officiel développé par l'Alliance de recherche numérique pour guider les chercheurs dans cette démarche. Il propose des modèles adaptés aux exigences des trois organismes fédéraux (CRSH, CRSNG, IRSC) et de plusieurs institutions.

### Collecte et analyse

Le chercheur collecte, manipule et analyse des données. Ces données peuvent prendre plusieurs formes : transcriptions d'entrevues, enregistrements audio, données de sondages, bases de données statistiques, scripts de code, de textes, d'images, de tableaux, etc. Les données de recherche peuvent poser des défis spécifiques en matière de sécurité, d'éthique et de collaboration.

La question centrale ici n'est pas seulement « où entreposer ? », mais surtout **« où peut-on entreposer selon la nature des données ? »** Car toutes les solutions ne se valent pas, et certaines sont inappropriées selon le type de données manipulées.

Avant de choisir une solution d'entreposage, il faut d'abord déterminer si les données sont **sensibles** ou non. Cette distinction n'est pas qu'une question théorique : elle a des implications éthiques et pratiques importantes.

Une donnée est considérée comme sensible si elle permet d'identifier directement ou indirectement des individus, ou si elle contient des informations confidentielles ou privées. Voici des exemples courants en sciences sociales :

- **Entrevues ou groupes de discussion** contenant des informations personnelles (noms, lieux, événements personnels)
- **Sondages ou questionnaires** incluant des données démographiques détaillées qui pourraient identifier des participants
- **Données administratives** obtenues auprès d'organisations (dossiers médicaux, dossiers scolaires, données d'entreprise)
- **Enregistrements audio ou vidéo** de participants identifiables
- **Données géolocalisées** précises (traces GPS, adresses)

Dans les universités canadiennes, la gestion de données sensibles est encadrée par un comité d'éthique de la recherche. Lorsqu'un chercheur soumet son protocole de recherche pour approbation éthique, il doit spécifier comment il va collecter, entreposer, protéger et éventuellement détruire ses données sensibles. Le non-respect de ces engagements peut avoir des conséquences sérieuses, allant du retrait de l'approbation éthique à des sanctions institutionnelles.

Les principes à respecter pour les données sensibles incluent la minimisation (ne collecter que les données nécessaires), l'anonymisation (retirer ou masquer les identifiants dès que possible), le chiffrement (protéger les fichiers par mot de passe ou chiffrement), l'accès limité (seules les personnes autorisées dans le certificat éthique peuvent accéder aux données) et l'entreposage sécurisé (utiliser des serveurs approuvés, idéalement situés au Canada).

À l'inverse, certaines données ne posent pas de problème de confidentialité. C'est le cas des statistiques publiques, des corpus textuels ouverts comme les articles de presse ou les transcriptions de débats parlementaires, des données administratives agrégées sans possibilité d'identification, ou encore du code d'analyse (scripts $\textsf{R}$ par exemple). Pour ces données, les contraintes sont beaucoup plus souples. On peut les entreposer sur des plateformes grand public, les partager plus librement, et éventuellement les rendre publiques sans approbation éthique spécifique.

#### Solutions d'entreposage pour données sensibles

Pour les données sensibles, il est essentiel de privilégier des solutions institutionnelles qui respectent les exigences éthiques et légales. La plupart des universités canadiennes offrent des services d'entreposage sécurisé à leur communauté de recherche.

***OneDrive* institutionnel (avec serveurs canadiens)** : Plusieurs universités canadiennes ont négocié des ententes avec *Microsoft* pour que les données de leur *OneDrive* institutionnel soient hébergées sur des serveurs situés au Canada. Cette configuration respecte généralement les exigences des comités d'éthique. Il est important de vérifier auprès de son institution si cette option est disponible et si elle est approuvée pour les données sensibles.

***Nextcloud* institutionnel** : Certaines universités offrent *Nextcloud*, une plateforme de stockage libre qui peut être hébergée sur les serveurs de l'institution. C'est souvent la solution privilégiée pour les données hautement confidentielles, car les fichiers ne quittent jamais l'infrastructure universitaire. *Nextcloud* offre des fonctionnalités similaires à *Dropbox* (synchronisation, partage de fichiers), mais avec un contrôle total sur l'emplacement des données.

**Serveurs de recherche dédiés** : Certaines facultés ou groupes de recherche disposent de leurs propres serveurs sécurisés. Ces infrastructures sont particulièrement adaptées aux projets impliquant des données très sensibles ou des volumes importants.

Avant de choisir une solution pour des données sensibles, il convient de consulter le bureau de soutien à la recherche de son institution pour connaître les options approuvées et les procédures à suivre.

#### Solutions d'entreposage pour données non sensibles

Si les données ne sont pas sensibles, le chercheur dispose de beaucoup plus de flexibilité dans le choix de ses outils. *Dropbox* est très utilisé dans plusieurs milieux, même à titre d'utilisation personnelle. Dans le milieu académique, le partage de fichiers peu volumineux par *Dropbox* est très fréquent. Son utilisation est relativement accessible : 2 Go sont offerts gratuitement, puis un abonnement est nécessaire pour entreposer davantage. *Dropbox* se démarque par sa popularité et par son interface intuitive, s'intégrant facilement dans le gestionnaire de fichiers. Sa synchronisation est rapide et familière pour la plupart des utilisateurs, bien que sa capacité gratuite soit limitée et que ses serveurs américains le rendent problématique pour des données sensibles. *Dropbox* est ainsi très utile dans le cadre de gestion de données collaborative relativement simple.

*Google Drive* est facile d'utilisation par son intégration au sein des outils *Google*. La plateforme, en général, est fréquemment utilisée dans le cadre de rédaction, pour son suivi des modifications et la possibilité de travailler à plusieurs sur un document en même temps. *Google Drive* offre 15 Go gratuits, et son utilisation est simple et intuitive. Pour des projets impliquant du code et des données non sensibles de taille modérée, *GitHub* et *GitLab* sont d'excellentes options. Nous y reviendrons en détail plus loin, mais sachez qu'ils permettent non seulement d'entreposer les fichiers, mais aussi de suivre rigoureusement leur évolution et de collaborer efficacement grâce au versionnement et aux demandes de fusion (*pull requests*). Ils sont gratuits, tant pour les projets publics que pour les dépôts privés (avec certaines limitations), et sont très utilisés en recherche. Ils nécessitent toutefois une courbe d'apprentissage et ne sont pas adaptés pour des fichiers très volumineux.

### Archivage et partage

Les deux sections précédentes concernaient les données **actives** : celles que le chercheur utilise quotidiennement pendant sa recherche. Mais arrive un moment où le projet se termine, le mémoire est déposé, et le chercheur s'apprête peut-être à publier un article. C'est à ce stade que les données deviennent **finales** et qu'il faut penser à leur archivage et à leur partage potentiel.

Le dépôt sécurisé suit une logique scientifique : cela rend les données trouvables, citables et potentiellement réutilisables par d'autres chercheurs, ce qui contribue à l'avancement de la science de façon *transparente*.

Les **données actives** sont celles que le chercheur manipule pendant sa recherche. Elles évoluent constamment : il ajoute de nouvelles observations, corrige des erreurs, crée de nouvelles variables. Ces données vivent dans son espace de travail.

Les **données finales** sont celles qui accompagnent une publication ou un mémoire déposé. Elles ne changeront plus, ou très peu. Au moment où elles sont hébergées dans un dépôt et qu'un identifiant unique leur est attribué (un DOI, par exemple), elles sont figées. Toute modification ultérieure entraîne la création d'une nouvelle version des données.

Cette distinction est importante parce qu'elle détermine où et comment entreposer les données. Les outils utilisés pendant la recherche (*Nextcloud*, *GitHub*) ne sont pas conçus pour l'archivage. À l'inverse, les dépôts de données (*Borealis*, *Zenodo*, etc.) ne sont pas pratiques pour un travail quotidien, mais excellent pour la préservation et le partage.

Plusieurs raisons justifient le dépôt de données finales :

**Conformité aux exigences** : Certains organismes subventionnaires et certaines revues académiques exigent le dépôt des données dans un dépôt reconnu. Ne pas le faire peut compromettre l'acceptation d'un article ou l'admissibilité à des subventions futures.

**Conservation sûre** : Les dépôts de données offrent une infrastructure professionnelle de sauvegarde à long terme. Contrairement à un compte *OneDrive* qui pourrait disparaître après le départ de l'université, un dépôt institutionnel garantit que les données seront accessibles pendant des décennies.

**Visibilité et impact** : Déposer ses données augmente la visibilité de la recherche. D'autres chercheurs peuvent découvrir ces données, les citer, et potentiellement les réutiliser pour de nouvelles analyses. Cela augmente l'impact du travail au-delà de la publication initiale.

**Reproductibilité** : La science ouverte repose sur la capacité de reproduire et de vérifier les résultats. En rendant les données accessibles (avec la documentation appropriée), le chercheur permet à d'autres de reproduire ses analyses et de valider ses conclusions.

**Respect du principe FAIR** : Les données déposées dans un dépôt structuré deviennent facilement **trouvables** (avec des métadonnées), **accessibles** (via un identifiant permanent), **interopérables** (avec des formats standards), et **réutilisables** (avec une licence claire). Ce sont les quatre piliers du principe FAIR qui guide la gestion moderne des données de recherche.

#### Borealis : le dépôt Dataverse canadien

*Borealis* est le dépôt *Dataverse* canadien, une infrastructure nationale accessible à tous les chercheurs canadiens. Plusieurs institutions canadiennes mettent à la disposition de leur communauté de recherche une collection institutionnelle dans *Borealis*, avec un accompagnement complet pour les chercheurs qui souhaitent l'utiliser. *Borealis* accepte tous les types de fichiers, héberge les données sur des serveurs canadiens sécurisés, émet des DOI (identifiants permanents) pour chaque jeu de données, et permet un contrôle précis de l'accès. Le chercheur peut choisir de rendre ses données entièrement publiques, de les mettre en embargo pour une période déterminée, ou de les garder en accès restreint si ses contraintes éthiques l'exigent.

Les données déposées dans *Borealis* sont repérables via des moteurs de recherche grâce aux métadonnées structurées. Pour utiliser *Borealis*, on crée un compte, puis un nouveau jeu de données, on remplit les champs de métadonnées, on téléverse les fichiers, et on publie. À ce moment, un DOI est attribué et les données deviennent officiellement archivées. Il y a de fortes chances que la bibliothèque institutionnelle offre un soutien pour l'utilisation de *Borealis*.

Plusieurs autres dépôts multidisciplinaires sont disponibles :

**Dépôt fédéré de données de recherche (DFDR)** : Comme *Borealis*, le DFDR est une infrastructure canadienne hébergée sur des serveurs canadiens. Il est gratuit pour les projets académiques, accepte tous les types de fichiers, et offre des fonctionnalités similaires à *Borealis*. Le choix entre *Borealis* et le DFDR dépend souvent de préférences institutionnelles ou de contraintes techniques spécifiques.

*Zenodo* : Hébergé par le CERN en Suisse, *Zenodo* est un dépôt multidisciplinaire très populaire en sciences. Il est gratuit jusqu'à 50 Go par jeu de données, offre une intégration étroite avec *GitHub* (idéal pour déposer du code), et permet de déposer non seulement des données mais aussi des présentations, des rapports, etc. *Zenodo* est particulièrement apprécié pour sa simplicité et sa rapidité de dépôt.

*Open Science Framework* (*OSF*) : *OSF* est une plateforme complète pour la recherche ouverte. Au-delà du simple dépôt de données, *OSF* permet de gérer l'ensemble d'un projet de recherche : pré-enregistrement, collaboration, versionnement, liens avec d'autres dépôts. C'est une solution intéressante pour ceux qui adoptent une approche de science ouverte dès le début de leur projet.

*Figshare* : Similaire à *Zenodo*, *Figshare* offre un dépôt gratuit pour des fichiers de taille modérée, avec attribution de DOI et intégration avec des outils de recherche. Il est particulièrement utilisé pour déposer des figures, des tableaux supplémentaires, ou des jeux de données complémentaires à une publication.

Lors du dépôt de données, il faut également spécifier sous quelle **licence** elles sont partagées. La licence définit ce que les autres chercheurs peuvent faire avec ces données : les télécharger, les réutiliser, les modifier, les republier, etc. Les licences Creative Commons (CC) sont les plus courantes pour les données de recherche.

Il n'est pas nécessaire de préserver toutes les données collectées pendant un projet. Les données à préserver devraient être **réutilisables** (par soi-même ou d'autres), **compréhensibles** (avec une documentation adéquate), et avoir une **certaine valeur** (complexes, coûteuses à obtenir, ou impossibles à récolter à nouveau). Certaines données peuvent être détruites à la fin du projet : les notes préliminaires, les premières versions de documents, ou du matériel facile à recueillir à nouveau. Les données de recherche qui ne mèneront pas à une publication et qui ont servi pour l'enseignement ou un travail académique n'ont généralement pas besoin d'être conservées après l'obtention du diplôme, sauf si elles ont une valeur pour des projets futurs.

En résumé, le dépôt de données finales est une étape importante qui garantit la pérennité du travail, contribue à la science ouverte, et peut augmenter l'impact de la recherche.

### `Git` et *GitHub* tout au long d'un projet de recherche

Dans les sections précédentes, nous avons exploré différentes solutions pour entreposer les données pendant la recherche et les données finales. Chaque solution a ses forces, mais il existe un outil transversal qui mérite une attention particulière, car il peut traverser plusieurs de ces phases : **`Git` et *GitHub***. `Git` offre un niveau de contrôle, de traçabilité et de collaboration qui dépasse largement les outils conventionnels. Cette section explique ce qu'est `Git`, pourquoi il est devenu si populaire en recherche, et comment il pourrait s'intégrer dans un flux de travail de recherche.

`Git`, développé par Linus Torvalds en 2005, s'est imposé comme le système de gestion de versions décentralisé de référence. Avec l'essor des projets logiciels dans les décennies précédentes, un besoin s'est créé pour suivre l'évolution des fichiers de code au fil du temps. Quand plusieurs centaines de développeurs travaillent sur un même projet, ce suivi est essentiel pour éviter les conflits entre les versions. Bien que des systèmes centralisés existaient pour ce type de gestion, `Git` se distingue par sa décentralisation : chaque développeur détient une copie complète du projet, incluant toutes les modifications passées. Cela permet aux équipes de travailler de manière indépendante, sans dépendre d'un serveur central, ce qui réduit les risques de conflits. Sa principale force réside dans sa capacité à suivre l'évolution d'un projet en enregistrant les modifications du code source. Chaque modification, ou validation (*commit*), est enregistrée avec un message explicatif et un identifiant unique, permettant aux collaborateurs de comprendre facilement les évolutions du projet et d'assurer l'intégrité des données.

*GitHub*, lancé en 2008, est né pour répondre aux besoins de collaboration croissants dans le monde du développement du logiciel libre. Avant *GitHub*, les développeurs pouvaient utiliser `Git` en local pour gérer les versions, mais il manquait un espace centralisé pour partager le code et coordonner les efforts. *GitHub* a donc apporté cette solution en intégrant `Git` à une plateforme web, où les développeurs pouvaient héberger leurs dépôts, collaborer, et suivre le développement en ligne. Ce qui distingue *GitHub*, c'est son aspect social : les fonctionnalités comme les demandes de fusion (*pull requests*), les gestionnaires de problèmes, et le suivi des modifications permettent aux équipes de travailler efficacement et d'interagir facilement autour du code. Cette combinaison d'outils a transformé *GitHub* en un espace incontournable pour les projets en libre accès, facilitant le partage des connaissances et la collaboration.

Si `Git` a été créé pour gérer du code informatique, pourquoi un étudiant en sciences sociales devrait-il s'y intéresser ? Parce que les problèmes que `Git` résout ne sont pas spécifiques à la programmation. Ce sont des problèmes universels de **gestion de versions**, de **collaboration** et de **traçabilité**. Qui n'a jamais eu un dossier rempli de fichiers nommés `Memoire_v2_final.docx`, `Memoire_v3_VRAIMENT_final.docx`, `Memoire_final_corrige_2.docx` ? Ce problème n'existe pas avec `Git`. À chaque étape significative du travail, on crée une validation (*commit*), une « version enregistrée » avec un message descriptif. On n'a jamais besoin de créer des copies multiples de ses fichiers. On travaille toujours sur la version actuelle, et `Git` conserve automatiquement tout l'historique. `Git` et *GitHub* incarnent efficacement l'évolution vers des méthodes de travail plus flexibles, collaboratives et transparentes. Grâce à leur capacité de gestion de versions et d'historique des modifications, ils permettent aux chercheurs d'embrasser une philosophie *Agile* en assurant un suivi minutieux des changements, facilitant ainsi l'itération rapide et la reproductibilité des projets.

Le tableau suivant compare les principales solutions selon leur capacité à accompagner le chercheur à travers les trois phases du cycle de recherche.

```{r}
#| tbl-cap: Comparaison des outils de gestion de données selon les critères de sélection
#| echo: false
#| message: false
#| warning: false

library(tidyverse)
library(gt)

tbl_outils <- tibble(
  Critères = c(
    "Accessibilité",
    "Existence d'une communauté d'utilisateurs",
    "Popularité en sciences sociales",
    "Compatibilité avec d'autres outils",
    "Transparence et réplicabilité",
    "Adaptabilité et flexibilité"
  ),
  `*OSF*` = c(
    "Gratuit", "Grande", "De plus en plus populaire",
    "Facilement compatible", "Excellente", "Très flexible"
  ),
  `*GitHub*/*GitLab*` = c(
    "Gratuit", "Très grande", "De plus en plus populaire",
    "Facilement compatible", "Excellente", "Très flexible"
  ),
  `Solutions institutionnelles` = c(
    "Gratuit (institutionnel)", "Grande", "Populaire",
    "Facilement compatible", "Faible", "Peu flexible"
  ),
  `Solutions grand public` = c(
    "Gratuit (limité)", "Très grande", "Très populaire",
    "Facilement compatible", "Faible", "Peu flexible"
  )
)

tbl_outils |>
  gt() |>
  tab_caption("Comparaison des outils de gestion de données pour la recherche") |>
  cols_label(
    Critères = "Critères",
    `*OSF*` = md("*OSF*"),
    `*GitHub*/*GitLab*` = md("*GitHub*/<br>*GitLab*"),
    `Solutions institutionnelles` = md("Solutions<br>institutionnelles"),
    `Solutions grand public` = md("Solutions<br>grand public")
  ) |>
  tab_footnote(
    footnote = md("*OneDrive*, *Nextcloud*"),
    locations = cells_column_labels(columns = `Solutions institutionnelles`)
  ) |>
  tab_footnote(
    footnote = md("*Dropbox*, *Google Drive*"),
    locations = cells_column_labels(columns = `Solutions grand public`)
  ) |>
  cols_align("left", columns = everything()) |>
  cols_width(
    Critères ~ pct(28),
    everything() ~ pct(18)
  ) |>
  tab_style(
    style = list(cell_text(weight = "bold")),
    locations = cells_column_labels(everything())
  ) |>
  opt_table_lines() |>
  tab_options(
    table.font.size = px(11),
    data_row.padding = px(8),
    column_labels.background.color = "white",
    table.width = pct(100)
  )
```

*OSF* et *GitHub*/*GitLab* se démarquent par leur capacité à accompagner le chercheur à travers les trois phases : planification (préinscription, documentation), collecte et analyse (versionnement, collaboration), et archivage (DOI via *Zenodo* pour *GitHub*, archivage intégré pour *OSF*). Les solutions institutionnelles et grand public restent utiles pour l'entreposage quotidien, mais ne répondent pas aux exigences de transparence et de réplicabilité de la science ouverte.

## Manuel d'instruction : utiliser `Git` et *GitHub* pour un projet de recherche

Lorsqu'un projet est initialisé avec `Git`, un dossier caché appelé « .git » est créé dans le répertoire. Ce dossier contient tout l'historique des modifications apportées, y compris les informations sur chaque validation (*commit*), sur les différentes « branches » créées (versions parallèles du projet) et sur les métadonnées associées. Ainsi, même en travaillant avec plusieurs collaborateurs, chacun peut voir quels changements ont été effectués, par qui et pourquoi, tout en s'assurant qu'aucune version du travail ne soit perdue.

Contrairement aux outils comme *OneDrive* ou *Google Docs* qui font du versionnement automatique en arrière-plan, `Git` exige que l'utilisateur enregistre explicitement ses changements. Cela peut sembler contraignant au début, mais c'est en réalité une force : on décide consciemment quand créer une « version » et on documente pourquoi. Ce processus explicite crée une trace historique beaucoup plus riche et utile.

`Git` et *GitHub* favorisent le travail collaboratif de manière plus structurée que les outils conventionnels. Plusieurs chercheurs peuvent travailler sur le même projet simultanément, chacun dans sa branche de développement. Une fois les modifications effectuées, il est possible de fusionner les branches pour intégrer les changements. Cette approche évite les conflits majeurs. Un conflit survient lorsque deux collaborateurs modifient et enregistrent le même fichier ou la même partie de texte de manière indépendante, comme cela peut arriver dans un dossier *Dropbox* partagé. Ainsi, contrairement aux outils classiques comme *Microsoft Word*, où deux personnes peuvent se retrouver avec des versions distinctes d'un même document et perdre beaucoup de temps à fusionner manuellement les modifications, `Git` identifie automatiquement les zones conflictuelles et aide à les résoudre de façon plus méthodique.

`Git` fonctionne mieux avec des fichiers **textuels** : code, documents en format texte, fichiers de configuration, scripts, etc. Pour ces types de fichiers, `Git` peut montrer exactement quelles lignes ont changé entre deux versions, ce qui est extrêmement utile. `Git` fonctionne moins bien avec des fichiers **binaires** comme *Word* (.docx), *Excel*, PDF, des images ou des bases de données. Ces fichiers peuvent être stockés dans `Git`, mais on perd la capacité de voir les changements ligne par ligne. Pour un mémoire rédigé en *Word*, `Git` peut quand même être utile (il garde l'historique des versions du fichier), mais on n'exploite pas pleinement ses capacités.

C'est pourquoi `Git` est particulièrement populaire auprès des chercheurs qui :

- Rédigent en \LaTeX{} ou `Markdown`
- Écrivent du code pour leurs analyses
- Collaborent sur des projets techniques avec plusieurs contributeurs
- Valorisent la traçabilité complète de leur travail

En sciences sociales numériques, où le partage et la collaboration sont essentiels, `Git` et *GitHub* offrent plusieurs avantages majeurs.

1.  *Intégration et adoption répandue* : `Git` est devenu un standard *de facto* dans l'industrie du développement logiciel. Sa popularité et son adoption répandue signifient que de nombreuses ressources d'apprentissage, des tutoriels et des forums de support sont disponibles en ligne, ce qui facilite l'utilisation de cet outil pour les chercheurs en sciences sociales débutants. *GitHub*, en tant que plateforme principale de gestion des versions, bénéficie également d'une grande base d'utilisateurs et d'une communauté active, ce qui encourage la collaboration et le partage des connaissances.

2.  *Facilité de collaboration* : `Git` et *GitHub* sont conçus pour faciliter la collaboration entre les individus et les équipes. Les chercheurs en sciences sociales travaillent souvent ensemble sur des projets de recherche, et la capacité de suivre les modifications, de gérer les conflits et de fusionner les contributions devient essentielle. L'interface conviviale de *GitHub*, avec des fonctionnalités telles que les demandes de fusion et les commentaires en ligne, simplifie grandement la collaboration.

3.  *Suivi des versions et recherche reproductible* : Les chercheurs en sciences sociales doivent s'assurer que leurs travaux sont reproductibles et vérifiables. `Git` permet de suivre les versions du code, ce qui signifie que les chercheurs peuvent retrouver facilement des versions antérieures pour reproduire des analyses spécifiques ou corriger des erreurs. Cette fonctionnalité est cruciale pour maintenir l'intégrité des résultats de recherche.

4.  *Infrastructure et sécurité* : *GitHub* offre une infrastructure robuste pour l'entreposage sécurisé des dépôts `Git`. Les chercheurs peuvent être assurés que leurs travaux sont sauvegardés et protégés contre les pertes de données accidentelles. De plus, les contrôles d'accès et les autorisations granulaires de *GitHub* permettent aux chercheurs de contrôler qui peut accéder et contribuer à leurs projets.

Bien qu'il existe plusieurs alternatives à l'utilisation combinée de `Git` et de *GitHub* sur le marché, ces deux plateformes liées continuent de dominer le domaine de la gestion de versions décentralisée. Parmi les alternatives notables, on peut citer *Mercurial*, *Bitbucket*, *GitLab* et *SourceForge*. Chacun de ces outils offre des fonctionnalités similaires à celles de `Git` et *GitHub*. *GitHub* n'est pas un logiciel libre, mais il est gratuit pour ses fonctionnalités essentielles et est abondamment utilisé pour y déposer des codes sources ouverts. *GitLab*, quant à lui, offre une version communautaire à code source ouvert (*GitLab Community Edition*) qui peut être hébergée sur ses propres serveurs, ce qui en fait une option intéressante pour les institutions soucieuses de leur souveraineté numérique. Cela permet aux chercheurs en sciences sociales numériques de partager leurs textes et leurs codes avec la communauté académique et de bénéficier des contributions d'autres chercheurs. Cela favorise un environnement de partage des connaissances et de collaboration fructueuse.

En somme, `Git` et *GitHub* offrent aux chercheurs en sciences sociales numériques un moyen puissant de gérer leur code, de collaborer efficacement et de contribuer à la communauté académique grâce au logiciel libre. Bien que leur apprentissage puisse représenter un défi initial, les avantages qu'ils apportent en termes de suivi des versions, de collaboration et de partage des connaissances en font des outils essentiels dans l'arsenal de tout chercheur moderne.

Maintenant que les avantages de `Git` et *GitHub* pour la recherche sont clairs, passons à la pratique. Cette section guide le chercheur dans les étapes essentielles pour mettre en place `Git` pour un projet typique de recherche en sciences sociales.

**Prérequis** : Il faut d'abord installer `Git` sur son ordinateur (téléchargeable depuis [git-scm.com](https://git-scm.com)) et créer un compte *GitHub* gratuit sur [github.com](https://github.com).

**Étape 1** : Création d'un répertoire local et initialisation de `Git`

Il faut ouvrir le terminal (sur *macOS* et *Linux*) ou l'application `Git Bash` (sur *Windows*) et naviguer vers le dossier où l'on souhaite enregistrer le projet.

```bash
cd chemin/vers/votre/dossier
```

On crée un nouveau répertoire pour le projet et on y accède.

```bash
mkdir mon_projet
cd mon_projet
```

On initialise `Git` dans ce répertoire.

```bash
git init
```

**Étape 2** : Structurer le projet

Une bonne organisation facilite la gestion à long terme. On crée une structure logique :

```bash
mkdir redaction
mkdir code
mkdir data
mkdir resultats
mkdir documentation
```

Le projet ressemblera maintenant à ceci :

```
mon_projet/
├── redaction/          # Chapitres du mémoire
├── code/               # Scripts d'analyse ($\textsf{R}$, `Python`)
├── data/               # Données NON SENSIBLES uniquement
├── resultats/          # Graphiques et tableaux générés
└── documentation/      # README, notes méthodologiques
```

**Important** : Le dossier `data/` ne devrait contenir que des données **non sensibles**. Les données sensibles restent dans *Nextcloud* ou *OneDrive* institutionnel.

**Étape 3** : Le fichier .gitignore

On crée un fichier `.gitignore` à la racine du projet pour indiquer à `Git` quels fichiers ne doivent **jamais** être ajoutés au dépôt :

```
# Ne JAMAIS pousser sur GitHub :
data/entrevues/              # Données sensibles
data/sondages_bruts/         # Données avec infos personnelles

# Fichiers système et temporaires
.DS_Store
.Rhistory
*.tmp
~$*.docx
```

**Étape 4** : Ajout du code et des fichiers

On ajoute les fichiers $\textsf{R}$ contenant le code pour les analyses dans le répertoire. Par exemple, on peut avoir des fichiers `analyse_medias.R` et `analyse_sondages.R`.

On utilise la commande `git status` pour vérifier l'état des fichiers.

```bash
git status
```

**Étape 5** : Ajout, validation et dépôt des modifications

On ajoute les fichiers pour qu'ils soient prêts à être validés.

```bash
git add -A
```

On valide les modifications avec un message descriptif.

```bash
git commit -m "Structure initiale du projet avec code d'analyse"
```

**Étape 6** : Création du répertoire sur *GitHub* et du lien avec le répertoire local

On va sur *GitHub* et on se connecte à son compte. On crée un nouveau répertoire (privé ou public selon les besoins) avec le nom du projet.

De retour dans le terminal, on ajoute le lien *GitHub* au répertoire local.

```bash
git remote add origin https://github.com/votre-utilisateur/mon_projet.git
```

**Étape 7** : Pousser le travail sur *GitHub*

On envoie les dépôts locaux vers *GitHub*.

```bash
git push -u origin main
```

**Étape 8** : Collaboration avec les collègues

Si des collègues souhaitent contribuer au projet, ils peuvent dupliquer (en langage `Git`, on utilise le terme *fork*) le répertoire sur *GitHub*, ce qui créera une copie dans leur propre compte.

Lorsqu'ils ont fait des modifications dans leur copie, ils peuvent soumettre une demande de fusion (*pull request*) pour demander la permission de fusionner leurs modifications dans le répertoire principal.

**Étape 9** : Acceptation des modifications des collègues

Lorsque les collègues ont soumis des modifications et demandé de les fusionner, on peut mettre à jour le répertoire local avec leurs changements.

```bash
git pull origin main
```

**Étape 10** : Répéter le processus

On répète les étapes au fur et à mesure que le projet se développe, que du code s'ajoute, que des analyses s'effectuent et que la collaboration avec les collègues se poursuit. Il convient de valider et de pousser régulièrement les modifications pour maintenir le répertoire à jour. Des validations (*commits*) fréquentes avec des messages clairs sont préférables à une grosse validation en fin de semaine.

### Alternative : *GitHub Desktop* et intégrations

Alors que le terminal reste une approche fondamentale pour maîtriser `Git` et *GitHub*, il existe des outils conviviaux tels que **`GitHub Desktop`** qui offrent une alternative intuitive. Cet outil simplifie le processus de gestion de versions décentralisée, en particulier pour ceux qui souhaitent commencer par une approche visuelle. La plupart des environnements de code comme `RStudio` et `VS Code` ont également des interfaces pour faciliter ces opérations.

```{r, out.width="80%"}
#| label: fig-github
#| echo: false
#| fig-cap: "`GitHub Desktop` facilite l'utilisation conjointe de `Git` et de *GitHub*."
knitr::include_graphics("images/chapitre_4_github_desktop.png", dpi = 600)
```

`GitHub Desktop` fournit une vue claire des répertoires, des modifications, des branches et des demandes de fusion (*pull requests*). Il élimine la nécessité de mémoriser les commandes en ligne de terminal, ce qui peut être un défi pour certains chercheurs. L'application simplifie également la résolution des conflits lors de la fusion des branches.

Toutefois, en utilisant `GitHub Desktop`, il est possible de perdre la compréhension des commandes `Git` en ligne de commande, ce qui pourrait devenir un inconvénient dans un environnement sans interface visuelle. De plus, `GitHub Desktop` est spécifiquement conçu pour interagir avec *GitHub*. Pour ceux qui doivent travailler avec d'autres plateformes de gestion de versions, cela pourrait poser des problèmes.

La décision entre l'utilisation du terminal et de `GitHub Desktop` dépend des préférences et des besoins de chacun. Pour les chercheurs qui débutent, `GitHub Desktop` offre une transition en douceur vers les concepts de gestion de versions. Cependant, il est important de ne pas se limiter à une interface visuelle. Comprendre les commandes `Git` en ligne de commande reste essentiel pour résoudre des problèmes complexes, gérer des projets avancés et collaborer avec d'autres chercheurs qui utilisent des approches basées sur le terminal.

## En terminant

La gestion des données de recherche est devenue une compétence essentielle pour tout chercheur en sciences sociales numériques. Ce chapitre a présenté les différentes dimensions de cette gestion : la planification en amont, la gestion des données actives (sensibles ou non) et l'archivage des données finales selon les principes de la science ouverte.

Le choix des outils dépend des besoins spécifiques, mais quelques principes fondamentaux s'appliquent universellement : ne jamais conserver ses données uniquement en local, distinguer clairement les données sensibles des données non sensibles, documenter rigoureusement ses fichiers, et planifier l'archivage dès le début du projet.

`Git` et *GitHub*, bien qu'initialement conçus pour les développeurs, offrent aux chercheurs un système de gestion de versions puissant qui favorise la traçabilité, la collaboration et la reproductibilité. L'investissement initial dans l'apprentissage de ces outils est largement compensé par les bénéfices à long terme pour la qualité et la transparence de la recherche.

<!-- ### Où entreposer des données massives -->

<!-- Les sections précédentes ont couvert les besoins typiques de la plupart des projets de recherche aux cycles supérieurs : quelques gigaoctets de données, des fichiers de taille raisonnable, des outils standards. Mais certains projets génèrent des volumes de données qui dépassent largement ces limites. Ceux qui travaillent avec des images satellite, du *web scraping* à grande échelle, des enregistrements vidéo, ou des simulations computationnelles ont de fortes chances d'utiliser des **données massives**. -->

<!-- Qu'est-ce qu'une donnée massive en recherche ? Il n'y a pas de définition universelle, mais voici quelques indicateurs pratiques. Un **volume** important de données ; une **vélocité**, c'est-à-dire une collecte continue ou très fréquente de données ; de la **variété**, présente dans des données hétérogènes nécessitant un traitement comme des images ; ou une **complexité computationnelle** lorsqu'il y a un besoin de traitements avec plusieurs jours de calcul. Si un projet correspond à une ou plusieurs de ces caractéristiques, les solutions présentées plus tôt ne conviendront pas. Des infrastructures spécialisées deviennent nécessaires. -->

<!-- ### Le cycle de vie des données massives -->

<!-- Les données massives nécessitent une approche structurée qui couvre quatre phases distinctes : -->

<!-- **Collecte** : Le cycle de vie d'une donnée commence au moment où elle est créée ou collectée. Pour des données massives, la collecte est souvent automatisée (scripts de *scraping*, API). Avant de commencer, il convient de déterminer clairement la valeur et la pertinence des données. Il faut établir des règles pour collecter les données d'une manière qui préserve leur utilité : documenter quand, où, comment et pourquoi elles ont été générées. Il faut planifier l'infrastructure dès le début : a-t-on besoin d'un serveur qui tourne en continu ? D'un espace de stockage temporaire pour les données brutes ? -->

<!-- **Entreposage** : Les données doivent être entreposées dans un environnement stable et adapté à leurs origines et à leurs applications potentielles. Pour des volumes importants, des espaces de grande capacité sont nécessaires, souvent avec des systèmes de fichiers distribués. Toute donnée digne d'être collectée mérite d'être protégée, ce qui suppose des sauvegardes régulières et une infrastructure fiable. Les données sensibles doivent être chiffrées pour respecter les exigences éthiques et réglementaires. C'est à cette étape que les infrastructures de calcul haute performance (comme Calcul Québec) deviennent essentielles, car elles offrent non seulement de l'entreposage massif, mais aussi les ressources computationnelles pour traiter et transformer les données brutes. -->

<!-- **Partage** : Les données n'ont de valeur que si elles peuvent être mises à la disposition des utilisateurs autorisés. Pour des projets collaboratifs, il faut établir clairement qui peut accéder aux données, quand et comment. Les utilisateurs doivent pouvoir localiser, accéder, modifier et analyser les données selon les besoins. Il convient de documenter les conventions de nomenclature, les structures de fichiers et les transformations appliquées. Cette documentation est cruciale pour que les collaborateurs puissent comprendre et utiliser les données efficacement. -->

<!-- **Archivage** : À la fin du projet, les données cessent d'être utilisées quotidiennement, mais elles conservent une valeur à long terme. Pour des données massives, l'archivage complet n'est pas toujours faisable ou nécessaire dans un dépôt comme *Borealis*. Il faut décider quelles données conserver : les données brutes irremplaçables, un échantillon représentatif, les données agrégées, ou simplement les scripts et métadonnées permettant de régénérer les résultats. Les données archivées doivent rester organisées et protégées, même si leur accessibilité immédiate perd de son importance. -->

<!-- ### Solutions pour données massives -->

<!-- Comme mentionné dans la section précédente, l'Alliance de recherche numérique du Canada opère des centres de calcul haute performance à travers le pays. Pour des données massives, ces infrastructures deviennent non seulement recommandées, mais essentielles. Au-delà du simple stockage sécurisé, elles offrent l'accès à des grappes de serveurs pour traiter les données, ce qui est crucial lorsque les analyses nécessitent plusieurs heures ou jours de calcul. Pour des projets nécessitant des ressources substantielles (plusieurs téraoctets de stockage ou des milliers d'heures de calcul), il faut soumettre une demande d'allocation annuelle avec justification scientifique lors des concours d'allocation. -->

<!-- Certaines institutions offrent des crédits pour des services infonuagiques académiques (*AWS Educate*, *Google Cloud for Education*, *Microsoft Azure for Research*). Ces services peuvent être utiles pour des projets nécessitant une grande flexibilité ou des outils spécifiques non disponibles sur Calcul Québec. Toutefois, les crédits gratuits sont généralement limités dans le temps et en montant. Comme mentionné plus tôt, certaines facultés ou groupes de recherche disposent de leurs propres serveurs. Pour ceux qui font partie d'un laboratoire avec de telles ressources, c'est souvent la solution la plus simple pour démarrer. -->

<!-- En résumé, les données massives nécessitent une planification et des infrastructures spécialisées. **Calcul Québec et l'Alliance** sont les meilleurs alliés pour des projets académiques d'envergure. La courbe d'apprentissage ne doit pas être sous-estimée, mais ne doit pas non plus décourager : les ressources et le soutien sont disponibles. Il convient de commencer tôt, de documenter rigoureusement, et d'automatiser autant que possible. -->
